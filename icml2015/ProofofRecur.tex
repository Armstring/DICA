%!TEX root =  appendix.tex
\subsection{Proof of Theorem \ref{thm:recursiveAlg}}
Firstly note that this algorithm compute an estimation of the mixing matrix. 
By Lemma \ref{lem:BhatinverseB}, $\hat{B}B^{-1}$ is close to some orthonormal matrix $R^*$.
Define $y = R^*B^{-1}x = R^*R K^{-1/2}D_{\phi}^{-1/2}s + R^*B^{-1}\eps$. Recall that this procedure  is exactly the qusi-whitening in Section \ref{subsec:quasiwhite}. Therefore, 
$\hat{y}$ is the empirical observation of $y$ which is a orthonormal mixture of independent sources contaminated by Gaussian Noise. 
Since the mixing matrix is orthonormal, we can apply the recursion idea of \citet{vempala2014max} to recover $R^*R$. 
Then $\hat{B}R^*R= BB^{-1}\hat{B}R^*R \approxeq AD_{\phi}^{1/2}K^{1/2}R^{\top} (R^*)^{\top}R^*R = AD_{\phi}^{1/2}K^{1/2}$, thus recover $A$ up to column permutation and scaling. 

Moreover, note that the calculation of $M$ in the helper function `Recur' is exactly the $M$ in the algorithm of DICA (we call it recursion version of HKICA, because in the helper function `Recur', it has the same format as HKICA).
Therefore, by Lemma \ref{lem:Tvariantion},
\[
\|M-\hat{M}\|_2 \le Q.
\]

We now follow the idea of \citet{vempala2014max} to analyze the error accumulation of the recursion.
Recall that $M = \bar{R}\Lambda\bar{R}^{\top}$, where $\bar{R} = R^*R$ is an orthonormal matrix.
Assume we have computed a m-dimensional subspace in a recursion of depth $k-1$ whose orthonormal projection matrix is $V^{(k-1)} \in \R^{d\times m}$, such that there exists $m$ columns of $\bar{R}$ (WLOG assume it is $1,\ldots,m$.) satisfying
\[
\sin\left(\Theta\left(V^{(k-1)}, \bar{R}_{1:m}\right)\right) \le E_{k-1},
\] 
Where $\bar{R}_{1:m}$ is the first $m$ columns of $\bar{R}$ and $E_{k-1}$ is a error upper bound for depth $k-1$ recursion.
Then
\begin{align*}
& \quad {V^{(k-1)}}^\top MV^{(k-1)} = \left( {V^{(k-1)}}^\top\bar{R}_{1:m}, {V^{(k-1)}}^\top\bar{R}_{m+1:d}\right)\Lambda
\left(
\begin{tabular}{c}
$\bar{R}_{1:m}^{\top} V^{(k-1)}$ \\
$\bar{R}_{m+1:d}^{\top} V^{(k-1)}$
\end{tabular}
\right) \\
& = {V^{(k-1)}}^\top\bar{R}_{1:m}\Lambda_{1:m} \bar{R}_{1:m}^{\top} V^{(k-1)} + {V^{(k-1)}}^\top\bar{R}_{m+1:d}\Lambda_{m+1:d}\bar{R}_{m+1:d}^{\top} V^{(k-1)},
\end{align*}
where $\Lambda_{1:m}$ and $\Lambda_{m+1:d}$ are the first $m\times m$  and last $(d-m)\times (d-m)$ submatrices of the diagonal matrix $\Lambda$.

Recall that the diagonal elements of $\Lambda$ is the square of Cauchy random variables. The following proposition characterize the maximal spacing of i.i.d. Cauchy random variables.
\begin{prop}
\label{prop:maximalGapCauchy}
Given $\{Z_1,\ldots, Z_d\}$ are i.i.d. Cauchy random variables, with probability at least $1-3\delta$, the following inequality hold simultaneously.
\begin{itemize}
\item $\max_i\min_{j\neq i} | Z_i - Z_j| \ge \ell$;
\item $ \max_i |Z-i| \le L$;
\item $\min |Z_i| \ge \ell$;
\end{itemize} 
where $\ell = \frac{\pi}{2(d+1)}\delta^{1/d}$ and $ L = \frac{3d}{\pi\delta}$.
\end{prop}
\begin{proof}
Note that the first inequality is about the maximal spacing.
\[
\Prob{\max_i\min_{j\neq i} | Z_i - Z_j| \ge \ell} \ge \Prob{\exists i, |Z_i| \ge d\ell} = 1-\Prob{\forall i, |Z_i| \le d\ell} \le 1- (\frac{2d\ell}{\pi})^d.
\]
Picking $\ell = \frac{\pi}{2d}\delta^{1/d}$, with probability at least $1-\delta$, 
\[
\max_i\min_{j\neq i} | Z_i - Z_j| \ge \frac{\pi}{2d}\delta^{1/d}.
\]

Similarly, picking $\ell = \frac{\pi}{2d}\delta^{1/d}$,
\[
\Prob{\min_i |Z_i| \ge \ell} \le (1- \frac{2\ell}{\pi})^d \ge (1-\frac{\delta^{1/d}}{d})^d \ge (1-\frac{\delta}{d})^d \ge (1-\frac{\delta}{d})e^{-\delta} \ge 1-\frac{d+1}{d}\delta.
\]
Therefore, with probability at least $1-\delta$, 
\[
\min_i |Z_i| \ge \frac{\pi}{2d}(\frac{d}{d+1}\delta)^{1/d}\ge \frac{\pi}{2(d+1)}\delta. 
\]

Lastly, for a Cauchy random variable $Z$, $\Prob{|Z|\le \frac{3L}{\pi}}  = \frac{2}{\pi}\arctan(\frac{3L}{\pi})$.
Note that for $L\ge \pi$,
\[
\tan(\frac{\pi}{2} - \frac{\pi}{2L}) \le \frac{1}{\cos (\frac{\pi}{2} - \frac{\pi}{2L})} \le \frac{1}{\sin(\frac{\pi}{2L})}\le \frac{1}{\frac{\pi}{2L} - \left( \frac{\pi}{2L}\right)^3}\le \frac{2L}{\pi} \frac{1}{1 - \left(\frac{\pi}{2L}\right)^2} \le \frac{3L}{\pi}.
\]
Thus, 
\[
\Prob{|Z|\le \frac{3L}{\pi}}  \ge 1 - \frac{1}{L}.
\]
Therefore, picking $L = \frac{d}{\delta}$, the probability that $\max_i |Z_i| \le \frac{3L}{\pi}$ is at least 
\[
\left(1-\frac{1}{L}\right)^d \le 1-\frac{d}{L} \le 1-\delta.
\]
\end{proof}
Denote the Event of Proposition \ref{prop:maximalGapCauchy} as $\Ez$.
Therefore, by Proposition \ref{prop:maximalGapCauchy}, with probability at least $1-3\delta$, 
the estimation error of ${V^{(k-1)}}^\top\bar{R}_{1:m}\Lambda_{1:m} \bar{R}_{1:m}^{\top} V^{(k-1)}$ is 
\begin{align*}
& \quad \| {V^{(k-1)}}^\top \hat{M}V^{(k-1)} - {V^{(k-1)}}^\top\bar{R}_{1:m}\Lambda_{1:m} \bar{R}_{1:m}^{\top} V^{(k-1)} \|_2 \\
& \le \|{V^{(k-1)}}^\top \hat{M}V^{(k-1)} - {V^{(k-1)}}^\top MV^{(k-1)}\|_2 + \| {V^{(k-1)}}^\top MV^{(k-1)} -{V^{(k-1)}}^\top\bar{R}_{1:m}\Lambda_{1:m} \bar{R}_{1:m}^{\top} V^{(k-1)} \|_2 \\
&  = \|{V^{(k-1)}}^\top \hat{M}V^{(k-1)} - {V^{(k-1)}}^\top MV^{(k-1)}\|_2 + \|{V^{(k-1)}}^\top\bar{R}_{m+1:d}\Lambda_{m+1:d}\bar{R}_{m+1:d}^{\top} V^{(k-1)} \|_2 \\
& \le Q + E_{k-1}^2 \frac{9d^2}{\pi^2\delta^2} 
\end{align*}
Also, the minimal spacing of the diagonal elements of $\Lambda_{1:m}$ satisfying
\[
\max_i\min_{j\neq i} |Z_i|^2 - |Z_j^2| \ge 2\min_i |Z_i| \max_i\min_{j\neq j} |Z_i - Z_j| \ge \frac{\pi^2}{2(d+1)^2}\delta^2.
\] 
By Wedin's Theorem \citep{stewart1990matrix}, 
\[
E_k \le \frac{Q + E_{k-1}^2 \frac{9d^2}{\pi^2\delta^2} }{\frac{\pi^2}{2(d+1)^2}\delta^{2/d}} = \frac{2(d+1)^2Q}{\pi^2\delta^{2/d}} + \frac{18d^2(d+1)^2}{\pi^4\delta^{2+2/d}}E_{k-1}^2.
\]
Therefore, by Claim 4.8 of \citet{vempala2014max}, given that $Q\le \frac{\pi^6\delta^{2+4/d}}{36d^2(d+1)^4}$, for $0\le k\le d$,
\[
E_k \le \frac{4(d+1)^2}{\pi^2\delta^{2/d}}Q. 
\]
Thus Line 6 returns a matrix $\hat{R}$ s.t.
\[
\|\hat{R} - R^*R\|_2 = 2-2\cos(\Theta)= 2 - 2\sqrt{1-\sin^2(\Theta)} \le 2 - 2\sqrt{1-\frac{16(d+1)^4}{\pi^4\delta^{4/d}}Q^2}
\]
Therefore,
\begin{align*}
& \quad \| \hat{B}\hat{R} - AD_{\phi}^{1/2}K^{1/2}\|_2 \\
& = \| \hat{B}\hat{R} - AD_{\phi}^{1/2}K^{1/2}R^{\top}{R^*}^{\top}R^*R\|_2 \\
& \le \| \hat{B}\hat{R} -  \hat{B}R^*R\|_2 + \|\hat{B}R^*R - AD_{\phi}^{1/2}K^{1/2}R^{\top}{R^*}^{\top}R^*R \|_2 \\
& \le \|B\|_2\|B^{-1}\hat{B}\|_2\|\hat{R} - R^*R\|_2 + \|\hat{B} - AD_{\phi}^{1/2}K^{1/2}R^{\top}{R^*}^{\top}\|_2 \\
& \le  \|B\|_2\|B^{-1}\hat{B}\|_2\|\hat{R} - R^*R\|_2 +\|B\|_2\|B^{-1}\hat{B} - {R^*}^{\top}\|_2.
\end{align*}

Note that by Lemma \ref{lem:dmin}, 
 $ \|B\|_2 \le \sigma_{\max}(A)L_uA_{(2,\max)}\kappa_{\max}^{1/2}$. 
Similarly by Lemma \ref{lem:BhatinverseB}, $\| B^{-1}\hat{B}\|_2 \le (1+\bar{\xi})^{1/2}$. 
Also, given that $\bar{xi} \le 1/2$, by Lemma \ref{lem:inversevariation}, $\|B^{-1}\hat{B} -{R^*}^{\top}\|_2 \le 2\bar{\xi}$.
Adding all the terms together,
\[
\| \hat{B}\hat{R} - AD_{\phi}^{1/2}K^{1/2}\|_2 \le \sigma_{\max}(A)L_uA_{(2,\max)}\kappa_{\max}^{1/2}\left((1+\bar{\xi})^{1/2}\left(  2 - 2\sqrt{1-\frac{16(d+1)^4}{\pi^4\delta^{4/d}}Q^2}\right) + 2\bar{\xi}\right).
\] 

Note that we need Event $\Epsi$ to be satisfied once, and $\Ephi$ and $\Ez$ for at most $d$ times. 
Thus, given the conditions of Theorem \ref{thm:finalRes}, with probability $1-7d\delta$, we have a polynomial error bound. 