\documentclass[english]{article} % For LaTeX2e

\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
%\definecolor{darkgreen}{RGB}{26, 82, 87} % solid linking color
\usepackage[bookmarks=false]{hyperref}
  \hypersetup{
  	pdftex,
    pdffitwindow=true,
    pdfstartview={FitH},
    pdfnewwindow=true,
    colorlinks,
    linktocpage=true,
    linkcolor=Green,
    urlcolor=Green,
    citecolor=Green
}


% uncomment the following line and comment the line after it if you want to turn
% off todos
%\usepackage[disable,backgroundcolor = White,textwidth=\marginparwidth]{todonotes}
\usepackage[backgroundcolor = White,textwidth=\marginparwidth]{todonotes}
\newcommand{\todoc}[2][]{\todo[color=Apricot,#1]{#2}}
\newcommand{\todob}[2][]{\todo[color=Cerulean!20,#1]{#2}}
\newcommand{\todoa}[2][]{\todo[color=Purple!20,#1]{#2}}
\newcommand{\todor}[2][]{\todo[color=Blue!10,#1]{#2}}

%\usepackage{nips12submit_e,times}
%\usepackage{geometry}
\usepackage{comment}
\usepackage{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[authoryear]{natbib}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{setspace}
\newcommand{\cset}[2]{\left\{#1\,:\,#2\right\}}
\DeclareMathOperator{\esssup}{ess\;sup}

\usepackage{svn-multi}
% For a good documentation of svn-multi, see http://www.tug.org/pracjourn/2007-3/scharrer/scharrer.pdf
% Do this for new svn tex files:
% svn propset svn:keywords "Id Author Date Rev URL" *.tex
\svnidlong {$HeadURL$} {$LastChangedDate$} {$LastChangedRevision$} {$LastCngedBy: ruitong $}
\svnid{$Id$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Setting up the page geometry
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[margin=1.0in]{geometry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Setting up the headers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[ol]{\slshape\leftmark}
\fancyfoot[ol]{Rev: \svnrev\ (\svnfilerev)}
\fancyfoot[or]{\svnyear -\svnmonth -\svnday\ \svnhour:\svnminute} %Date
% If the information should be also placed % on the chapter page use:
\fancypagestyle{plain}{%
	\fancyhead{}
	\fancyhead[ol]{\slshape\leftmark}
	\fancyfoot[ol]{Rev: \svnrev\ (\svnfilerev)} \fancyfoot[or]{\svnyear -\svnmonth -\svnday\ \svnhour:\svnminute} %Date
}

%\usepackage{fullpage}
\setlength{\headheight}{15.0pt}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version


\usepackage{algorithmic}
\usepackage{babel}


\newcommand{\cN}{\cal{N}}
\renewcommand{\P}{{\mathcal P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ex}[1]{\mathbb{E}[#1]}
\newcommand{\Em}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\snorm}[1]{\left\|#1\right\|} % scaling norm
\newcommand{\lmax}[1]{\lambda_{\mathrm{max}}(#1)}
\newcommand{\lmin}[1]{\lambda_{\mathrm{min}}(#1)}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\dprod}[2]{\langle #1,#2 \rangle_{M}}
\newcommand{\hA}{\hat{A}}
\newcommand{\hb}{\hat{b}}
\newcommand{\hC}{\hat{C}}
\newcommand{\hAp}{\hat{A}^\prime}
\newcommand{\hbp}{\hat{b}^\prime}
\newcommand{\hCp}{\hat{C}^\prime}
\renewcommand{\th}{\theta}
\newcommand{\vth}{\hat{\theta}_{V}}
\newcommand{\lth}{\hat{\theta}_{\lambda}}
\newcommand{\hth}{\hat{\theta}}
\newcommand{\ltho}{\hat{\theta}_{\lambda_1}}
\newcommand{\ltht}{\hat{\theta}_{\lambda_2}}
\newcommand{\hlth}{\hat{\theta}_{\hat{\lambda}}}
\newcommand{\slth}{\hat{\theta}_{\lambda^*}}
\newcommand{\plth}{\hat{\theta}_{\lambda_p}}
\newcommand{\hthp}{\hat{\theta}^\prime}
\newcommand{\lthp}{\hat{\theta}^\prime_{\lambdap}}
\newcommand{\ath}{\hat{\theta}_{a}}
\newcommand{\thetap}{\theta^\prime}
\newcommand{\sth}{\theta^*}
\newcommand{\ind}[1]{\mathbb{I}_{\left\{ #1 \right\}}}
\newcommand{\hzeta}{\hat{\zeta}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\cf}{\emph{cf.}}
\newcommand{\T}[1]{T\left( #1 \right)}
\newcommand{\St}[1]{S\left( #1 \right)}
\newcommand{\Ts}[2]{T_{#1}\left( #2 \right)}
\newcommand{\Ss}[2]{S_{#1}\left( #2 \right)}
\newcommand{\Ord}[1]{O\left( #1 \right)}
\newcommand{\etc}{\emph{etc.}}
%\newcommand{\det}{\mathrm{det}}
\newcommand{\bsth}{\bar{\theta}^*}
\newcommand{\sal}{\emph} %Emphasis the J.D. Salinger way :)
\newcommand{\gradL}{\nabla \mathcal{L}}
\newcommand{\lambdap}{{\lambda^\prime}}
\newcommand{\lambdahp}{\hat{\lambda}^\prime}
\newcommand{\pqLoss}[1]{\mathcal{L}_{#1}}
\newcommand{\pLoss}[2]{\pqLoss{#1}( #2 )}
\newcommand{\qLoss}{\pqLoss{M}}
\newcommand{\Loss}[1]{\qLoss(#1)}

\newcommand{\hpqLoss}[1]{\mathcal{\hat{L}}_{#1}}
\newcommand{\hpLoss}[2]{\hpqLoss{#1}( #2 )}
\newcommand{\hqLoss}{\hpqLoss{M}}
\newcommand{\hLoss}[1]{\hqLoss(#1)}
\newcommand{\bX}{\bar{X}}
\newcommand{\bY}{\bar{Y}}


\newcommand{\rth}{\hat{\theta}}
\newcommand{\tth}{\tilde{\theta}}
\newcommand{\maxeig}{\nu_{\max}}
\newcommand{\mineig}{\nu_{\min}}

\newcommand{\ra}{\rightarrow}
\newcommand{\real}{\mathbb{R}}

\newcommand{\one}[1]{\mathbf{1}_{\{#1\}}}

\newcommand{\rl}[1]{\mathbb{R}^{#1}}
\newcommand{\Mset}{\mathcal{M}(\varepsilon)}
\newcommand{\Cset}{\mathcal{C}}
\newcommand{\truel}{L}
\newcommand{\zol}{L_{\mathrm{0-1}}}
\newcommand{\hinl}{L_{\mathrm{hinge}}}
\newcommand{\phil}{L_{\varphi}}
\newcommand{\lspace}{\left\{1, \dots,K \right\}}
\newcommand{\risk}{\mathcal{R}}
\newcommand{\scoref}{\in \mathcal{H}}
\newcommand{\subscoref}{\in \mathcal{H}^\prime}
\newcommand{\hQ}{\hat{Q}}
\DeclareMathOperator{\supp}{supp}
\renewcommand{\natural}{\mathbb{N}}


\newcounter{assumption}%[section]
\newcommand{\theassumptionletter}{A}
\renewcommand{\theassumption}{\theassumptionletter\arabic{assumption}}

\newenvironment{ass}[1][]{\begin{trivlist}\item[] \refstepcounter{assumption}%
 {\bf Assumption\ \theassumption\ #1} \it}{%\par\nobreak\noindent\sl\ignorespaces}{%
 \ifvmode\smallskip\fi\end{trivlist}}
\newcommand{\aref}[1]{(\ref{#1})}
\newenvironment{ass*}[1][]{\begin{trivlist}\item[] %
 {\bf Assumption\  #1} }{%\par\nobreak\noindent\sl\ignorespaces}{%
 \ifvmode\smallskip\fi\end{trivlist}}


\usepackage[capitalize]{cleveref}

% theorems/definitions
\newtheorem{lemma}{Lemma}[section]
\newtheorem{thm}[lemma]{Theorem}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{cor}[lemma]{Corollary}
\newtheorem{example}[lemma]{Example}
\newtheorem{prop}[lemma]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{remark}[lemma]{Remark}
%\newtheorem{exercise}{Exercise}[chapter]
\newtheorem*{solution}{Solution}
%\newtheorem{assumption}{Assumption}[section]
\newtheorem{note}[lemma]{Note}
\newtheorem{problem}[lemma]{Problem}

%\crefname{assumption}{assumption}{assumptions}
%\Crefname{assumption}{Assumption}{Assumptions}
%\crefname{Exercise}{Exercise}{Exercises}
%\Crefname{Exercise}{Exercise}{Exercises}


\newcommand{\FF}{\mathcal{F}}
\newcommand{\TT}{\mathcal{T}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\KK}{\mathcal{K}}
\newcommand{\eps}{\varepsilon}



\title{Deterministic ICA}
\author{
}
\date{
Version: \svnrev\\
Date: \svndate\\
Last change: \svnauthor
%\today\ @ \currenttime \ Version 0.5 (r5322)
}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\section{Motivation}
\label{sec:motivation}
% Handbook of Blind Source Separation, Independent Component Analysis and Applications
% Chapter 1, Contextual difficulties 1.1.2.1 Independence; sweeps the problem under the rug:
% At first glance, this assumption seems very strong. In fact, we may think it rarely occurs, and it is more difficult to satisfy than second order independence i.e. non- correlation. In the above biological problem, it was indeed tricky to explain statistical independence between p(t) and v(t). In fact, one can remark that “speed v(t) is related to location p(t) through v(t) = dp(t)/dt. The two variables then cannot be independent”. However, this functional dependence is not a statistical dependence. The knowledge of p(t) at a given instant t does not provide any information on v(t), and vice versa. In other words, while the random vectors of speed and position, [v(t1),v(t2),...,v(tk)]T	and [p(t1), p(t2),..., p(tk)]T	are generally not independent, the (marginal) random variables, v(t) and p(t), at any given time t are independent. Therefore, for instantaneous ICA algorithms, the dependence is irrelevant.
ICA (independent component analysis) algorithms are known to work well for unmixing the mixtures of various deterministic signals.
In fact, it is quite common that the papers start by showing pictures of periodic functions (perfectly deterministic and predictable) and then demonstrating that the functions can be recovered from their mixtures \citep[e.g.,][]{HyvOja00}.
The ICA identifiability question is what signals (functions) can be recovered from their mixtures and is the subject of this paper. \todoc{A nicer intro would show some examples when they can and when they cannot be identified. I suggest adding some examples, some figures.}

By definition, any two deterministic signals are independent of each other. Can then ICA algorithms unmix any mixtures of deterministic signals (with the usual restrictions that the mixing matrix is nonsingular and no components are repeated)? 
Clearly, this is not possible (even when the mixture is non-singular).
That ICA algorithms work well on some deterministic signals is a curious phenomenon which indicates that the usual probabilistic notion of independence is unsatisfactory if one wishes to have a deeper understanding of how and why ICA algorithms work.

Formally, the ICA identifiability question is as follows:
Let $f:\natural \ra \real^d$ be a $d$-dimensional deterministic ``signal''. 
We will denote by $f_i$ the $i$th component of $f$ and we will call $f$ the source.
Let $A = (a_{ij})$ be a $d\times d$ non-singular matrix and let $g:\natural \ra \real^d$ be defined by
\[
g(t) = A f(t), \quad t\in \natural.
\]
We will call $g$ the observed signal and by slightly abusing notation we will also write $g = A f$.%
\footnote{In fact, if $f$, $g$ are viewed as a $d\times \infty$ ``matrices'', then there is no abuse.}
In the ICA problem one observes the values of $g$ in a sequential manner and the goal is to recover the components of $f$ up to scaling and permutation. For $f,f':\natural \ra \real^d$, we shall call $f$ and $f'$ equivalent (which we denote by $f\sim f'$) if the components of $f$ and $f'$ agree up to scaling and permutation.
Then, we say that the source $f$ is identifiable if there is a mapping $\FF$
 between the space of $d$-dimensional signals such that 
  for any nonsingular $A\in \real^{d\times d}$,
  $\FF( Af )\sim f$ holds.
In the ICA identifiability question one asks for a ``large'' set of multidimensional signals that are identifiable. Ideally, one would like to ask for the largest set, but it is unclear whether such a set exists. \todoc{Does it?}

We are not the first to notice that ICA algorithms (sometimes) work for deterministic signals.
In particular, in their paper Pando G. Georgiev and Fabian J. Theis note the following:
\begin{quote}
 ``Our objective is to estimate the source signals sequentially one-by-one or simultaneously assuming that they are statistically independent.
 %
The uniqueness of such estimation (up to permutation and scaling), or identifiability of the linear ICA model, is justified in the literature by the Skitovitch-Darmois theorem [41,17].
 %
 Whereas this theorem is probabilistic in nature, an elementary lemma from optimization theory (although with a non-elementary proof) can serve the same purpose -- rigorous justification of the identifiability of ICA model, when maximization of the cumulants is used.\footnote{
Optimization Techniques for Data Representations with Biomedical Applications, in
P.M. Pardalos, H.E. Romeijn (eds.), Handbook of Optimization in Medicine,	253 Springer Optimization and Its Applications 26,  Springer Science+Business Media LLC 2009
pp. 253--290.}
\end{quote}

Our purpose is to extend this observation to other ICA models that do not rely on the maximization of cumulants, but rely on the concepts of independence (and thus, on the Darmois-Skitovitch theorem).

\todoc[inline]{This is a rather naive introduction. Browing the above-mentioned paper, it seems that at least Theis and Georgiev are well aware that identifiability is an algebraic question. A more thorough overview of the literature will be needed.
Note also that ICA is a special case of the ``Blind-Source Separation'' problem and these guys talk about at least two other interesting cases: Nonnegative matrix factorization and sparse component analysis.
Furthermore, the above ICA problem is the simplest of its kind. More interesting (and difficult) cases are when the number of observed components is smaller than the number of sources (underdetermined problem) or when there is noise added to the observations.
}

\section{Independence of Components}
\label{sec:IndeofComp}

The purpose of this section is to define a natural notion of independence for functions.
Let $f_1,f_2: \natural \ra \real$ be real-valued functions from the set of natural numbers.
Then, a natural definition for $f_1$ and $f_2$ being independent is that if 
$\nu_{t,i}$ is the empirical distribution of the values $(f_i(0),\ldots,f_i(t-1))$ over the reals
and $\nu_t$ is the empirical distribution of the values of the pairs $( (f_1(s),f_2(s))_{0\le s \le t-1})$
then $\nu_t \approx \nu_{t,1} \otimes \nu_{t,2}$ and we expect equality to hold as $t\ra\infty$.
We also see that it is natural to expect $\nu_t$, and also $\nu_{t,i}$ ($i=1,2$) to converge as $t\ra\infty$.

Since the choice to consider functions over the natural numbers is somewhat arbitrary, we will generalize these notions to arbitrary measurable domains $(\Omega,\AA)$ and a sequence of measures $(\mu_t)_{t=0,1,\ldots}$ over it.
If $\Omega = \natural$, $\AA = 2^\natural$ and $\mu_t$ is the uniform probability distribution on $\{0,\ldots,t\}$.
In what follows we fix $\Omega$ and $(\mu_t)$ and define all the concepts that follow relative to these.

Let $f: \Omega \ra \real^d$.
For $t\ge 0$, we define $\nu_t$ to be the probability measure over $\real^d$ that is induced by $f$ and $\mu_t$:
In particular, for any Borel set $A\subset \real^d$,
\[
\nu_t(A)=\int \ind{f(x) \in A}\, d\mu_t(x) = \mu_t\Bigl( \cset{x}{f(x)\in A} \Bigr)
= \mu_t ( f^{-1}(A) ).
\]
In short, we will call $\nu_t$ the $t$th probability measure induced by $f$.
Remember that a sequence of probability measures $(\nu_t)$ is said to converge weakly to a probability measure $\nu$ if $\int h d\nu_t$ converges to $\int h d\nu$ for any bounded, continuous function $h$.

\begin{definition}
A function $f:\Omega \rightarrow \real^d$ is called \emph{ergodic} w.r.t. $(\mu_t)$
if the sequence of  induced probability measures $(\nu_t)_{t\ge 0}$ is weakly convergent.
\end{definition}
\todoc{Why ``ergodic''? A similar concept from dynamical systems is ``asymptotically mean stationary'', see Definition 325 in Chapter 23 of the notes at \url{http://www.stat.cmu.edu/~cshalizi/754/}.
In (nonhomogeneous) Markov chains, the similar property is called weak ergodicity.
This paper: ``Jialin Hong, R. Obaya, A.M. Sanz, Ergodic solutions via ergodic sequences, Nonlinear Anal., TMA 40 (2000), pp. 265-277. '' also introduces ergodicity of functions. Our definition is different. They only require the time averages exists.
}
In what follows, when $(\mu_t)$ is fixed, we will simply call $f$  ergodic, not mentioning $(\mu_t)$.
When the limit of $(\nu_t)$ exists, it will be denoted by $\nu$
and if we want to emphasize the dependence on $f$ then we will use $\nu^{(f)}$.
Similarly, when the dependence on $\nu_t$ on $f$ is important, we will use $\nu^{(f)}_t$.
A simple example of an ergodic function is a function over the reals (positive reals, natural numbers, etc.) that is periodic. 
\todoc[inline]{Somewhere we should make the remark that this framework is a generalization of the standard stochastic framework where you can take $\mu_t = \mu$.
And in fact, in some sense what we try to overcome here is that over the real line there is no uniform probability measure; in some sense we are building the probability theory of improper measures..?
}


The next question to be investigated is whether the space of ergodic functions is a subspace, i.e., whether it is closed under addition and multiplication by reals. The latter is clear. However, concerning closeness under addition, we have 
the following observation:
\begin{claim}
The space of ergodic functions is not closed under addition.
\end{claim}
\begin{proof}
Let $f,g:\natural \rightarrow \{-1,+1\}$ and $(\mu_t)$ be defined as follows: 
\[
f(t) = \begin{cases} +1, & t \equiv 0 \pmod 4; \\
					-1, & t \equiv 1 \pmod 4; \\
					+1,  & t \equiv 2  \pmod 4; \\
					-1, & t \equiv 3  \pmod 4,
					\end{cases}
\qquad\qquad 
g(t) = \begin{cases} +1, & t\equiv 0 \pmod 4; \\
					-1, & t\equiv 1 \pmod 4; \\
					-1,  & t\equiv 2 \pmod 4; \\
					+1, & t\equiv 3 \pmod 4.
					\end{cases}
\]
\[
\quad \mu_{2t+1}(x) = \begin{cases} 1/2, & x=0; \\
					1/2, & x=1; \\
					0,  & \text{otherwise},
					\end{cases}
\qquad\qquad 
\mu_{2t}(x) = \begin{cases} 1/2, & x=2; \\
					1/2, & x=3; \\
					0,  & \text{otherwise} .
					\end{cases}
\]
Then, $\nu^{(f)}_t$, $\nu^{(g)}_t$ are both the uniform  distribution on $\{-1,+1\}$, hence they converge and the limit is the same uniform  distribution. 
However, while $\nu^{(f+g)}_{2t+1}$ is the uniform  distribution on $\{-2,+2\}$,  $\nu^{(f+g)}_{2t}$ is the degenerate
distribution that puts all the mass at $0$. Thus, $\nu^{(f+g)}_{t}$ fails to be convergent. 
\end{proof}
Notice that the above example shows that $\nu_t^{(f)}$  and $\nu_t^{(g)}$ alone do not determine $\nu_t^{(f+g)}$ (which is not that surprising).
\todoc[inline]{What happens when $\Omega = \natural$ and $\mu_t$ is uniform on $\{0,1,\ldots, t\}$?
}
While the space of ergodic functions is not closed under addition, the space spanned by the components of an ergodic function is a space of ergodic functions, as in the following proposition. 
Such observation lead to our definition of independence of components, and build the foundation of the rest work of our paper.
\begin{prop}\label{prop:comp}
$f = (f_1,\ldots, f_d)^{\top}$ is a $d$-dimensional ergodic function, if and only if 
 for any $m\times d$ matrix $A$, $g = A f$ is an $m$-dimensional ergodic function.
\end{prop}
For the proof, we will need the Cram\'er-Wold theorem.
\newcommand{\BB}{\mathcal{B}}
%\begin{thm}[\citep{cramer1936some}]
%Let $F_1$ and $F_2$ be probability distributions over the Borel-sets $\BB_d$ of $\real^d$,
% and $t\in \real^d\setminus\{0\}$ be some nonzero $d$-dimensional vector.
%Then, $F_1=F_2$ if and only if $F_1(S_{t,z}) = F_2(S_{t,z})$ holds for all $z\in \real$,
%	where, for a given $z\in \real$, $S_{t,z} = \cset{ x\in \real^d }{t^\top x \le z }$. 
%\end{thm}
Fix a Borel probability distribution $F$ over $\real^d$, $t\in \real^d \setminus \{0\}$. Then,
the Borel probability distribution $F^{(t)}: \BB_1 \ra [0,1]$ defined using $F^{(t)}(S) = F( \cset{x\in \real^d}{t^\top x\in S} )$ is called
a one-dimensional projection of $F$ (with the projection specified using $t$). 
(We denote by $\BB_d$ the $\sigma$-algebra of Borel-sets of $\real^d$.)
%The previous result states that a $d$-dimensional Borel distributions is uniquely determined by its one dimensional projections.
%From this, the following result, which will be most useful to us, follows immediately:
\begin{thm}[\citealt{cramer1936some}]
\label{cor:cw}
Let $F$, $F_1,F_2,\ldots$ be Borel probability distributions over $\real^d$.
Then $F_t$ weakly converges to $F$ if and only if all the one-dimensional projections of $F_t$ weakly converge to the corresponding one-dimensional projection of $F$.
\todoc{I could not quite find this result in the cited paper, but it will surely follow from it. But maybe a better citation would be useful, e.g., to some probability book.}
\end{thm}
This result is stated and proved, for example, in the book of \citet{AtLa06}
as  Theorem 10.4.5.
\begin{proof}[Proof of \cref{prop:comp}] %\todoc{I think given the above corollary, this proof should be simplified.}
The converse direction is straightforward by picking $A$ to be the $d\times d$ identity matrix.

Now assume that $f= (f_1,\ldots, f_d)^{\top}$ is ergodic, thus $\nu_t^{(f)}: \mathcal{B}_d \rightarrow [0,1]$ weakly converges to $\nu^{(f)}$. Let $A\in \real^{m\times d}$ and $g = A f$.
 We need to show that $\nu_t^{(g)}$ weakly converges to some distribution $\nu^{(g)}$.
In particular, we also claim that $\nu^{(g)}(S) = \nu^{(f)}( \cset{x\in \real^d}{Ax \in S} )$, $S\in \BB_m$.
By \cref{cor:cw}, for this it suffices to show that the one-dimensional projections of $\nu_t^{(g)}$ weakly converge to the one-dimensional projection of $\nu^{(g)}$.
%Without loss of generality, we may assume that $A$ is non-singular. \todoc{Explain why..}
Clearly, any one-dimensional projection of $\nu_t^{(g)}$, defined by say $t\in \real^m$,
is a one-dimensional projection of $\nu_t^{(f)}$ defined by $A^\top t$ and the same also holds for $\nu^{(g)}$ and $\nu^{(f)}$.
By \cref{cor:cw}, the one-dimensional projections of $\nu_t^{(f)}$ weakly converge to the corresponding one-dimensional projections of $\nu^{(f)}$, hence the same holds
for the pair $(\nu_t^{(g)}, \nu^{(g)})$, finishing the proof.
\if0
Denote by $\bX_t$ a $d$-dimensional random vector with distribution $\nu_t^{(f)}$ and 
	let $\bX$ be a $d$-dimensional  random vector with distribution $\nu^{\top}$. 
Let $\bY_t =  A\bX_t$ and $ \bY =  A\bX$. 
By definition, $\nu_t^{(g)}$ is the distribution of $A\bX_t$,
while $\nu^{(g)}$ is the distribution of $A\bX$. 

To prove that $\bY_t$ weakly converges to $\bY$, by the Cram\'{e}r-Wold Theorem we only need to prove that 
for any $m$-dimension vector $\alpha = (\alpha_1,\ldots,\alpha_m)$, 
$\alpha\bY_t$ weakly converges to $\alpha\bY$. \todoc{Why does the convergence of $\bY_t$ to $\bY$ follows from this result?}
Note that again by the Cram\'{e}r-Wold Theorem 
\[
\alpha\bY_t = (\alpha A) \bX_t \xrightarrow{\mathcal{L}}  (\alpha A) \bX = \alpha \bY\,,
\]
finishing the proof.
\fi
\end{proof}

We define the independence of the components of a multidimensional ergodic function $f$ based on its induced limiting distribution $\nu^{(f)}$:
\begin{definition}
Given an ergodic function $f = (f_1,\ldots,f_d)^{\top}$, 
we say that $\{f_1,\ldots,f_d\}$ are \emph{independent} components, 
	if $\nu^{(f_1)}\otimes\ldots\otimes\nu^{(f_d)} = v^{(f)}$, 
	where $f = (f_1,\ldots,f_d):\Omega \rightarrow \real^d$.
	\if0
	We say $\{f_1,\ldots,f_d\}$ are pairwise independent, if $\{f_i,f_j\}$ are independent components of the function $\tilde{f}_{ij} = (f_i,f_j)^{\top}$ for any $1\le i\neq j \le d$. 
	\fi
\end{definition}

\begin{remark}
Note that the existence of $\nu^{(f_i)}$ 
%and $\nu^{(\tilde{f}_{ij})}$ are
is guaranteed by \cref{prop:comp}. 
The definition of independent components is also extended to the independence of ergodic functions. 
For example, $f$ and $g$ are called independent, if $(f,g)$ is an ergodic function with independent components.
% by requiring that the joint function of all the functions is ergodic with independent components. 
\todoc{This is a bit unclear.}
\end{remark}
Do there exist ergodic functions that have independent (or dependent) components? 
Here, we provide a few examples based on periodic functions.
Let $m$ denote the Lebesgue measure over the reals.
\begin{prop}
Assume that $f = (f_1,f_2)^{\top}:\real\rightarrow\real^2$ is an ergodic function, and both $f_1$ and $f_2$ are periodic with respective periods $T_1$ and $T_2$. 
Let $\mu_t$ be the uniform probability distribution on $[0,t]$. 
%$\mu_t((a,b)) = (b-a)/t$ for $0\le a\le b\le t$.
 If $T_1/T_2$ is irrational, then $f_1$ and $f_2$ are independent. 
\end{prop}
%\vspace{-0.5cm}
\begin{proof}
 For two Borel sets $A\subset \real$ and $B\subset \real$, let $S_1 =f_1^{-1}(A)\cap [0,T_1] $ and $ S_2 = f_2^{-1}(B)\cap [0,T_2]$. It is easy to verify that $\nu^{(f_1)}(A) = \frac{m(S_1)}{T_1}$, and $\nu^{(f_2)}(B) = \frac{m(S_2)}{T_2}$. 
 Note that $\nu^{(f_1)}_t(A\times B) = \frac{m( f_1^{-1}(A)\cap f_2^{-1}(B)\cap [0,t] )}{t}$. Now assume that $nT_1 \le t< (n+1)T_1$ and $mT_2\le t <(m+1)T_2$. Thus,
 \begin{align*}
 \nu^{(f_1)}_t(A\times B) =  \frac{1}{t} \int_{ \substack{x\in f_1^{-1}(A) \\ 0\le x \le t}} \ind{x\in f_2^{-1}(B)}\, dx 
 					=	 \frac{1}{t} \sum_{u=0}^{n-1} \int_{S_1} \ind{uT_1+s \in f_2^{-1}(B)}\, ds + \frac{C}{t}
 \end{align*}
 for some constant $0\le C<T_1$. Letting $t\ra\infty$, we get
 \begin{align*}
  \lim_{t\rightarrow \infty} \nu^{(f_1)}_t(A\times B) 
  &=  \int_{S_1} \lim_{n\rightarrow \infty} \sum_{u=0}^{n-1} \frac{\ind{uT_1+s \in f_2^{-1}(B)}}{n T_1} \, ds,
 \end{align*}
 where the interchange of the limit and the integration is justified by Lebesgue's dominated theorem, once we show that the integrands converge.
To see this, fix $s\in \real$. Then,
 $\lim_{n\rightarrow \infty}\frac1n \sum_{u=0}^{n-1} \ind{uT_1+s \in f_2^{-1}(B)}
 =\lim_{n\rightarrow \infty}\frac1n | \cset{ 0\le u \le n-1 }{ f_2(uT_1+s) \in (B)}
 = \nu^{(f_2)}(B)$ since thanks to $T_1/T_2$ being irrational, by the equidistribution theorem of Weyl,
the empirical distribution of $(u T_1+s \pmod T_2 )_{u\in \natural}$ is uniform on $[0,T_2)$.
 \todoc{We can cite P\'olya-Szeg\"o. }
Thus, the interchange is justified and
 \begin{align*}
 \lim\limits_{n\rightarrow \infty} \nu^{(f_1)}_t(A\times B) = & \int_{x\in S_1} \frac{\nu^{(f_2)}(B)}{T_1}\, dc 
 														=  \frac{m(S_1)\nu^{(f_2)}(B)}{T_1} 
 														=  \nu^{(f_1)}(A)\nu^{(f_2)}(B),
 \end{align*}
which shows that  $f$ and $g$ are independent.
\end{proof}

When the ratio of the periods of two periodic functions is rational, the two functions may or may not be independent of each other:
\begin{prop}
Assume that $f = (f_1,f_2)^{\top}:\real\rightarrow\real^2$ is an ergodic function, and both $f_1$ and $f_2$ are periodic with respective periods $T_1$ and $T_2$. 
Let $\mu_t$ be the uniform probability distribution on $[0,t]$.   
If $T_1/T_2 = \frac{u}{v}$ is rational, then $f$ and $g$ are independent iff 
\[
\frac{m(\{f^{-1}(A)\cap g^{-1}(B) \cap [0,vT_1]\})}{v} = \frac{m(f^{-1}(A)\cap [0,T_1]) \,m(g^{-1}(B)\cap [0,T_2])}{T_2}
\]
for any Borel sets $A,B\subset \real$. \todoc{Can this condition be satisfied? Example?}
\end{prop}
The proof of the above proposition simply follows the definition of independent component. An example of the above proposition is:
$f=(f_1,f_2)^{\top}:\natural \rightarrow \{-1,+1\}^2$: 
\[
f_1(t) = \begin{cases} +1, & t\in(n,n+1/2]; \\
					 -1, & t\in(n+1/2,n+1]; 
					\end{cases}
\qquad\qquad 
f_2(t) = \begin{cases} +1, & t\in(n,n+1/4]; \\
					 -1, & t\in(n+1/4,n+3/4];\\
				  	 +1, & t\in(n+3/4,n+1];
					\end{cases}
\]
for any $n\in\natural$. It is easy to verify that $f$ is ergodic with independent components.
\todoc{Proof?}

When the domain of the function is discrete (e.g., the set of natural numbers),
	the condition in the previous result can be simplified. This simplified result clearly shows that the condition of the result
	is non-vacuous.
\begin{prop}
Assume that $f = (f_1,f_2)^{\top}:\natural\rightarrow\real^2$ is an ergodic function, and both $f_1$ and $f_2$ are periodic with respective periods $T_1$ and $T_2$. 
Let $\mu_t$ be the uniform probability distribution on $\{0,1,\ldots,t-1\}$.   
Then $f$ and $g$ are independent iff
\begin{align*}
|\cset{x\in\natural}{ f(x) = a, g(x) = b, 1\le x \le T_1T_2 }| = & \, |\cset{x\in\natural}{ f(x) = a, 1\le x \le T_1 }| \\
& \quad \times |\cset{x\in\natural}{ g(x) = b, 1\le x \le T_2 } |
\end{align*}
for any $a,b\in \real$. \todoc{Maybe we should use $0\le x \le T-1$ in the above condition everywhere?}
\end{prop}

In fact, the induced distribution of a sum of two independent components is the convolution of the induced distributions of the two component.
To state the result, let us remind the reader of the definition of the convolution $\nu*\mu$ of two distributions, 
 $\nu,\mu:\BB_d \ra [0,1]$:
For $S\in \BB_d$, 
 $(\nu* \mu)(S) = \int \nu( S-x) d\mu(x)$, where $S - x$ is the translation of $S\in \BB_d$ by $-x$, i.e., $S-x =\cset{y-x}{y\in S}$.

\if0
While the space of ergodic functions is not closed under addition, the sum of two independent ergodic functions is also ergodic and in fact its induced distribution is the convolution of the induced distributions of the two functions:
\fi
\begin{prop}
\label{prop:convolution}
If $(f,g)$ is ergodic with independent components then
$f+g$ is ergodic and
 $\nu^{(f+g)} = \nu^{(f)} * \nu^{(g)}$,
 where for $\nu,\mu: \BB_d \ra [0,1]$.
\end{prop}
\begin{proof}
The ergodicity of $f+g$ immediately follows from \cref{prop:comp}, i.e., $\nu^{(f+g)} = \lim\limits_{t\ra \infty} \nu_t^{(f+g)}$ exists.

Denote by $(X_{t,1},X_{t,2})^{\top}$ (and $(X_1,X_2)^{\top}$) the random vector with distribution $\nu_t^{((f,g)^{\top})}$ (and $\nu^{((f,g)^{\top})}$ respectively). 
Note that for any $t\ge0 $, $\nu_t^{(f+g)}$ is the probability distribution of $X_{t,1}+X_{t,2}$,
which by \cref{cor:cw} will converge to the distribution of $X_1+X_2$. By our assumption on $(f,g)$, $X_1$ and $X_2$ are independent with respective distributions $\nu^{(f)}$ and $\nu^{(g)}$, so the distribution of $X_1+X_2$ is $\nu^{(f)}*\nu^{(g)}$. Thus,
\[
\nu^{(f+g)} = \lim_{t\ra \infty} \nu_t^{(f+g)} = \nu^{(f)} * \nu^{(g)}\,.
\]  
\end{proof}
The result can be easily extended to any number of independent functions.
\if0
Consider the sequence of empirical measures over $\real^d$ introduced by $f:\natural \rightarrow \real^d$: for any $t\ge 1$, let $\nu_t$ denote the a probability measure over $\real^d$ such that
for $ $ any Borel set $A \in \real^d$, $\nu_t(A)=\tfrac{1}{t} \sum_{k=1}^t \ind{f(k) \in A}$. 
\fi

\subsection{Will the limiting distribution be a probability distribution?}
In this section, we provide a sufficient condition under which all the cluster points of the induced distributions $(\nu_t)$ will be probability distributions. In particular, we have the following result:
\begin{prop}
\label{lem:ergodicfunction}
Let $f_i$ denote the $i$th coordinate function of $f$.
Assume that
\begin{equation}
\label{eq:ergodicproperty}
\lim_{t\to\infty} \int |f_i(x)|\, d\mu_t(x) 
\end{equation}
exists and is finite for all $1 \le i \le d$ and that $\esssup_{\mu_t} |f_i|<\infty$ for any $1\le i \le d$ and $t\ge 0$.
Then all limit points of the sequence $\{\nu_t\}$, with respect to the weak topology, are probability measures over $\real^d$.
\end{prop}
\todoc{Relationship to the Vitali-Hahn Theorem? Proposition~327 in \url{http://www.stat.cmu.edu/~cshalizi/754/}?}
As we we will soon see, the statement of the lemma will follow from Prokhorov theorem.
For this, we need to introduce the concept of a ``tight'' set of probability measures.
Let $(X,\TT)$ be a topological space, $\AA$ be a $\sigma$-algebra over $X$ that contains the topology $\TT$,
$\KK$ a set of probability measures over $\AA$. Then $\KK$ is called \emph{tight} if for any $\eps>0$ there exists a compact set $K$ of $X$ such that for all measures $\mu \in \KK$, $\mu(K)\ge 1-\eps$. The intuitive idea behind tightness is that the measures ``do not escape to infinity''.
\begin{thm}[Prokhorov's Theorem]
Let $(S,\rho)$ be a separable metric space, $\KK$ be a set of probability measures over the Borel $\sigma$-algebra $\AA$ of $S$.
Then $\KK$ is tight iff the closure of $\KK$ is sequentially compact in the space of probability measures over $\AA$ equipped with the topology of weak convergence.
\end{thm}
An immediate corollary of this theorem is that if $\KK$ is tight than any cluster point of $\KK$ is also a probability measure.
With this, let us continue with the proof of \cref{lem:ergodicfunction}.
\begin{proof}
If we show that the set of probability measures $\{\nu_t\}$ is tight then, as suggested beforehand,
the statement will indeed follow from Prokhorov's theorem.
Denote by $m_i\in \mathbb{R}$ the limit of $ m_{t,i} = \int |f_i(x)|\, d\mu_t(x)$.
Pick any $\eps>0$.
By the convergence of $m_{t,i}$, there exists some $T>0$ such that
\begin{equation}
\label{eq:converge}
\left| \int |f_i(x)|\, d\mu_t (x)- m_i \right| <\eps
\end{equation}
for all $1 \le i \le d$ and $t>T$. Let $u_{i}=\tfrac{m_i+\eps}{\eps}$. Then the marginals
$\nu_{t,i}$ of $\nu_t$, $1\le i \le d$, satisfy $\nu_{t,i}([-u_{i},u_{i}]) > 1-\eps$ for all $1\le i\le d$ and $t>T$, otherwise \eqref{eq:converge} would be violated. 
Indeed, $\nu_{t,i}([-u_i,u_i]^c)=\int_{|f_i(x)|>u_i} 1\cdot d\mu_t(x)
\le \frac{1}{u_i}\int_{|f_i(x)|>u_i} |f_i(x)| d\mu_t(x)
\le \frac{1}{u_i} \int |f_i(x)| d\mu_t(x) \le \frac{m_i+\eps}{u_i}\le \eps$.

Let $F_{t,i} = \esssup_{\mu_t} |f_i|$ be the essential supremum of $|f_i|$ w.r.t. $\mu_t$.
Define 
$K=\prod_{i=1}^d [-u'_{i},u'_{i}]$ where $u'_{i}=\max\{u_{i},F_{1,i}, \ldots,F_{T,i}$.
We claim that 
$\nu_t(K)>1-d \eps$ for all $t \ge 1$, which would show that $\{\nu_t\}$ is tight.
To lower bound the $\nu_t$-measure of $K$, we upper bound the measure of its complement:
$\nu_t(K^c) \le \nu_t( \cup_{1\le i \le d} \real^{i-1} \times [-u_i',u_i']^c \times \real^{d-i})
\le \sum_{i=1}^d \nu_t( \real^{i-1} \times [-u_i',u_i']^c \times \real^{d-i})
=   \sum_{i=1}^d \nu_{t,i}( [-u_i',u_i']^c )
\le d \eps$, where the last inequality follows from $u_i'\ge u_i$ and our choice of $u_i$ when $t>T$,
while it follows from $u_i'\ge \esssup_{\mu_t} |f_i|$ when $t\ge T$.
\end{proof}


\if0
\begin{lemma}
\label{lem:ergodicfunction}
Let $f_i(t)$ denote the $i$th coordinate of $f(t)$, and assume that
\begin{equation}
\label{eq:ergodicproperty}
\lim_{t\to\infty} \tfrac{1}{t}\sum_{k=1}^t |f_i(k)|=m_i
\end{equation}
exists and is finite for all $1 \le i \le d$.
Then all limit points of the sequence $\{\nu_t\}$, with respect to the weak topology, are probability measures over $\real^d$.
\end{lemma}
\begin{proof}
The statement of the lemma follows by Prokhorov's theorem if we show that the set of probability measures $\{\nu_t\}$ is tight.
By the convergence of $|f_i(t)|$, for any $\eps>0$ there exists a $T>0$ such that
\begin{equation}
\label{eq:converge}
\left| \tfrac{1}{t} \sum_{k=1}^t |f_i(k)| - m_i \right| <\eps
\end{equation}
for all $1 \le i \le d$ and $t>T$. Let $u_{T,i}=\tfrac{m_i+\eps}{\eps}$. Then the marginals
$\nu_{t,i}$ of $\nu_t$, $1\le i \le d$, satisfy $\nu_{t,i}([0,u_{T,i}]) > 1-\eps$ for all $i,t$ considered, otherwise \eqref{eq:converge} would be violated. Thus, the compact set
$K_{\eps}=\prod_{i=1}^d [0,u'_{T,i}]$ with $u'_{T,i}=\max\{u_{T,i},|f_i(1),\ldots,|f_i(T)|\}$ satisfies
$\nu_t(K_{\eps})>1-d \eps$ for all $t \ge 1$ by the union bound, showing that $\{\nu_t\}$ is tight.
\end{proof}
\fi
%Note that \cref{lem:ergodicfunction} guarantees that all the convergent subsequences of $\{\nu_t\}$ will converge to probability measures. The results of this paper depend on a stronger assumption, as follows.


\section{Identifiability conditions}
In this section we present a set of results that guarantee identifiability of the sources
based on an observed mixture, given that the sources are independent.

\subsection{Identifiability based on the Darmois-Skitovich theorem}
Our first result requires the minimal assumption that each component $f_i$ is non-Gaussian:%
\footnote{
In fact, as a multidimensional standard normal distribution is invariant under orthogonal transformations, if more than two components had a Gaussian distribution, identifying the mixing matrix would be impossible. \todoc[inline]{Now, can the result be extended to allow one Gaussian? Or restrict identifiability to the non-Gaussian components?}
}
\begin{thm}
\label{thm:CorofICA}
Let $f = (f_1,\ldots,f_d)^{\top}$ be an ergodic function with independent components and %\todoc{We always have real-valued functions, right? So maybe mention this always or nowhere} 
assume that neither of $\nu_{f_i}$, $i=1,\ldots,d$ is a Gaussian distribution. If
\begin{equation}
\left(
\begin{array}{ccc}
g_1 \\
\vdots \\
g_d
\end{array}
\right) = A
\left(
\begin{array}{ccc}
f_1 \\
\vdots \\
f_d
\end{array}
\right)
\end{equation}
are independent, then $A$ is equivalent to a permutation matrix up to scaling.
\end{thm}

The proof of \cref{thm:CorofICA} is based on the celebrated Darmois-Skitovich theorem:
\begin{thm}[Darmois-Skitovich theorem, \citep{Darmois53,Skitovich53}]
Let $\xi_1,\ldots,\xi_d$ be independent random variables. If the linear forms $L_1 = \alpha_1\xi_1 + \ldots + \alpha_d\xi_d$ and $L_2 = \beta_1\xi_1 +\ldots + \beta_d\xi_d$, where the coefficients $\alpha_i$ and $\beta_i$ are nonzero, are independent, then the random variables $\xi_i$ $(1\le i \le d$) are Gaussian variables.
\end{thm}
Although the theorem is usually stated in the above form, it is clear that it is a statement that expresses the relationship between certain distributions. In particular, the theorem can be rewritten with no mention of random variables at all. We do this as this will be immediately useful for us:
%For the theorem, introduce the operator $M_c$ for $c\in \real\setminus \{0\}$ that acts on probability distributions and is defined by $(M_c \nu)(A) = \nu( \cset{ x/c}{x\in A} )$.
\begin{prop}[Darmois-Skitovich theorem -- distributional form]
\label{prop:Darmois-Skitovich-distr}
For $i=1,\ldots, d$, let $\nu_i$ be probability distributions over the reals.
Fix the nonzero real numbers $(\alpha_i)$, $(\beta_i)$.
Define the probability distributions $\nu^{(L_1)}$, $\nu^{(L_2)}$ over the reals and 
the probability distribution
 $\nu^{(L_1,L_2)}$ over $\real^2$ using
\begin{align*}
\nu^{(L_1)}(A) & = \int \ind{ \sum_i \alpha_i x_i \in A} \,d\nu_1(x_1)\dots d\nu_d(x_d)\,,\\
\nu^{(L_2)}(B) & = \int \ind{ \sum_i \beta_i x_i \in B} \,d\nu_1(x_1)\dots d\nu_d(x_d)\,,\\
\nu^{(L_1,L_2)}(A\times B) 
& = \int \ind{ \sum_i \alpha_i x_i \in A, \sum_i \beta_i x_i\in B} \,d\nu_1(x_1)\dots d\nu_d(x_d)\,,
\end{align*}
where $A,B\subset \real$ are Borel sets.
If $\nu^{(L_1,L_2)} = \nu^{(L_1)} \otimes \nu^{(L_2)}$ then $\nu_i$ ($1\le i \le d$) are Gaussian distributions.
\end{prop}
From this result, we see that the following holds true:
\begin{lemma}
Let $f = (f_1,\ldots,f_d)^{\top}$ be an ergodic function with independent components and fix some nonzero reals
 $(\alpha_i)$, $(\beta_i)$.
Let $g = \sum_i \alpha_i f_i$, $h = \sum_i \beta_i f_i$.
If $g$ and $h$ are independent  then $\nu_{f_i}$, $i=1,\ldots,d$ are Gaussians.
\end{lemma}
\begin{proof}
We apply~\cref{prop:Darmois-Skitovich-distr} with $\nu_i = \nu^{(f_i)}$, $i=1,\ldots,d$.
We claim that $\nu^{(g)} = \nu^{(L_1)}$, $\nu^{(h)} = \nu^{(L_2)}$,
$\nu^{(g,h)} = \nu^{(L_1,L_2)}$.
From this the result follows since if $g$ and $h$ are independent then $\nu^{(L_1,L_2)} = \nu^{(g,h)} = \nu^{(g)} \otimes \nu^{(h)} = \nu^{(L_1)} \otimes \nu^{(L_2)}$.
The claimed equalities follow by \cref{prop:convolution}. \todoc{At least I think..}
\end{proof}

As an immediate corollary, we have the following result:
\begin{cor}
\label{cor:DSTheoremCor}
Let $f_1,\ldots,f_d$ be independent, ergodic functions such that $\nu^{(f_i)}$ are not Gaussian distributions.
Assume further that functions $g = \alpha_1 f_1 + \ldots + \alpha_d f_d$ and 
$h= \beta_1f_1 +\ldots + \beta_d f_d$ are also independent.
Then, $\cset{i}{ \alpha_i \neq0} \cap \cset{j}{ \beta_j \neq0} = \emptyset$.
\end{cor}
Now, the proof of  \cref{thm:CorofICA}  follows immediately from this result:
\begin{proof}[Proof of \cref{thm:CorofICA}]
From \cref{cor:DSTheoremCor} it follows that $\cset{j}{ a_{ij}\ne 0 }$, $1\le i \le d$,
 are $d$ non-empty mutually exclusive subsets of the size $d$ set $\{1,\ldots,d\}$.
 Hence, these sets must form a partition of the set, with one element in each subset, which proves the theorem.
\end{proof}

%\subsection{Identifiability based on kurtosis maximization}
%(Delfosse & Loubaton, 1995)

%\subsection{Negentropy minimization}
\subsection{How Difficult ICA problem is?}
We would like define the difficulty of the ICA problem. In practice, we are not going to observe the true distribution of $g$, but $g(t)$, $1\le t\le n$. Note if $f$ is a continuous function, then $f(t)$, $1\le t\le n$ would never to be of independent components.
Now assume that we have a distribution $f'$, and a true $A$, thus we are going to observe $g'=Af'$. 

Ideally, we would like to claim that:
 for any $D>0$, and any algorithm denoted by $\text{Alg}$ that learns $A$ correctly if $f'$ is an ergodic function with independent components,
 there always exists an ergodic function $f$ with independent components, such that $f'$ and $f$ are 'close', and $d(\hat{A},A)\ge H(D,A,f')$, 
 where $\hat{A}$ is learned by the algorithm $\text{Alg}$ from $g'$, $H(D,A,f')$ is a function of $D$, $A$, and $f'$, and $d(\hat{A},A)$ is some distance between $\hat{A}$ and $A$.  

\subsubsection{Correctness built on independence}
 We try to get to a weaker claim.
 Assume the algorithm is built on maximizing the independence, i.e. $\text{Alg}$ learns a matrix $B$ such that $q(t) = Bg(t) = BAf(t)$ is 'close' to independence, for $1\le t\le n$.
 Now the problem becomes given any number $D>0$, and an ergodic function $f$, there exists a matrix $M$, such that $q = Af$ is 'close' to independent (measure by $D$), while $d(q-f)\ge H(D,A,f)$, where $d(q,f)$ is some distance between $q$ and $f$. 

\subsection{A general observation}
The standard instantaneous noiseless ICA identifiability results can all be phrased as follows:
Let $S$ be a random $d$-dimensional vector, whose components are independent of each other.
Let $X = A S$, where $A$ is a $d\times d$ matrix. Then, given the joint distribution of $X$, assuming that the joint of $S$ satisfies some additional assumption, the matrix $A$ can be identified up to permutation and scaling. \todoc{The previous section has one example; do we need more!?}
What is important to realize that the identifiability results can be expressed in terms of the joint distributions of the source.

Now, it should be clear all these results carry through to the deterministic setting, where the joint distribution of $S$ is replaced by $\nu^{(f)}$, where $f$ is a $d$-dimensional deterministic signal.

\todoc{We should add the generalization of all the other ``characterization'' results or at least the generalization of some of them.}

\section{Analysis of \citet{DHsu2012}'s ICA algorithm}
In this section, we will take the ICA algorithm of \citet{DHsu2012} as an example. 
A detailed analysis shows that the sample procedure plays an important role in the performance. 

\subsection{\citet{DHsu2012}'s algorithm}

Denote the maximal (respectively minimal) singular value of a matrix $A$ by  $\sigma_{\max}(A)$ (respectively $\sigma_{\min}(A)$). Also, let $A_{(2,\min)} = \min_{i} \|A_i\|_2$, $A_{(2,\max)} = \max_{i} \|A_i\|_2$, and $A_{\max} = \max_{i,j} |A_{i,j}|$.
\begin{lemma}
The following properties hold.
\begin{itemize}
\item $A_{(2,\min)} \ge \sigma_{\min}(A)$;
\item $A_{(2,\max)} \le \sigma_{\max}(A)$;
\item $A_{\max} \le A_{(2,\max)}  \le \sigma_{\max}(A)$;
\end{itemize}
\end{lemma}
Introduce the notation $\E_{x\sim \nu}[f(x)]$ as a shorthand for $\int f(x) d\nu(x)$.
Assume the independent ergodic component functions $s_1,\ldots,s_d:\natural \ra \real$ are bounded by a constant $C$, and satisfy $\E_{x\sim\nu^{(s_i)}}[x]=0$ and $\kappa_i := \E_{x\sim \nu^{(s_i)}}[x^4] - 3\left(\E_{x\sim \nu^{(s_i)}}[x^2]\right)^2\neq 0$.
\footnote{In this section, we will denote the sources by $s_i$ to avoid a clash with the function $f$ used by  \citet{DHsu2012} for a different purpose.}
%Here $\E_{\nu_{f_i}}$ is the expectation is taken with respect to the probability measure of $\nu_{f_i}$. 
Note that sample a vector $\psi$ from a unit sphere, $\min_i |\psi^{\top}A_i|\neq 0$ with probability~1.  
We are interested in the behavior of $\min_i |\psi^{\top}A_i|$.
\begin{lemma}
\label{lem:dmin}
$\max_i |\psi^{\top}A_i| \le \max_i\|A_i\|_2 = A_{(2,\max)}$. Also, with probability at least $1/4$, $\min_i |\psi^{\top}A_i| \ge \frac{\sqrt{2\pi}}{2d}A_{(2,\min)}$.  
\end{lemma}
Let $\widehat{A}$ be the scaled $A$ such that $\psi^{\top}\widehat{A} = 1$. Since $A$ can only be recovered up to scaling and permutation, we instead try to recover $\widehat{A}$. 
In the rest of this section we will still denote $\widehat{A}$ by $A$, but keep it in mind that it has been rescaled. 
To maintain the same observations, $s_1,\cdots, s_d$ have to be rescaled by the same factor.  
By Lemma \ref{lem:dmin}, $\kappa_i$ is scaled by a factor of $k_i^4$, where $ \frac{1}{A_{(2,\max)}} \le k_i \le \frac{2d}{\sqrt{2\pi}A_{(2,\min)}}$. We also denote the scaled $\kappa$ by $\tilde{\kappa}$. 
Moreover, the scaled $s_i$ will still be bounded by $\frac{2dC}{\sqrt{2\pi}A_{(2,\min)}}$, denoted by $\tilde{C}$, with probability at least $1/4$. 
Denote the diagonal matrix $\text{diag}(\tilde{\kappa}_1,\cdots,\tilde{\kappa}_d)$ by $\tilde{K}$. 
Also, denote $\max_{i} \tilde{\kappa}_i$ by $\tilde{\kappa}_{\max}$ and $\min_{i} \tilde{\kappa}_i$ by $\tilde{\kappa}_{\min}$.

For $p\ge 1$, $\eta\in \real^d$, 
let 
\begin{equation}
\label{eq:mpdef}
m_p(\eta) = \E_{x\sim \nu^{(s)}}[ (\eta^\top A x)^p ]
\end{equation}
 and let
\begin{equation}\label{eq:fdef}
f(\eta) = \frac1{12} \left( m_4(\eta) - 3 m_2(\eta)^2 \right)\,.
\end{equation}
The following result is proven by \citet{DHsu2012}:
\begin{thm}[\citet{DHsu2012}, Theorem~4]
Assume that $\nu^{(s)} = \nu^{(s_1)}\otimes \dots \otimes \nu^{(s_d)}$,
$\E_{x\sim \nu^{(s_i)}}[x] = 0$, $\E_{x\sim \nu^{(s_i)}}[x^4] \ne 3\left(\E_{x\sim \nu^{(s_i)}}[x^2]\right)^2$, $i=1,\ldots,d$, $A$ is nonsingular. 
Let $m_4,m_2:\real^d \ra \real$ be defined by \eqref{eq:mpdef},
	while $f: \real^d \ra \real$ be defined by \eqref{eq:fdef}.
Let $\phi,\psi\in \real^d$ be vectors from the unit sphere of $\real^d$ such that $\psi^{\top}A = 1^{\top}$. Then, 
	the matrix
\begin{equation}
\label{eq:M}
M =(\nabla^2f(\phi))(\nabla^2f(\psi))^{-1} 
\end{equation}
can be written in the diagonal form
\begin{equation}
\label{eq:M2}
M = A 
\left(
\begin{array}{ccc}
\lambda_1 & & \\ %\left(\frac{\phi^{\top}A_1}{\psi^{\top}A_1}\right)^2 & &\\
    & \ddots & \\
    & & \lambda_d %\left(\frac{\phi^{\top}A_d}{\psi^{\top}A_d}\right)^2\\
\end{array} 
\right) 
A^{-1},
\end{equation}
where $\lambda_i = \left(\phi^{\top}A_i\right)^2$.
\end{thm}
It follows from this theorem that 
if $\phi,\psi$ are chosen independently from the uniform distribution on the unit sphere of $\real^d$, with probability one,
the eigenvalues of  $M$ are all distinct and the corresponding eigenvectors
determine the rows of $A$ up to permutation and scaling. Based on this idea, \citet{DHsu2012} proposed an algorithm as follows, and claimed that it has a provable performance. 

\begin{algorithm}[H]
\caption{ICA algorithm of \citet{DHsu2012} \label{alg:icaHsu}}
\begin{algorithmic}[1]
\INPUT $g(k)$ for $1\le k \le t$. \todoc[inline]{start at zero!?}
\OUTPUT the estimate of the mixing matrix $A$. 
\STATE Sample $\phi$ from the unit sphere of $\real^d$;
\STATE Evaluate $\nabla^2\widehat{f}(\phi)$ and $\nabla^2\widehat{f}(\psi)$, \\
\quad where $\widehat{m_p}(\eta) = \frac{1}{t}\sum_{k=1}^{t} (\eta^{\top}g(k))^p$, and $\widehat{f}(\eta) = \frac{1}{12}\big(\widehat{m_4}(\eta) - 3\widehat{m_2}(\eta)^2 \big)$;
\STATE Compute $\widehat{M} = (\nabla^2 \widehat{f}(\phi))(\nabla^2\widehat{f}(\psi))^{-1}$;
\STATE Compute all the eigenvectors of $\widehat{M}$, $\{\mu_1,\ldots,\mu_d\}$;
\STATE Return $\widehat{A} = (\mu_1,\ldots,\mu_d)$.
\end{algorithmic}
\end{algorithm}
In general, we denote by adding a ``hat'' on the top of a symbol its empirical estimate.
Note that the output of the algorithm entirely depends on the empirical distribution $\nu_t^g$ of $( g(k) )_{1\le k \le t}$ (i.e.,  $\nu_t^g(U) = \frac1t \sum_{k=1}^t \ind{g(k)\in U}$, $U\subset \real^d$). 
In particular, the output depends on 
$\nabla^2 \left(\E_{x\sim \nu_t^g}[(\eta^\top  x)^4 ]\right)$
and $ \nabla^2 \left(\E_{x\sim \nu_t^g}[(\eta^\top  x)^2 ]^2\right)$
 only, where $\eta\in \{\phi,\psi\}$. 
 \todoc[inline]{We expect that any ICA algorithm can be written as a mapping from the empirical distribution to the estimates. If $\FF$ is this mapping, the problem (when analyzing a particular ICA algorithm) is to derive an error estimate for  how close $\FF(\hat{\lambda})$ is to an unmixing matrix as a function of how close $\hat{\lambda}$ is to $\lambda(\cdot)$, $\lambda(U) = \E_{x\sim \nu}[ \ind{Ax\in U} ]$ where $A$ is some non-singular matrix and $\nu$ is some distribution which is the product of its marginals.
 Where should we discuss this?
 }

\subsection{Efficiency} 
We analyze \citet{DHsu2012}'s algorithm in a deterministic setting, so that we can decouple the dependences of the learning error on the sample procedure of $\phi$ and $\psi$, and that on the finite-sample estimation.

Concentration inequalities generally hold (under some boundness conditions) in a probabilistic setting, while in a deterministic one such property is not guaranteed. 
Hence we need some assumptions that the given sequence $g$ does not go arbitrarily wild.
\begin{definition}
Given two distributions $F_1$ and $F_2$ in $\real^d$, $D_k(F_1,F_2) = \sup_{f\in\mathcal{F}} |\int f(x)dF_1(x) - \int f(x)dF_2(x)|$, where $\mathcal{F}$ is the set of monomials up to degree $k$.
\end{definition}  

\if0
\begin{definition}
Given two distributions $F_1$ and $F_2$ in $\real^d$, we call function $D(F_1,F_2)$ a '***' distance, if it satisfies the following properties:
\begin{enumerate}
\item If $\nu_t$ is an empirical distribution of $\nu$, then $\lim_{t\rightarrow \infty} D(\nu_t,\nu) = 0$.
\item there exists a constant $L>0$, such that $ D_4(F_1,F_2) \le L D(F_1,F_2)$.
%\item There exists constant $C_D>0$ such that for any two sequences $Y = \{y_1, y_2,\cdots, y_T\}$ and $\Xi = \{y_1+\eps_1,\cdots, y_T+\eps_T\}$, $D(\nu_T^{Y}, \nu_T^{\Xi}) \le C_D \sum_i \| \eps_i\|_2$. 
\end{enumerate}
\end{definition}
\begin{remark}
It is easy to verify that $D_4$ itself is a '***' distance.
\end{remark}
\fi
The following lemma bounds $\|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta) \|_2$ by $D_4(\nu_t , \nu)$ and $D_2(\nu_t , \nu)$:
\begin{lemma}
\label{lem:nablavariation}
For any $1\le i,j\le d$,
\begin{align*}
\left|\left(\nabla^2 f(\eta)\right)_{i,j} - \left(\nabla^2 \widehat{f}(\eta) \right)_{i,j} \right| \le \|\eta\|_2^2 d^4  A_{(2,\max)}^2A_{\max}^2\xi.
\end{align*}
Thus,
\[
\|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta)  \|_2 \le \|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta)  \|_F\le  \|\eta\|_2^2  d^5 A_{(2,\max)}^2A_{\max}^2\xi,
\]
where $\xi = \left( 6\tilde{C}^2D_2(\nu, \nu_t) + D_4(\nu, \nu_t)\right)$.
\end{lemma}

Let 
\begin{equation}
\label{def:kappa}
\gamma =  \min_{i,j: i\neq j} \left\vert \left(\phi^{\top}A_i\right)^2 - \left(\phi^{\top}A_j\right)^2 \right\vert. 
\end{equation}
The performance of \citet{DHsu2012}'s algorithm depends on this parameter, as shown in the following theorem.

 \begin{thm}
 \label{thm:efficiency}
Let 
 \[ 
 Q= \frac{2d^5A_{(2,\max)}^4A_{\max}^2\tilde{\kappa}_{\max}\sigma_{\max}^2(A) + 2d^5A_{(2,\max)}^2A_{\max}^2\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}{\tilde{\kappa}^2_{\min}\sigma_{\min}^4(A)} \xi.
 \] 
 Assume the following conditions hold:
 \begin{enumerate}
 \item $\widehat{M}$ has distinct eigenvalues;
 \item $\gamma > 4\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} Q$
 \item $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\gamma}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } Q$;
 \item $\xi \le \frac12\frac{\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}{d^5 A_{(2,\max)}^2A_{\max}^2}$.
  \end{enumerate}
 Then there exist a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that
 \[
  \max_{1\le k\le d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4}{\gamma} \frac{\sigma_{\max}^2(A)}{ \sigma_{\min}(A)}Q .
  \]
  and therefore,
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4d}{\gamma} \frac{\sigma_{\max}^2(A)}{ \sigma_{\min}(A)}Q .
 \]
 \end{thm}
\if0 
\begin{thm}
 Assume that $\|g(k)\|_2 \le C$ for $1\le k\le n$, and
 there exists a product measure $\nu^*$, a non-singular matrix $A$ such that
  \[
  \xi = \left(6 \right) \le \min\{1,A^2_{(2,\min)}\} \frac{\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}{d^5 A_{(2,\max)}^2A_{\max}^2}
  \]
\end{thm}
\fi
\subsubsection{The parameter $\gamma$}
The behavior of the parameter $\gamma$ plays an important role in \cite{DHsu2012}'s algorithm. 
In fact, $\gamma$ affects not only the theoretical upper bound in \ref{thm:efficiency}, but also the validity its conditions.
Note that $M$ and $\widehat{M}$ are not symmetric. Thus in practice for the algorithm's output to be meaningful, we need $\widehat{M}$ still at least have $d$ real eigenvalues.
Intuitively, the stability of this property is also close related to the quantity $\gamma$. 
 
This parameter could be bounded from below with constant probability, as follows. We denote this event by $E_{\gamma}$.
\begin{thm}
\label{thm:gammabound}
With probability at least $1/2$,
\[
\gamma \ge \frac{2A_{(2,\min)}\min_{i\neq j}\|A_i-A_j\|_2}{\sqrt{ed}(d^2+d)}.
\]
\end{thm}


\begin{proof}
For any $i$ and $j$,
\[
|(\phi^{\top}A_i)^2 - (\phi^{\top}A_j)^2|  = |\phi^{\top}A_i - \phi^{\top}A_j||\phi^{\top}A_i+\phi^{\top}A_j| 
\]
WLOG, assume that $\phi^{\top}A_i$ and $\phi^{\top}A_j$ are both positive. Thus, with probability at least $1/2$,
\begin{align*}
|(\phi^{\top}A_i)^2 - (\phi^{\top}A_j)^2| &\ge 2 \min_t |\phi^{\top}A_t||\phi^{\top}A_i - \phi^{\top}A_j| \\
& \ge 2 \min_t |\phi^{\top}A_t|\frac{\min_{i\neq j} \|A_i-A_j\|_2}{\sqrt{ed}(d^2+d)},
\end{align*}
where the last inequality is by Lemma C.6 of \citep{anandkumar2012method}.

Thus, with probability at least $1/2$,
\[
\gamma \ge \frac{2\min_i\|A_i\|_2\min_{i\neq j}\|A_i-A_j\|_2}{\sqrt{ed}(d^2+d)}.
\]
\end{proof}

\begin{remark}
Even Theorem \ref{thm:gammabound} provides a probabilistic lower bound for $\gamma$, such lower bound is still vague since that each column of $A$ has been rescaled with different factors, which makes $\min_{i\neq j}\|A_i-A_j\|_2$ unpredictable. 

On the other hand, since $A$ can only be recovered up to scaling, we can instead start from the assumption that $\psi^{\top}A = 1^{\top}$. 
Then this quantity is in fact part of the setting of the problem.
However, such assumption will be difficult to verify.

Except the above Theorem, we have no meaningful analysis of the parameter $\gamma$ yet.
\end{remark}

Now we can establish a probabilistic upper bound for Theorem \ref{thm:gammabound}:
\begin{thm}
 \label{thm:probEfficiency}
Let 
 \[ 
 Q= \frac{2d^5A_{(2,\max)}^4A_{\max}^2\tilde{\kappa}_{\max}\sigma_{\max}^2(A) + 2d^5A_{(2,\max)}^2A_{\max}^2\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}{\tilde{\kappa}^2_{\min}\sigma_{\min}^4(A)} \xi.
 \] 
 Assume the following conditions hold:
 \begin{enumerate}
 \item $\widehat{M}$ has distinct eigenvalues;
 \item $\gamma > 4\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} Q$
 \item $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\gamma}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } Q$;
 \item $\xi \le \frac12\frac{\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}{d^5 A_{(2,\max)}^2A_{\max}^2}$.
  \end{enumerate}
 Then there exist a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that with probability at least $1/2$,
 \[
  \max_{1\le k\le d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{2\sqrt{ed}(d^2+d)}{\min_{i\neq j}\|A_i-A_j\|_2} \frac{\sigma_{\max}^2(A)}{A_{(2,\min)}\sigma_{\min}(A)}Q .
  \]
  and therefore,
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{2d^2\sqrt{ed}(d+1)}{\min_{i\neq j}\|A_i-A_j\|_2} \frac{\sigma_{\max}^2(A)}{A_{(2,\min)}\sigma_{\min}(A)}Q .
 \]
 \end{thm}
 
Another question comes with \cref{thm:efficiency} is when these conditions will be satisfied.
Note that the first condition will be satisfied with probability 1. Now assume that given training samples, we have a fixed $\xi$(and a fixed $Q$) that satisfies the last condition. 
We are more interested in the probability that the other two conditions hold:  
\subsubsection*{{\bf 1: $\gamma > 4\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} Q$ :}} 
%This inequality is weaker than the following one,
%\[
% \frac{A_{(2,\min)}\sigma_{\min}(A)\min_{i\neq j}\|A_i-A_j\|_2}{4\sqrt{ed}d^7\sigma_{\max}(A)A_{\max}^2A_{(2,\max)}^2} 
% >
%  \left( \frac{1} {\sigma_{\min}^2(A)\tilde{\kappa}_{\min}} + 4 \sigma_{\max}^6(A)\tilde{\kappa}_{\max}^3A_{(2,\max)}^2\right)(4\tilde{C}^2+1) D_4(\nu,\nu_t)
%\]

\subsubsection*{{\bf 2: $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\gamma}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } Q$ :}}
%This inequality is weaker than the following one,
%\[
%\frac{A_{(2,\min)}\sigma_{\min}(A)\min_{i,j:i\neq j} \|A_i - A_j\|_2^2 }{16\sqrt{ed}d^7\sigma_{\max}(A)A_{\max}^2A_{(2,\max)}^2}
%>
% \left( \frac{1} {\sigma_{\min}^2(A)\tilde{\kappa}_{\min}}+ 4 \sigma_{\max}^6(A)\tilde{\kappa}_{\max}^3A_{(2,\max)}^2\right)(4\tilde{C}^2+1) D_4(\nu,\nu_t)
%\]

First note that $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\gamma}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } Q$ actually implies  $\gamma > 4\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} Q$, by the fact that $\sigma_{\max}(A) \ge \min_{i,j:i\neq j} \|A_i - A_j\|_2$. 
Moreover, the second condition (or both) will be satisfied when the event in Theorem \ref{thm:gammabound} holds, and 
\[
\min_{i,j:i\neq j} \|A_i - A_j\|_2 >\left( \frac{4\sqrt{ed}(d^2+d)\sigma_{\max}^2(A)}{A_{(2,\min)}\sigma_{\min}(A) } Q\right)^{1/2}.
\]
This inequality is still related to $\psi$ and the rescaled $A$, which makes the probability of its validity vague.

In summary, the efficiency of the algorithm of \citet{DHsu2012} depends on $\min_{i\neq j}\|A_i-A_j\|_2$ which could be difficult to analyze.
%We would expect that the performance is going to depends on $A_{(2,\min)}$. But what about $\min_{i\neq j}\|A_i-A_j\|_2$? The parameter $\gamma$ is introduce in the recovery of the eigenspace $A$. 
To avoid this problem, we modify \citet{DHsu2012}'s algorithm based on an idea from the paper \citet{arora2012provable}, so that the performance of the new algorithm doesn't depend on $\min_{i\neq j}\|A_i-A_j\|_2$.

\subsection{Modified \citet{DHsu2012}'s algorithm}
In this subsection we introduce a new algorithm whose performance does not depends on $\min_{i\neq j}\|A_i-A_j\|_2$. The idea is inspired by the paper of \citet{arora2012provable}.
Similarly, we will still assume $A$ is scaled such that $\psi^{\top}A = 1^{\top}$, 
so $s$ and $\kappa$ is also scaled (increased or decreased) by the same factors, which are between $\frac{1}{A_{(2,\max)}}$ and $\frac{2d}{\sqrt{2\pi}A_{(2,\min)}}$. The modified algorithm is as follows. 
\begin{algorithm}[H]
\caption{Modified algorithm of \citet{DHsu2012} \label{alg:icaModHsu}}
\begin{algorithmic}[1]
\INPUT $g(k)$ for $1\le k \le t$. 
\OUTPUT the estimate of the mixing matrix $A$. 
\STATE Evaluate $\frac{1}{12}\nabla^2\widehat{f}(\psi)$, \\
\quad where $\widehat{m_p}(\eta) = \frac{1}{t}\sum_{k=1}^{t} (\eta^{\top}g(k))^p$, and $\widehat{f}(\eta) = \frac{1}{12}\big(\widehat{m_4}(\eta) - 3\widehat{m_2}(\eta)^2 \big)$;
\STATE Compute $\widehat{B}$ such that $\nabla^2\widehat{f}(\psi) = \widehat{B}\widehat{B}^{\top}$;
\STATE Sample $\phi$ from the unit sphere of $\real^d$;
\STATE Compute $\widehat{T} = \frac{1}{12}\nabla^2 (\widehat{f}(\widehat{B}^{-\top}\phi))$;
\STATE Compute all the eigenvectors of $\widehat{T}$, $\{\mu_1,\ldots,\mu_d\}$;
\STATE Return $\widehat{A} = \widehat{B}\widehat{R}$, where $\widehat{R}$ is the matrix $(\mu_1,\ldots,\mu_d)$.
\end{algorithmic}
\end{algorithm}

Let 
\[
\bar{\xi} = \frac{d^5A_{(2,\max)}^2A_{\max}^2}{\sigma_{\min}^2(A)\tilde{\kappa}_{\min}} \xi
\] 
and 
\begin{equation}
\label{def:gammaR}
\gamma_R =  \min_{i,j: i\neq j} \left\vert \left(\phi^{\top}(R^*R)_i\right)^2 - \left(\phi^{\top}(R^*R)_j\right)^2 \right\vert. 
\end{equation}
The following theorem shows that this algorithm does not depend on $\min_{i\neq j}\|A_i-A_j\|_2$.
\begin{thm}
\label{thm:Modefficiency}
Let 
 \[ 
 Q=  \frac{4d^5}{\tilde{\kappa}_{\min}^2}\xi + 2\bar{\xi},
 \]
 and $\delta_R = \gamma_R - 2Q$. 
 Assume the following conditions hold:
 \begin{itemize}
 \item $\widehat{T}$ has distinct eigenvalues;
 \item $\gamma_R > 4\sqrt{2}Q$;
 \item $\xi \le \frac{\sigma_{\min}^2(A)\tilde{\kappa}_{\min}}{2d^5A_{(2,\max)}^2A_{\max}^2}$ (so $\bar{\xi} \le 1/2$).
 \end{itemize}
There exists a permutation matrix $\Pi$ and constants $\{c_1,\ldots,c_d\}$, such that 
\[
\|\widehat{A}\Pi D(c) - A\|_2 
\le 
\sqrt{2}\sigma_{\max}(A)\tilde{\kappa}_{\max}^{1/2}\left( \bar{\xi} + \frac{4\sqrt{d}}{\gamma_R}Q\right). 
\]
\end{thm}
\begin{remark}
The upper bound now depends on $\gamma_R$ rather than $\gamma$.
Note that it can be bounded from below by $\frac{4}{\sqrt{ed}(d^2+d)}$ with probability at least $1/2$. The proof is similar to the proof of theorem \ref{thm:gammabound}.
\end{remark}

\section{Simulation results}
We investigate the performances of different ICA algorithms in a simulation setting. In particular, four algorithms are tested: 
\begin{itemize}
\item \citet{DHsu2012}'s algorithm(DHsu);
\item Our modified \citet{DHsu2012}'s algorithm (DICA);
\item \citet{anandkumar2012tensordecomposition}'s ICA algorithm (AA);
\item The default FastICA algorithm in the 'ITE' toolbox \cite{szabo12separation} (FICA). 
\end{itemize}

\subsubsection{Data generation}
In the simulation, a common mixing matrix $A$ is generated, then we sample $k$ groups of $n$ observations. 
For each observation $y$, $y = Ax+ 0.3\times\eps$ where the signal $x$ is generated from a product measure of $d$ uniform distributions $\text{Unif}(-\frac12, \frac12)$, and $\eps$ is generated from a standard $d$-dimensional Gaussian distribution. 
All the algorithms will be evaluated on these common $k$ group of observations.
\subsubsection{Error measures}
We measure the performances of the algorithms on three different measures for different purposes.

The first measure is the parameter recover error. in particular, we evaluate the following quantity between the true mixing matrix $A$ and the one returned by the algorithms $\widehat{A}$:
\begin{equation}
\label{equ:parerror}
\min_{\Pi,S} \|\widehat{A}\Pi S - A\|_{\text{Frob}},
\end{equation}
where $\Pi$ is a permutation matrix, and $S$ is a column scaling matrix (diagonal).

The second measure we used is an approximation of Equation \eqref{equ:parerror} proposed by \citet{comon1994independent}. 
Note that to evaluate Equation \eqref{equ:parerror} one has to enumerate all the permutation $\Pi$, which is not affordable in practice for a large dimension $d$. 
This error helps avoid this computation problem. 

The last measure is the divergence between the joint distribution of the recovered signals and the product of its marginal ones. 
This measure is common in practice when the true mixing matrix $A$ is unknown. 
Moreover, this measure can be reduced to the sum of the entropies of its marginal distribution, as shown in \cite{Learned-Miller:2003:IUS:945365.964306}. 
In particular for an output $\widehat{A}$, we evaluate the following quantity:
\begin{equation}
\sum_{i = 1}^{d} \text{Entropy}(\widehat{x_i}) + \log |\widehat{A}|,
\end{equation}
where $\widehat{x} = \widehat{A}^{-1}y$, and $|\widehat{A}|$ is the absolute value of the determine of $\widehat{A}$. We also use the entropy estimation function 'HShannon\_kNN\_k\_estimation' in the 'ITE' toolbox \cite{szabo14information}.

%For each algorithm, we also measure its running time. 
\subsubsection{Results}

\bibliography{DICA}
\bibliographystyle{plainnat}

\appendix
\section{Appendix}
\subsection{Proof of Lemma \ref{lem:nablavariation}}
Note that in \cref{alg:icaHsu}, 
\[
\nabla^2 f(\eta) = G_1(\eta) - G_2(\eta) -2G_3(\eta),
\]
and 
\[
\nabla^2 \widehat{f}(\eta) =\widehat{G_1}(\eta) - \widehat{G_2}(\eta) -2\widehat{G_3}(\eta),
\]
where 
\begin{align*}
& G_1(\eta) = \int (\eta^{\top}Ax)^2Axx^{\top}A^{\top}\,d\nu(x); \\
& G_2(\eta) = \int (\eta^{\top}Ax)^2\,d\nu(x) \int Axx^{\top}A^{\top} \,d\nu(x); \\
& G_3(\eta) = \Big(\int (\eta^{\top}Ax)Ax\,d\nu(x)\Big)\Big(\int (\eta^{\top}Ax)Ax\,d\nu(x)\Big)^{\top}. \\
&\widehat{ G_1}(\eta) = \frac1n\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)^2g(k)g(k)^{\top} = \int (\eta^{\top}Ax)^2Axx^{\top}A^{\top}\,d\nu_t(x); \\
& \widehat{G_2}(\eta) = \frac{1}{n^2}\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)^2 \sum_{k=1}^{n}g(k)g(k)^{\top} = \int (\eta^{\top}Ax)^2\,d\nu_t(x) \int Axx^{\top}A^{\top} \,d\nu_t(x); \\
& \widehat{G_3}(\eta) = \frac{1}{n^2}\Big(\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)g(k)\Big) \Big(\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)g(k)\Big)^{\top} = \Big(\int (\eta^{\top}Ax)Ax\,d\nu_t(x)\Big)\Big(\int (\eta^{\top}Ax)Ax\,d\nu_t(x)\Big)^{\top}.
\end{align*}
\begin{proof}
Without loss of generality, assume $ \|\eta\|_2 = 1$.
Note that all the integral functions of $G_i(\eta)$ or $\widehat{G_i}(\eta)$ are matrices of polynomials in $x$. Thus, we only need to bound its coefficients.
%Also, since $|b_1b_2b_3b_4| \le \frac14(b_1^4+b_2^4+b_3^4+b_4^4)$, we only need to upper bound the coefficients of $x_i^4$. 
Note that 
\[
\left(G_1\right)_{i,j} = \int (\sum_t \eta^{\top}A_tx_t)^2\sum_t A_{i,t}x_t \sum_t A_{j,t}x_t d\nu(x).
\]
Thus, the coefficient of the term $x_{t_1}x_{t_2}x_{t_3}x_{t_4}$ is $\eta^{\top}A_{t_1}\eta^{\top}A_{t_2}A_{i,t_3}A_{i,t_4}$, 
which is bounded by $\max_i |\eta^{\top} A_i|^2 A_{\max}^2 \le A_{(2,\max)}^2A_{\max}^2$. 
Thus,
\[
\left| (G_1)_{i,j} - (\widehat{G_1})_{i,j} \right| \le d^4  A_{(2,\max)}^2A_{\max}^2D_4(\nu, \nu_t).
\]

Similarly, 
\[
\left| \int A_ixx^{\top}A_j^{\top} \,d\nu(x) - \int A_ixx^{\top}A_j^{\top} \,d\nu_t(x) \right| \le d^2 A_{\max}^2 D_2(\nu,\nu_t)
\]
 and 
\[
\left| \int (\eta^{\top}Ax)^2\,d\nu(x) -\int (\eta^{\top}Ax)^2\,d\nu_t(x) \right| \le d^2 A_{(2,\max)}^2 D_2(\nu,\nu_t).
\]
Also note that $ \left| \int (\eta^{\top}Ax)^2\,d\nu(x) \right| \le d^2A_{(2,\max)}^2 \tilde{C}^2$, and
$\left| \int A_ixx^{\top}A_j^{\top} \,d\nu_t(x) \right| \le d^2A_{\max}^2 \tilde{C}^2$.
Now consider the difference between $G_2$ and $\widehat{G-2}$. 
\begin{align*}
& \left| (G_2)_{i,j} - (\widehat{G_2})_{i,j} \right| \\
=\, & \left| \int (\eta^{\top}Ax)^2\,d\nu(x) \int A_ixx^{\top}A_j^{\top} \,d\nu(x)  - 
\int (\eta^{\top}Ax)^2\,d\nu_t(x) \int A_ixx^{\top}A_j^{\top} \,d\nu_t(x) \right| \\
\le \, & \left| \int (\eta^{\top}Ax)^2\,d\nu(x) \int A_ixx^{\top}A_j^{\top} \,d\nu(x)  - 
\int (\eta^{\top}Ax)^2\,d\nu(x) \int A_ixx^{\top}A_j^{\top} \,d\nu_t(x) \right| \\ 
& \quad + \left| \int (\eta^{\top}Ax)^2\,d\nu(x) \int A_ixx^{\top}A_j^{\top} \,d\nu_t(x)  - 
\int (\eta^{\top}Ax)^2\,d\nu_t(x) \int A_ixx^{\top}A_j^{\top} \,d\nu_t(x) \right| \\
\le\, & \left| \int (\eta^{\top}Ax)^2\,d\nu(x) \right| \left|\int A_ixx^{\top}A_j^{\top} \,d\nu(x) - \int A_ixx^{\top}A_j^{\top} \,d\nu_t(x) \right| \\
& \quad + \left| \int (\eta^{\top}Ax)^2\,d\nu(x) -\int (\eta^{\top}Ax)^2\,d\nu_t(x) \right| \left| \int A_ixx^{\top}A_j^{\top} \,d\nu_t(x) \right| \\
\le\, & 2 d^4  A_{(2,\max)}^2A_{\max}^2\tilde{C}^2D_2(\nu, \nu_t).
\end{align*}
Similarly,
\[
\left| (G_3)_{i,j} - (\widehat{G_3})_{i,j} \right| \le 2 d^4  A_{(2,\max)}^2A_{\max}^2\tilde{C}^2D_2(\nu, \nu_t).
\]
Thus for any $1\le i,j\le d$,
\begin{align*}
\left|\left(\nabla^2 f(\eta)\right)_{i,j} - \left(\nabla^2 \widehat{f}(\eta) \right)_{i,j} \right| 
\le 
d^4  A_{(2,\max)}^2A_{\max}^2\left( 6\tilde{C}^2D_2(\nu, \nu_t) + D_4(\nu, \nu_t)\right).
\end{align*}
Therefore, 
\[
\|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta)  \|_2 \le \|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta)  \|_F \le d^5  A_{(2,\max)}^2A_{\max}^2\left( 6\tilde{C}^2D_2(\nu, \nu_t) + D_4(\nu, \nu_t)\right).
\]
\end{proof}
\subsection{Proof of Theorem  \ref{thm:efficiency}}

Before we can prove the theorem, we need to prove some lemmas.

The following lemma shows that a small perturbation of $M$ will only result in a small variation of its eigenvectors, at least under some mild regularity conditions.

\begin{lemma}
\label{lem:eigenvectorvariation}
Denote $\widehat{M} = M+E$ be a perturbation of matrix $M$, where $M$ is defined in  \eqref{eq:M2}. 
Assume $\widehat{M}$ has distinct eigenvalues. Let $\delta = \gamma -  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$. 
If $\gamma > 4 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$, and $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{4}{\delta}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } \|E\|_2$, then there exist a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that 
\[
\max_{1\le k\le d} \| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le 2  \frac{\sigma_{\max}^2(A)}{\delta \sigma_{\min}(A) } \|E\|_2\,,
\]
and therefore
\[
\sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le 2d  \frac{\sigma_{\max}^2(A)}{\delta \sigma_{\min}(A) } \|E\|_2\,,
\]
where $\widehat{A}$ is the matrix of eigenvectors of $\widehat{M}$. 
\end{lemma}

\begin{proof}
For $1\le k\le d$, assume 
\[
A_{(k)}^{-1} E A_{(k)} =  
\left(
\begin{array}{cc}
F_{1k} & F_{2k}\\
F_{3k} & F_{4k} \\
\end{array} 
\right), 
\]
where $A_{(k)}$ is the matrix $(A_k, A_1, \cdots, A_{k-1}, A_{k+1}, \cdots, A_d)$.
Let $\gamma_k = \|F_{3k}\|_2$, $\eta_k = \|F_{3k}\|_2$, and 
\[
\delta_k = \min_{j: j\neq k} 
\left\vert \left(\frac{\phi^{\top}A_k}{\psi^{\top}A_k}\right)^2 -\left( \frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert - \|F_{1k}\|_2 - \|F_{4k}\|_2\,.
\]
Note that by definition, $\gamma_k = \|F_{3k}\|_2\le\|A_{(k)}^{-1}EA_{k}\|_2\le\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$,
 $\eta_k = \|F_{2k}\|_2\le\|(A^{-1})_kEA_{(k)}\|_2\le\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$, 
 and $\|F_{1k}\|_2,\|F_{4k}\|_2\le\|A_{(k)}^{-1} E A_{(k)}\|_2\le\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$. 
 Thus,
\begin{align*}
\delta_k & = \min_{j:j\neq k} 
	\left\vert \left(\frac{\phi^{\top}A_k}{\psi^{\top}A_k}\right)^2 - \left(\frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert - \|F_{1k}\|_2 - \|F_{4k}\|_2\\
	& \ge \min_{j:j\neq k} \left\vert \left(\frac{\phi^{\top}A_k}{\psi^{\top}A_k}\right)^2 - \left(\frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert - 2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2\\
	& \ge  \gamma -  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2 = \delta\\
	& >  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2 >0,
\end{align*}
and $\delta_k^2 > 4\gamma_k\eta_k$. 
Therefore, by Theorem 2.8, Chapter V of \citep{stewart1990matrix}, there exist a unique vector $v$ satisfying $\|v\|_2\le 2\frac{\gamma_k}{\delta_k}$ such that there exists one of a eigenvector $\widehat{A_k}$ of $\widehat{M}$ satisfying
 \[
 \|\widehat{A_k} - A_k\|_2 \le \|A_{ct}\|_2 \|v\|_2 \le 2\sigma_{\max}(A)\frac{\gamma_k}{\delta_k}
 \le 
 \frac{2\sigma_{\max}^2(A)}{\delta \sigma_{\min}(A) } \|E\|_2.
 \]
 By condition, for $i\neq j$,  $\frac{4\sigma_{\max}^2(A)}{\delta \sigma_{\min}(A) } \|E\|_2 < \|A_i - A_j\|_2\le \|A_i - \widehat{A_i}\|_2 + \|A_j - \widehat{A_i}\|_2$, thus $\widehat{A_i} \neq \widehat{A_j}$.  Summing up the upper bound gets the result. 
\end{proof}

The next lemma shows that $\widehat{X}^{-1}$ is close to $X^{-1}$ with respect to  matrix $2$-norm.

\begin{lemma}
\label{lem:inversevariation}
If non-singular matrix $\widehat{X} = X+E$ satisfying that $\sigma_{\min}(X)\ge2\|E\|_2$, then $\|\widehat{X}^{-1}\|_2 \le \frac{2}{\sigma_{\min}(X)}$, and $\|\widehat{X}^{-1} - X^{-1} \|_2 \le \frac{2}{\sigma_{\min}^2(X)}\|E\|_2$.
\end{lemma} 
\begin{proof}
Note that $\|\widehat{X}^{-1}\|_2$ is the inverse of the minimal singular value of $\widehat{X}$. Also, 
\[
 \min_{v:\|v\|_2=1} \|\widehat{X}v\|_2 = \min_{v:\|v\|_2=1}\|(X+E)v\|_2 \ge \min_{v:\|v\|_2=1} \|Xv\|_2 - \|Ev\|_2 \ge \sigma_{\min}(X) - \|E\|_2.
\]
So $\|\widehat{X}^{-1}\|_2 \le \frac{1}{\sigma_{\min}(X) - \|E\|_2} \le \frac{2}{\sigma_{\min}(X)}$. Moreover,
\begin{align*}
\|\widehat{X}^{-1} - X^{-1} \|_2 \le \|X^{-1}\|_2\|\widehat{X}^{-1}\|_2\|\widehat{X} - X\|_2
\le \frac{2}{\sigma_{\min}^2(X)}\|E\|_2.
\end{align*}
\end{proof}

Now we can estimate the variance between $XY^{-1}$ and $(X+E_1)(Y+E_2)^{-1}$.
\begin{lemma}
\label{lem:Mvariation}
Assume that $\sigma_{\min}(X)\ge2\|E\|_2$, then
\[
\| XY^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \le \frac{\|E_1\|_2}{\sigma_{\min}(Y)} + 4\|X\|_2\|Y\|_2^2\|E_2\|_2.
\]
\end{lemma}
\begin{proof}
Applying \cref{lem:inversevariation},
\begin{align*}
	& \| XY^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \\
\le\, & \| XY^{-1} - X(Y+E_2)^{-1}\|_2 + \| X(Y+E_2)^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \\
\le\, & \|X\|_2\| Y^{-1} - (Y+E_2)^{-1}\|_2 + \|E_1\|_2\|(Y+E_2)^{-1}\|_2\\
\le\, & \frac{2\|X\|_2}{\sigma_{\min}^2(Y)}\|E_2\|_2 + \frac{2}{\sigma_{\min}(Y)}\|E_1\|_2
\end{align*}
\end{proof}


Note that $\nabla^2f(\psi) = \sum_{i=1}^{d} \tilde{\kappa}_iA_iA_i^{\top} = A\tilde{K}A^{\top}$. Thus, $\sigma_{\min}(\nabla^2f(\psi)) = \min_{v:\|v\|_2=1}\|\sum_{i=1}^{d} \kappa_i(\psi^{\top}A_i)^2A_iA_i^{\top}v\|_2$ 

\begin{lemma}
\label{lem:boundsigmaminnabla}
$\sigma_{\min}(\nabla^2f(\psi)) \ge \tilde{\kappa}_{\min}\sigma_{\min}^2(A)$, and 
$\sigma_{\max}(\nabla^2f(\psi)) \le \tilde{\kappa}_{\max}\sigma_{\max}^2(A)$.
\end{lemma}
\begin{proof}
For any unit vector $v$, $v^{\top}AKA^{\top}v \ge \tilde{\kappa}_{\min} \|v^{\top}A\|_2^2 \ge \tilde{\kappa}_{\min}\sigma_{\min}^2(A)$. Similar calculation for the maximum singular value.
\end{proof}

Lastly, we still need to bound $\|M - \widehat{M}\|_2$.
\begin{lemma}
\label{lem:Mvariation_alg}
Given that $\xi \le \frac12\frac{\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}{d^5 A_{(2,\max)}^2A_{\max}^2}$, 
\[ 
\|M - \widehat{M}\|_2 \le \left( \frac{d^5 A_{(2,\max)}^2A_{\max}^2} {\sigma_{\min}^2(A)\tilde{\kappa}_{\min}}+ 4d^5A_{\max}^2 \sigma_{\max}^6(A)\tilde{\kappa}_{\max}^3A_{(2,\max)}^4\right) \xi.
\]
\end{lemma}
\begin{proof}
Let $E_1 = \nabla^2 f(\phi) - \nabla^2 \widehat{f}(\phi)$ and $ E_2 = \nabla^2 f(\psi) - \nabla^2 \widehat{f}(\psi)$. Then $\|E_1\|_2 , \|E_2\|_2 \le d^5 A_{(2,\max)}^2A_{\max}^2\xi$.
Note $\nabla^2f(\phi) = A\tilde{K}\Lambda A^{\top}$ where $\Lambda = \text{diag}\left((\phi^{\top}A_1)^2,\cdots, (\phi^{\top}A_d)^2\right)$.
Given that $\xi \le \frac12 \frac{\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}{d^5 A_{(2,\max)}^2A_{\max}^2}$, the condition in Lemma \ref{lem:Mvariation} holds. Then apply Lemma \ref{lem:Mvariation} and \cref{lem:nablavariation}, we have
\begin{align*}
\|M - \widehat{M}\|_2 =\, & \|(\nabla^2 f(\phi))(\nabla^2f(\psi))^{-1} - (\nabla^2 \widehat{f}(\phi))(\nabla^2\widehat{f}(\psi))^{-1} \|_2 \\
\le \, &\frac{2\|\nabla^2 f(\phi)\|_2}{\sigma_{\min}^2(\nabla^2f(\psi))}\|E_2\|_2 + \frac{2}{\sigma_{\min}(\nabla^2f(\psi))}\|E_1\|_2 \\
\le \, & 2\left( \frac{A_{(2,\max)}^2\tilde{\kappa}_{\max}\sigma_{\max}^2(A)}{\tilde{\kappa}^2_{\min}\sigma_{\min}^4(A)} + 
\frac{1}{\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}
\right)d^5 A_{(2,\max)}^2A_{\max}^2\xi
\end{align*}

Thus, 
\[ 
\|M - \widehat{M}\|_2 \le \frac{2d^5A_{(2,\max)}^4A_{\max}^2\tilde{\kappa}_{\max}\sigma_{\max}^2(A) + 2d^5A_{(2,\max)}^2A_{\max}^2\tilde{\kappa}_{\min}\sigma_{\min}^2(A)}{\tilde{\kappa}^2_{\min}\sigma_{\min}^4(A)} \xi.
\]
\end{proof}
\begin{proof}[{\bf Proof of Theorem  \ref{thm:efficiency}}]
 By \cref{lem:eigenvectorvariation}, 
  \[
  \max_{1\le k\le d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le 2 \frac{\sigma_{\max}^2(A)}{\delta \sigma_{\min}(A) } \|M - \widehat{M} \|_2, 
  \]
 and 
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le 2d  \frac{\sigma_{\max}^2(A)}{\delta \sigma_{\min}(A) } \|M - \widehat{M} \|_2. 
 \]
 Note that $M = \nabla^2f(\phi))(\nabla^2f(\psi))^{-1}$,  then by \cref{lem:Mvariation_alg}, $\|M-\widehat{M}\|_2 \le Q$. Also note that $\delta \ge \frac12 \gamma$.
\end{proof}

\subsection{Proof of Theorem \ref{thm:Modefficiency}}
We need to introduce three lemmas before we can prove the theorem. 
Assume $B$ is a matrix such that $\nabla^2 (f(\psi)) = A\tilde{K}A^{\top}\stackrel{\Delta}{=:}BB^{\top}$. 
So there exists an orthonormal matrix $R$ such that $B =A\tilde{K}^{1/2}R^{\top}$. 
\begin{lemma}
\label{lem:BhatinverseB}
Given that $\xi \le \frac{\sigma_{\min}^2(A)\tilde{\kappa}_{\min}}{2d^5A_{(2,\max)}^2A_{\max}^2}$, there exist an orthonormal matrix $R^*$ such that 
\[
\|\widehat{B}^{-1}B - R^*\|_2 \le \bar{\xi}.
\]
\end{lemma}
\begin{proof}
Note that $\|B\|_2 \ge \sigma_{\min}(A)\tilde{\kappa}^{1/2}_{\min}$. For any unit vector $x$,
\begin{align*}
& x^{\top} B^{-1}\widehat{B}\widehat{B}^{\top}B^{-\top}x - x^{\top}x \\
& \quad  = x^{\top}  B^{-1} ( \widehat{B}\widehat{B}^{\top} -BB^{\top} )B^{-\top}x \\
& \quad  \le \|B^{-\top}x \|_2^2 \|E\|_2 \\
& \quad  \le \frac{d^5A_{(2,\max)}^2A_{\max}^2}{\sigma_{\min}^2(A)\tilde{\kappa}_{\min}} \xi = \bar{\xi}.
\end{align*}
Thus given that $\xi \le \frac{\sigma_{\min}(A)\tilde{\kappa}_{\min}^{1/2}}{2d^5A_{(2,\max)}^2A_{\max}^2}$, $\bar{\xi} \le 1/2$. Also every singular value of $\widehat{B}^{-1}B$ is bounded between $\frac{1}{\sqrt{1+\bar{\xi}}}$ and $\frac{1}{\sqrt{1-\bar{\xi}}}$, i.e. there exist an orthonormal matrix $R^*$ such that 
\[
\|\widehat{B}^{-1}B - R^*\|_2 \le \max \left\{ \left|1-\frac{1}{\sqrt{1+\xi}}\right| , \left|\frac{1}{\sqrt{1-\xi}}-1\right| \right\} \le \bar{\xi}.
\]
\end{proof}


Moreover, note that $f(R^*B^{-\top}\phi) = \sum_i\tilde{\kappa}_i(\phi^{\top}R^*B^{-1}A)_i^4 = \sum_i \tilde{\kappa}_i(\phi^{\top}R^*R\tilde{K}^{-1/2})_i^4=\sum_i\tilde{\kappa}_i^{-1}(\phi^{\top}R^*R)_i^4$.
Define $T$ as follows. 
\begin{equation}
\label{eq:T}
T = \frac{1}{12}\nabla^2(f(R^*B^{-\top}\phi)) = R^*R\tilde{K}^{-1}D(R^*R)^{\top},
\end{equation}
where $D = \text{diag}\left((\phi^{\top}R^*R_1)^2, \cdots, (\phi^{\top}R^*R_d)^2\right)$.

\begin{lemma}
\label{lem:Teigenvectorvariation}
Denote $\widehat{T} = T+E$ be a perturbation of matrix $T$, where $T$ is defined in Equation \eqref{eq:T}. 
Assume $\widehat{T}$ has distinct eigenvalues. Let $\delta_R = \gamma_R -  2\|E\|_2$. 
If $\gamma_R > 4 \sqrt{2}\|E\|_2$, then there exist a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that for $1\le k\le d$
\[
\| c_k\widehat{R}_{\pi(k)} - (R^*R)_k\|_2 \le \frac{2}{\delta_R} \|E\|_2\,,
\]
where $\widehat{R}$ is the matrix of eigenvectors of $\widehat{T}$. 
\end{lemma}
\begin{proof}
Note that 
\[
\min_{i\neq j} \|(R^*R)_i - (R^*R)_j\|_2 = \sqrt{2}>\frac{8}{\gamma_R}\|E\|_2 > \frac{4}{\delta_R}\|E\|_2.
\]
The rest of the proof is similar to the one of Lemma \ref{lem:eigenvectorvariation}.

\if0
Similar to Lemma \ref{lem:eigenvectorvariation}, for $1\le k\le d$ assume 
\[
R_{(k)}^{\top} E R_{(k)} =  
\left(
\begin{array}{cc}
F_{1k} & F_{2k}\\
F_{3k} & F_{4k} \\
\end{array} 
\right), 
\] 
where $R_{(k)}$ is the matrix $(R_k, R_1, \cdots, R_{k-1}, R_{k+1}, \cdots, R_d)$. 
Note that by definition, $\|F_{1k}\|_2,\|F_{4k}\|_2\le\|R_{(k)}^{\top} E R_{(k)}\|_2\le\|E\|_2$,
 $\|F_{3k}\|_2\le\|R_{(k)}^{-1}ER_{k}\|_2\le\|E\|_2$,
 and $\|F_{2k}\|_2\le\|(R^{\top})_kER_{(k)}\|_2\le\|E\|_2$.
Let $\gamma_k = \|F_{3k}\|_2$, $\eta_k = \|F_{3k}\|_2$, and 
\[
\delta_k = \min_{j: j\neq k} 
\left\vert \left(\phi^{\top}R_k\right)^2 -\left( \phi^{\top}R_j\right)^2 \right\vert - \|F_{1k}\|_2 - \|F_{4k}\|_2\,.
\]
Thus 
\begin{align*}
\delta_k & = \min_{j: j\neq k} 
\left\vert \left(\phi^{\top}R_k\right)^2 -\left( \phi^{\top}R_j\right)^2 \right\vert
 - \|F_{1k}\|_2 - \|F_{4k}\|_2\\
	& \ge \min_{j: j\neq k} 
	\left\vert \left(\phi^{\top}R_k\right)^2 -\left( \phi^{\top}R_j\right)^2 \right\vert - 2\|E\|_2\\
	& \ge  \gamma_R -  2 \|E\|_2\\
	& >  2\|E\|_2 >0,
\end{align*}
and $\delta_k^2 > 4\gamma_k\eta_k$. 
Therefore, by Theorem 2.8, Chapter V of \citep{stewart1990matrix}, there exist a unique vector $v$ satisfying $\|v\|_2\le 2\frac{\gamma_k}{\delta_k}$ such that there exists one of a eigenvector $\widehat{R_k}$ of $\widehat{T}$ satisfying
 \[
 \|\widehat{R_k} - R_k\|_2 \le \|R\|_2 \|v\|_2 \le 2\frac{\gamma_k}{\delta_k}.\le \frac{2}{\delta_R} \|E\|_2.
 \]
 By condition, for $i\neq j$,  $\frac{4}{\delta_R} \|E\|_2 < \|R_i - R_j\|_2\le \|R_i - \widehat{R_i}\|_2 + \|R_j - \widehat{R_i}\|_2$, thus $\widehat{R_i} \neq \widehat{R_j}$.  Summing up the upper bound gets the result. 
 \fi
\end{proof}

It remains to bound $\|E\|_2$.
\begin{lemma}
\label{lem:Binversenablavariation}
Given that $\bar{\xi}\le 1/2$, then
\[
\frac{1}{12}\|\nabla^2(f(R^*B^{-\top}\phi)) - \nabla^2(\widehat{f}(\widehat{B}^{-\top}\phi))\|_2
\le \frac{d^5}{(1-\bar{\xi})^2\tilde{\kappa}_{\min}^2}\xi + 2\bar{\xi} 
\le \frac{4d^5}{\tilde{\kappa}_{\min}^2}\xi + 2\bar{\xi} .
\]
\end{lemma}
\begin{proof}
Note that by Lemma \ref{lem:nablavariation} $\|E\|_2 = \frac{1}{12}\|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta)\|_2 \le d^5  A_{(2,\max)}^2A_{\max}^2\xi$, and 
\[
\frac{1}{12}\|\nabla^2(f(R^*B^{-\top}\phi)) - \nabla^2(\widehat{f}(\widehat{B}^{-\top}\phi))\|_2
\le 
\frac{1}{12}\|\nabla^2(f(R^*B^{-\top}\phi)) - \nabla^2(f(\widehat{B}^{-\top}\phi))\|_2
+ \frac{1}{12}\|\nabla^2(f(\widehat{B}^{-\top}\phi)) - \nabla^2(\widehat{f}(\widehat{B}^{-\top}\phi))\|_2.
\]

To Bound $\frac{1}{12}\|\nabla^2(f(\widehat{B}^{-\top}\phi)) - \nabla^2(\widehat{f}(\widehat{B}^{-\top}\phi))\|_2$, we will need the following properties which are straightforward based on Lemma \ref{lem:BhatinverseB}: 
for $1\le i, j\le d$,
\begin{itemize}
\item $|\phi^{\top}\widehat{B}^{-1}A_i| \le \|\widehat{B}^{-1}B\|_2\|B^{-1}A_i\|_2\le \frac{1}{\sqrt{1-\bar{\xi}}\tilde{\kappa}_{\min}^{1/2}}$;
\item $|(\widehat{B}^{-1}A)_{ij}| \le \frac{1}{\sqrt{1-\bar{\xi}}\tilde{\kappa}_{\min}^{1/2}}$;
\end{itemize}
Thus for $1\le i, j\le d$,
\[
|(G_1(\widehat{B}^{-\top}\phi))_{i,j} - (\widehat{G_1}(\widehat{B}^{-\top}\phi))_{i,j}| \le
\frac{d^4}{(1-\bar{\xi})^2\tilde{\kappa}_{\min}^{2}}D_4(\nu,\nu_t).
\]
Also note that for $1\le i, j\le d$ and any distribution $\Prob{y}$ of $y$,
\begin{itemize}
\item $|\int(\phi^{\top}\widehat{B}^{-1}y)(\widehat{B}^{-1})_{i:}y d\Prob{y}| \le \frac{d\tilde{C}^2}{(1-\bar{\xi})\tilde{\kappa}_{\min}}$; 
\item $|\int(\phi^{\top}\widehat{B}^{-1}y)^2 d\Prob{y}|\le \frac{d\tilde{C}^2}{(1-\bar{\xi})\tilde{\kappa}_{\min}}$;
\item $|\int(\widehat{B}^{-1})_{i:}yy^{\top}(\widehat{B}^{-\top})_{:j}d\Prob{y}|\le \frac{d\tilde{C}^2}{(1-\bar{\xi})\tilde{\kappa}_{\min}}$.
\end{itemize}
Thus, 
\[
|(G_2(\widehat{B}^{-\top}\phi))_{i,j} - (\widehat{G_2}(\widehat{B}^{-\top}\phi))_{i,j}| \le
\frac{2d^3\tilde{C}^2}{(1-\bar{\xi})^2\tilde{\kappa}_{\min}^2}D_2(\nu,\nu_t),
\]
and 
\[
|(G_3(\widehat{B}^{-\top}\phi))_{i,j} - (\widehat{G_3}(\widehat{B}^{-\top}\phi))_{i,j}| \le
\frac{2d^3\tilde{C}^2}{(1-\bar{\xi})^2\tilde{\kappa}_{\min}^2}D_2(\nu,\nu_t).
\]
Therefore for $1\le i,j\le d$,
\[
\frac{1}{12}\left|\left(\nabla^2(f(\widehat{B}^{-\top}\phi))\right)_{i,j} - \left(\nabla^2(\widehat{f}(\widehat{B}^{-\top}\phi))\right)_{i,j}\right| 
\le
\frac{d^4}{(1-\bar{\xi})^2\tilde{\kappa}_{\min}^2}\xi, 
\]
thus, 
\begin{equation}
\label{eq:fBhatfhatBhat}
 \frac{1}{12}\|\nabla^2(f(\widehat{B}^{-\top}\phi)) - \nabla^2(\widehat{f}(\widehat{B}^{-\top}\phi))\|_2 \le 
 \frac{d^5}{(1-\bar{\xi})^2\tilde{\kappa}_{\min}^2}\xi.
\end{equation}

On the other hand, 
\begin{align*}
\frac{1}{12}\|\nabla^2(f(R^*B^{-\top}\phi)) - \nabla^2(f(\widehat{B}^{-\top}\phi))\|_2 
= & \, \|R^*B^{-1}A\tilde{K}A^{\top}B^{-\top}(R^*)^{\top}- \widehat{B}^{-1}A\tilde{K}A^{\top}\widehat{B}^{-\top}\|_2 \\
= & \, \|I - (\widehat{B}^{-1}B)B^{-1}A\tilde{K}A^{\top}B^{-\top}(\widehat{B}^{-1}B)^{\top}\|_2 \\
= & \, \|I - (\widehat{B}^{-1}B)(\widehat{B}^{-1}B)^{\top} \|_2\\
\le & \, \|\widehat{B}^{-1}B\|_2^2\|B^{-1}\widehat{B}\widehat{B}^{\top}B^{-\top} - I\|_2 \\
\le & \, \frac{\bar{\xi}}{1-\bar{\xi}} 
\end{align*}
Thus,
\begin{equation}
\label{eq:fBfBhat}
\frac{1}{12}\|\nabla^2(f(R^*B^{-\top}\phi)) - \nabla^2(f(\widehat{B}^{-\top}\phi))\|_2 \le 2\bar{\xi}. 
\end{equation}

Combine Equation \eqref{eq:fBhatfhatBhat} and and Equation \eqref{eq:fBfBhat},
\[
\frac{1}{12}\|\nabla^2(f(B^{-\top}\phi)) - \nabla^2(\widehat{f}(\widehat{B}^{-\top}\phi))\|_2
\le 
 \frac{d^5}{(1-\bar{\xi})^2\tilde{\kappa}_{\min}^2}\xi + 2\bar{\xi}.
\]
\end{proof}
\begin{proof}[{\bf Proof of Theorem \ref{thm:Modefficiency}}]
Note that by Lemma \ref{lem:Binversenablavariation}, 
\[
\|\widehat{T} - T\|_2 = \|\frac{1}{12}\nabla^2(f(R^*B^{-\top}\phi)) - \frac{1}{12}\nabla^2(\widehat{f}(\widehat{B}^{-\top}\phi))\|_2
\le  \frac{4d^5}{\tilde{\kappa}_{\min}^2}\xi + 2\bar{\xi} = Q.
\]
Then by Lemma \ref{lem:Teigenvectorvariation}, there exists a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that for $1\le k\le d$
\[
\|c_k\widehat{R}_{\pi(k)} - (R^*R)_k\|_2 \le \frac{2}{\delta_R} \|\widehat{T} - T\|_2\,,
\]
where $\widehat{R}$ is the matrix of eigenvectors of $\widehat{T}$. 
Denote $\text{diag}(c_1, \cdots, c_d )$ by $D(c)$ and the permutation matrix corresponding to $\pi$ by $\Pi$.
Therefore, 
\begin{align*}
\|\hat{A}\Pi D(c) - A\tilde{K}^{1/2} \|_2 & = \| \hat{B}\hat{R}\Pi D(c) - BR \|_2 \\
& \le \|\widehat{B}\widehat{R}\Pi D(c) - \widehat{B}R^*R \|_2 + \|\widehat{B}R^*R - B(R^*)^{\top}R^*R \|_2 \\
& \le \|B\|_2\|B^{-1}\widehat{B}\|_2\|\widehat{R}\Pi D(c) - R^*R\|_2 + \|B\|_2\|B^{-1}\widehat{B}\|_2\|R^* - \widehat{B}^{-1}B\|_2 \\
& \le \sigma_{\max}(A)\tilde{\kappa}_{\max}^{1/2}(1+\bar{\xi})^{1/2} \frac{2\sqrt{d}}{\delta_R}\|\widehat{T} - T\|_2 + \sigma_{\max}(A)\tilde{\kappa}_{\max}^{1/2}(1+\bar{\xi})^{1/2}\bar{\xi}\\
& \le  \sigma_{\max}(A)\tilde{\kappa}_{\max}^{1/2}(1+\bar{\xi})^{1/2} \left( \bar{\xi} + \frac{2\sqrt{d}}{\delta_R}Q\right). \\
%& \le 2\sigma_{\max}(A)\tilde{\kappa}_{\max}^{1/2} 
%\left( \frac{d^5A_{(2,\max)}^2A_{\max}^2}{\sigma_{\min}(A)\tilde{\kappa}_{\min}^{1/2}} + \frac{4d^{5\frac12}A_{(2,\max)}^2A_{\max}^2}{\delta_R\sigma_{\min}(A)\tilde{\kappa}_{\min}^{1/2}} + \frac{8d^{5\frac12}}{\delta_R\tilde{\kappa}_{\min}^2}\right)\xi
\end{align*}
Similarly, note that $\bar{\xi} \le 1/2$ and $\delta_R \le \frac12 \gamma_R$, thus
\[
\|\hat{A}\Pi D(c) - A\tilde{K}^{1/2} \|_2 
\le 
\sqrt{2}\sigma_{\max}(A)\tilde{\kappa}_{\max}^{1/2}\left( \bar{\xi} + \frac{4\sqrt{d}}{\gamma_R}Q\right).
\]
\end{proof}
%=========================================================================================
\if0
\section{Adapting probabilistic algorithms to deterministic setting}
\subsection{Adapting \cite{arora2012provable}'s algorithm}
\citet{arora2012provable} proposed a polynomial-time algorithm to provably recover the transition matrix $A$ in a probabilistic setting. Note that the algorithm only relies on the moments, thus it should be applicable to both probabilistic setting and deterministic setting.

We also need the assumption that the given sequence is close to some independent distribution $\nu^*$ in the measure of '***' distance. 
Introduce the notation $\E_{\nu}[x]$ as a shorthand for $\int x d\nu(x)$. 
Assume the independent ergodic component functions $f_1,\ldots,f_d:\natural \ra \real$ satisfy $\E_{\nu^{(f_i)}}[x]=0$, and $\kappa_i := \E_{\nu^{(f_i)}}[x^4] - 3\left(\E_{\nu^{(f_i)}}[x^2]\right)^2\neq 0$.\\
*******************************************************\\
Introduce $K$, $D_A(u)$, $d_i$, $\kappa_i$, $d_{\max}$, $d_{\min}$\\
********************************************************\\

\begin{lemma}
With probability at least $e^{-1/\sqrt{\pi}}$, $d_{\min} \ge \frac{12}{d}A_{(2,\min)}^2$.
\end{lemma} 
We state the result of \cite{arora2012provable} in a deterministic setting, as follows. 
\begin{thm}
There exists a constant $c>0$, such that for any zero mean product probability $\nu^*$, non-singular matrix $A$, and signals $g(t)$, if 
\[
D(\nu_T^{A^{-1}g}, \nu^*) \le c\frac{d_{\min}^{9}\kappa_{\min}^{17/2}\sigma_{\min}^{2}(A)}{d^{37/2}\sigma_{\max}^4(A)d_{max}^7\kappa_{\max}^{15/2}\sum_i\Em{\nu^*}{f_i^2}}
\]
where $\kappa_{\min}$ and $\kappa_{\max}$ are the minimal and maximal kurtosis of those of $\nu^*$'s components, then there is algorithm $\text{Alg}(g,\beta)$, such that for any $\beta\in [\beta_0,\beta_1]$ with probability at least $1/3$, $\text{Alg}(g,\beta)$ returns $\hat{A}$ in time $O(n/\beta^2)$ satisfying 
\[
\|\hat{A}_i - c_iA_{\pi(i)}\|_2 \le C_A D_4^{1/2}(\nu_T^{A^{-1}g}, \nu^*),
\]
 for some permutation $\pi$ and constants $c_i$'s, where $\beta_0$, $\beta_1$, and $C_A$ are constants depends on the matrix $A$, $d$, and $\nu^*$. 
\end{thm}

\begin{proof}
Let $f_{A}(u) = \Em{\nu^*}{(u^{\top}Ax)^4}  - 3\left(\Em{\nu^*}{(u^{\top}Ax)^2}\right)^2 = \sum_i \kappa_i(u^{\top}A_i)^4$,
 and $\nabla^2 \widehat{f}_A(u) = \Em{\nu^{A^{-1}g}}{(u^{\top}Ax)^4}  - 3\left(\Em{\nu^{A^{-1}g}}{(u^{\top}Ax)^2}\right)^2$.
By algebraic calculation, 
\[
\nabla^2 f_A(u) = AKD_A(u)A^{\top},
\]
where $D_A(u) = \text{diag}\left(12(u^{\top}A_1)^2,\cdots, 12(u^{\top}A_d)^2\right)$, and $K = \text{diag}\left( \kappa_1,\cdots,\kappa_d\right)$.


\begin{lemma}
\[
\left|E_{i,j}\right| := \left|\left( \nabla^2 f_A(u) - \nabla^2 \widehat{f}_A(u) \right)_{i,j}\right| \le d^4A_{\max}^2 A_{(2,\max)}^2D_4(\nu^{A^{-1}g},\nu^*).
\]
\end{lemma} 
Let $\nabla^2 f_A(u_0) = BB^{\top}$ and $\nabla^2 \widehat{f_A}(u_0) = \widehat{B}\widehat{B}^{\top}$ for some unit vector $u_0$, thus $B = AK^{1/2}D_A^{1/2}(u_0)R$ for some orthonormal matrix $R$. Then every singular value of $B^{-1}\widehat{B}$ is bounded as follows. For any unit vector $x$,
\begin{align*}
& x^{\top} B^{-1}\widehat{B}\widehat{B}^{\top}B^{-\top}x - x^{\top}x \\
& \quad  = x^{\top}  B^{-1} ( \widehat{B}\widehat{B}^{\top} -BB^{\top} )B^{-\top}x \\
& \quad  \le \|B^{-\top}x \|_2^2 \|E\|_2 \\
& \quad  \le \frac{d^5A_{\max}^2 A_{(2,\max)}^2}{\sigma_{\min}(BB^{\top})}D_4(\nu^{A^{-1}g},\nu^*) \\
& \quad  \le \frac{d^5A_{\max}^2 A_{(2,\max)}^2}{d_{\min}\sigma_{\min}^2(A)\kappa_{\min}}D_4(\nu^{A^{-1}g},\nu^*) \quad   =: \xi
\end{align*}

Under the assumption that $D_4(\nu^{A^{-1}g},\nu^*) \le \frac{1}{2}\frac{d_{\min}\sigma_{\min}^2(A)\kappa_{\min}}{d^5A_{\max}^2 A_{(2,\max)}^2} < 1$, we have $\xi\le 1/2$. 
Thus every singular value of $\widehat{B}^{-1}B$ is between $\frac{1}{\sqrt{1+\xi}}$ and $\frac{1}{\sqrt{1-\xi}}$. So there exists a orthonormal matrix $R^*$ such that 
\[
\|\widehat{B}^{-1}B - R^*\|_2 \le \max \left\{ \left|1-\frac{1}{\sqrt{1+\xi}}\right| , \left|\frac{1}{\sqrt{1-\xi}}-1\right| \right\} \le \xi.
\]

Consider the function 
\[
f_{R^*B^{-1}A}(u) = \sum_i \kappa_i (u^{\top}B^{-1}A_i)^4 = \sum_i \kappa^{-1}_i d_i^{-2} (u^{\top}R^*R_i)^4
\]
for some unit vector $u$. Similarly we use its empirical estimation in the algorithm, 
\[
\left|\widehat{f}_{\widehat{B}^{-1}A}(u) - f_{R^*B^{-1}A}(u) \right| \le \left|\widehat{f}_{\widehat{B}^{-1}A}(u) - f_{\widehat{B}^{-1}A}(u) \right| + \left|f_{\widehat{B}^{-1}A}(u) - f_{R^*B^{-1}A}(u) \right|.
\]

Then for the second term 
\begin{align*}
& \left|f_{\widehat{B}^{-1}A}(u) - f_{R^*B^{-1}A}(u) \right| = \left| \sum_i \kappa_i(u^{\top}\widehat{B}^{-1}A_i)^4 - \sum_i \kappa_i(u^{\top}R^*B^{-1}A_i)^4\right| \\ 
& \quad = \left| \sum_i \kappa_i (u^{\top}\widehat{B}^{-1}A_i - u^{\top}R^*B^{-1}A_i)\left[(u^{\top}\widehat{B}^{-1}A_i+u^{\top}R^*B^{-1}A_i)\left((u^{\top}\widehat{B}^{-1}A_i)^2+(u^{\top}R^*B^{-1}A_i)^2\right)\right]\right| \\
& \quad \le \sum_i \left|\kappa_i ( u^{\top}\widehat{B}^{-1}A_i - u^{\top}R^*B^{-1}A_i) \right| \left| u^{\top}\widehat{B}^{-1}A_i+u^{\top}R^*B^{-1}A_i\right| \left|(u^{\top}\widehat{B}^{-1}A_i)^2+(u^{\top}R^*B^{-1}A_i)^2 \right|
\end{align*}
Note that $\left| u^{\top} \widehat{B}^{-1}A_i\right| \le \| \widehat{B}^{-1}A_i\|_2 \le \|\widehat{B}^{-1}B\|_2\|B^{-1}A_i\|_2 \le \frac{\sqrt{2}}{\kappa_{\min}^{1/2}d_{\min}^{1/2}}$ and $ \left| u^{\top} R^*B^{-1}A_i\right| \le \frac{1}{\kappa_{\min}^{1/2}d_{\min}^{1/2}}$.
\begin{align*}
& \left|f_{\widehat{B}^{-1}A}(u) - f_{B^{-1}A}(u) \right| \le \frac{9}{\kappa_{\min}^{3/2}d_{\min}^{3/2}}\sum_i  \left| \kappa_i( u^{\top}\widehat{B}^{-1}A_i - u^{\top}R^*B^{-1}A_i )\right|  \\
& \quad \le \frac{9\sqrt{d}}{\kappa_{\min}^{3/2}d_{\min}^{3/2}}\|u^{\top}\widehat{B}^{-1}AK - u^{\top}R^*B^{-1}AK\|_2 \\
& \quad \le \frac{9\sqrt{d}}{\kappa_{\min}^{3/2}d_{\min}^{3/2}}\|\widehat{B}^{-1}B - R^*\|_2\|B^{-1}AK\|_2 \\
& \quad \le \frac{9\sqrt{d}\kappa_{\max}^{1/2}}{\kappa_{\min}^{3/2}d_{\min}^{2}} \xi \\
& = \frac{9 d^{11/2}\kappa_{\max}^{1/2}A_{\max}^2 A_{(2,\max)}^2 }{\kappa_{\min}^{5/2}d_{\min}^{3}\sigma_{\min}^2(A)} D_4(\nu^{A^{-1}g},\nu^*)
\end{align*}

Now for the first term, note that it is a degree-4 polynomial in $u$. Thus we only need a uniform upper bound for its coefficients. 
Note that $f_{\widehat{B}^{-1}A}(u) = \sum_i \kappa_i (u^{\top}\widehat{B}^{-1}A_i)^4 $. Let $\alpha_i = |u^{\top}\widehat{B}^{-1}A_i| \le \|\widehat{B}^{-1}A_i\|_2 \le \frac{\sqrt{2}}{\kappa_{\min}^{1/2}d_{\min}^{1/2}}$.
Thus, 
\begin{align*}
& \quad \left|\widehat{f}_{\widehat{B}^{-1}A}(u) - f_{\widehat{B}^{-1}A}(u) \right| \\
& \le \left| \int (u^{\top}\widehat{B}^{-1}Af)^4\,d\nu^* - \int (u^{\top}\widehat{B}^{-1}Af)^4\,d\nu^{A^{-1}g} \right|\\
& \quad  + 3\left| \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^* - \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^{A^{-1}g} \right| \left| \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^* + \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^{A^{-1}g} \right| \\
& \le \left| \int (u^{\top}\widehat{B}^{-1}Af)^4\,d\nu^* - \int (u^{\top}\widehat{B}^{-1}Af)^4\,d\nu^{A^{-1}g} \right|\\
& \quad  + 3\left| \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^* - \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^{A^{-1}g} \right| \left( \left| \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^* - \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^{A^{-1}g} \right| + 2 \left| \int (u^{\top}\widehat{B}^{-1}Af)^2\,d\nu^*\right| \right)\\
& \le \frac{4d^4}{\kappa_{\min}^2d_{\min}^2}D_4(\nu^{A^{-1}g},\nu^*)
+\frac{12d^4}{\kappa_{\min}^2d_{\min}^2}D_4^2(\nu^{A^{-1}g},\nu^*)
+ \frac{12\sqrt{2}d^2\sigma_{\max}^2(A)\sum_i\Em{\nu^*}{f_i^2}}{\kappa_{\min}^{3/2}d_{\min}^{3/2}}D_4(\nu^{A^{-1}g},\nu^*)\\
%& \le Q\frac{d^4\sum_i\Em{\nu^*}{f_i^2}}{\kappa_{\min}^2d_{\min}^2} D_4(\nu^{A^{-1}g},\nu^*)
\end{align*}


Thus, 
\[
\left|\widehat{f}_{\widehat{B}^{-1}A}(u) - f_{B^{-1}A}(u) \right| \le 
C \frac{d^{11/2}\kappa_{\max}^{1/2}A_{\max}^2 \sigma_{\max}^2(A) \sum_i\Em{\nu^*}{f_i^2}}{\kappa_{\min}^{5/2}d_{\min}^{3}\sigma_{\min}^2(A)} D_4(\nu^{A^{-1}g},\nu^*)
\]
for some constant $C>0$. We denote the upper bound by $MD_4(\nu^{A^{-1}g},\nu^*)$ where $M$ is the fractional coefficient.

Given the upper bound for $\left|\widehat{f}_{\widehat{B}^{-1}A}(u) - f_{B^{-1}A}(u) \right| $, we now need to pick parameters for the algorithm Alg. To guarantee the interval for $\beta$ is not empty, $D_4(\nu^{A^{-1}g},\nu^*)$ has to be small enough. In general, we need to pick 6 parameters. One of a valid picking schemes for the algorithm is as follows.
\begin{align*}
 & \beta' = \frac{\kappa_{\min}d_{\min}}{Qd^2\kappa_{\max}d_{\max}}; \quad \delta' = \frac{\beta'^2}{100d_{\max}\kappa_{\max}d}; \quad  \gamma' = 3\sqrt{d}\beta'; \\
 & \beta = \frac{1}{K}\beta'; \quad \delta = \frac{\beta^2}{100d_{\max}\kappa_{\max}d}; \quad \gamma = 3\sqrt{d}\beta, 
\end{align*}
where $Q$ and $K$ will be specified later, such that they satisfy the following conditions: 
\begin{enumerate}[(1)]
\item $\beta'\le \frac{\kappa_{\min}d_{\min}}{10d^2\kappa_{\max}d_{max}}$;
\item $K\ge \frac{90000d^{5/2}d_{\max}\kappa_{\max}}{d_{\min}\kappa_{\min}\beta'}$;
\item $Q\ge 1200$;
\item $\frac{\beta'^2}{800d_{\max}\kappa_{\max}dK^2} \ge MD_4(\nu^{A^{-1}g},\nu^*)$
\end{enumerate}
To satisfy all the constraints, $\beta'^4$ has to be in the interval \[
\left[
 \frac{\Lambda_1 d^{21/2}d_{\max}^3\kappa_{\max}^{7/2}\sigma_{\max}^4(A)\sum_i\Em{\nu^*}{f_i^2}}{\kappa_{\min}^{9/2}d_{\min}^5\sigma_{\min}^2(A)}D_4(\nu^{A^{-1}g},\nu^*),
 \frac{d_{\min}^4\kappa_{\min}^4}{\Lambda_2d^8d_{\max}^4\kappa_{\max}^4}
\right]
\]
where $\Lambda_1 = 800\times90000^2$ and $\Lambda_2 = 1200^2$. Setting $Q = 1200$ and 
\begin{align*}
& \beta' = 2  \frac{\Lambda_1^{1/4} d^{21/8}d_{\max}^{3/4}\kappa_{\max}^{7/8}\sigma_{\max}(A)(\sum_i\Em{\nu^*}{f_i^2})^{1/4}}{\kappa_{\min}^{9/8}d_{\min}^{5/4}\sigma_{\min}^{1/2}(A)}D_4^{1/4}(\nu^{A^{-1}g},\nu^*); 
\quad K= \frac{90000d^{5/2}d_{\max}\kappa_{\max}}{d_{\min}\kappa_{\min}\beta'}.
\end{align*}
By the upper bound assumption of $D_4(\nu^*,\nu^{A^{-1}g})$, $\beta'$ is in the valid interval.
Thus with probability at least $e^{-1/\sqrt{\pi}}$, Alg($g$,$\beta$) will return $\widehat{R}$ in time $O(n/\beta^2)$ satisfying 
\[
\|\widehat{R}_i - \tilde{R}_{\pi(i)}\|_2 \le 3\sqrt{d}\beta 
= O\left( \frac{ d^{13/4}d_{\max}^{1/2}\kappa_{\max}^{3/4}\sigma_{\max}^2(A)(\sum_i\Em{\nu^*}{f_i^2})^{1/2}}
{\kappa_{\min}^{5/4}d_{\min}^{3/2}\sigma_{\min}(A)}D_4^{1/2}(\nu^{A^{-1}g},\nu^*) \right) .\]

Without loss of generality, we assume the permutation $\pi$ is the identity mapping. Note that 
\[
B{R^*}^{\top}\tilde{R} = AK^{1/2}D_A^{1/2}(u_0)(R^*R)^{\top}\tilde{R} =AK^{1/2}D_A^{1/2}(u_0).
\]
We will expect $\widehat{B}\widehat{R}_i$ would be a good approximation of $B{R^*}^{\top}\tilde{R}_i = d_i^{1/2}\kappa_iA_i$. Note that $\|\widehat{R}_i\|_2\le 2$ and $\|B\|_2 \le \kappa_{\max}^{1/2}d_{\max}^{1/2}\sigma_{\max}(A)$.
\begin{align*}
\| \widehat{B}\widehat{R}_i - B{R^*}^{\top}\tilde{R}_i \|_2 & \le \|\widehat{B}\widehat{R}_i - B{R^*}^{\top} \widehat{R}_i\|_2+\|B{R^*}^{\top} \widehat{R}_i - B{R^*}^{\top}\tilde{R}_i\|_2 \\
& \le \|B\|_2\|B^{-1}\widehat{B}-{R^*}^{\top}\|_2\|\widehat{R}_i\|_2 + \|B{R^*}^{\top}\|_2\|\widehat{R}_i - \tilde{R}_i\|_2 \\
& \le \|B\|_2\|\widehat{R}_i\|_2\xi + \|B\|_2O\left(D_4^{1/2}(\nu^{A^{-1}g},\nu^*)\right) \\
&  \le C_AD_4^{1/2}(\nu^{A^{-1}g},\nu^*)
\end{align*}
\end{proof}


\begin{remark}
the length of the interval $\beta_1 - \beta_0$.
\end{remark}
In the probabilistic setting since $g$ is generated by $A$ and $\nu^*$, $\beta_0$ will decrease to 0 with $D_4(\nu_T^{A^{-1}g}, \nu^*)$ as $n$ goes to infinity, thus one only need to pick $\beta$ small enough for the algorithm. 
However, in the deterministic setting there is no guarantee on the existence of such $\beta$, let alone picking it. 
 Assume there exists an oracle measure of independence of a joint probability $\text{Dep}(\nu)$ that is easy to compute and satisfies some mild properties, then with a grid search for valid $\beta$ we can adapt Arora's algorithm to a deterministic setting.  

\begin{definition}
A function $\text{Dep}$ is called an independence measure if it satisfies the following properties:
\begin{enumerate}[(i)]
\item $\text{Dep}(\nu^{\underline{X}}) = \text{Dep}(\nu^{\underline{X}+b})$
\item \label{ProdPro} $\text{Dep}(\nu^*) = 0$ for any product measure $\nu^*$
\item \label{SmoothPro} there exist constant $C_{\text{Dep}}>0$, such that $\left| \text{Dep}(\nu^{\underline{X}_1})  - \text{Dep}(\nu^{\underline{X}_2}) \right| \le C_{\text{Dep}} D(\nu^{\underline{X}_1},\nu^{\underline{X}_2})$
\end{enumerate}
\end{definition} 

\begin{remark}
From \eqref{ProdPro} and \eqref{SmoothPro}, note that $\nu^{\underline{X}_2}$ could be any product measure, thus  $ \text{Dep}(\nu^{\underline{X}_1}) \le C_{\text{Dep}} \min_{\nu^*} D(\nu^{\underline{X}_1},\nu^*)$.
\end{remark}
 The new adapted algorithm is guaranteed to return a 'good' transition matrix with high probability if there is one exist.
\begin{algorithm}[H]
\caption{DICA algorithm}
\begin{algorithmic}[1]
\INPUT $g(k)$ for $1\le k \le t$, $\delta >0$
\OUTPUT the estimation $\hat{A}$ of the mixing matrix $A$. 
\STATE $\hat{A} = I$
\STATE Centering: $g = g-\bar{g}$
\FOR{$\beta\in{1/2,1/4,3/4,1/8,3/8,5/8,...}$} 
\STATE  $\tilde{A}_{\beta,k} = \text{Alg}(Y,\beta)$  for $k = 1,2,..., \log 1/\delta$ 
\ENDFOR
\IF{$\text{Dep}(\tilde{A}_{\beta,k}^{-1}g) \le \text{Dep}(\hat{A}^{-1}g)$}
\STATE $\hat{A} = \tilde{A}_{\beta,k}$
\ENDIF
\end{algorithmic}
\end{algorithm}

\begin{thm}
There exists a constant $c>0$, such that for any $g(t)$ and $b\in\real^d$, if there exists a product probability $\nu^*$ and a non-singular matrix $A$ such that 
\[
D(\nu_T^{A^{-1}(g-b)}, \nu^*) \le c\frac{\kappa_{\min}^6\sigma_{\min}^{7}(A)}{d^{3}\kappa_{\max}^7\sigma_{\max}^{10}(A)}.
\]
Then after a long enough running time $T$ , the algorithm DICA will returns a matrix $\hat{A}$ such that with probability at least $1-\delta$,
\[
\text{Dep}(\hat{A}^{-1}g) \le \text{Dep}(A^{-1}Y) + KD(\nu_T^{A^{-1}g}, \nu^*), 
\] 
for some constant $K$.
\end{thm}

\begin{proof}
Let $l = \beta_1 - \beta_0$ and $I_l = \lceil -\log l \rceil$. Thus, there exist at least one valid $\beta$ in the first $2^{I_l} \le \frac{2}{l}$ tried $\beta$'s. Thus, after time ****, with probability at least $1-\delta$, there exist at least one candidate $\tilde{A}$ such that  
\[
\|\tilde{A}_i - c_iA_{\pi(i)}\|_2 \le C_A D(\nu_T^{A^{-1}g}, \nu^*),
\]
 for some permutation $\pi$ and constants $c_i$'s. Thus, 
 \[
 \text{Dep}(\nu^{\hat{A}^{-1}g}) \le  \text{Dep}(\nu^{\tilde{A}^{-1}g}) =  \text{Dep}(\nu^{\tilde{A}^{-1}C\Pi g})\le  \text{Dep}(\nu^{A^{-1}g})+C_{\text{Dep}}D(\nu^{\tilde{A}^{-1}C\Pi g}, \nu^{A^{-1}g}) \le C_{\text{Dep}}\left(D(\nu^{\tilde{A}^{-1}C\Pi g}, \nu^{A^{-1}g})  + D(\nu^{A^{-1}g}, \nu^*) \right)  
 \]

It remains to bound $D(\nu^{\tilde{A}^{-1}C\Pi g}, \nu^{A^{-1}g})$. We only need to bound for $\|\tilde{A}^{-1} C\Pi g - A^{-1}g\|_2$. Note that 
\[
\|\tilde{A}^{-1}C\Pi g_i - A^{-1}g_i\|_2 \le \|\tilde{A}^{-1}C\Pi- A^{-1}\|_2\|g_i\|_2 \le \|\tilde{A}^{-1}\|_2\|A^{-1}\|_2\|\tilde{A}-C\Pi A\|_2\|g_i\|_2.
\]
Still need to bound $\|\tilde{A}^{-1}\|_2$ and $\|\tilde{A}-C\Pi A\|_2$, by $\|\tilde{A}_i - c_iA_{\pi(i)}\|_2 $.
For any unit vector $v$,
\begin{align*}
v^{\top}(\tilde{A}-C\Pi A)(\tilde{A}-C\Pi A)^{\top}v & = \sum_i \|v^{\top}(\tilde{A}-C\Pi A)_i\|_2^2 \\
& \le \sum_i \|v^{\top}(\tilde{A}_i-c_iA_{\pi(i)})\|_2^2 \\
& \le \sum_i \|\tilde{A}_i-c_iA_{\pi(i)}\|_2^2 \\
& \le d C_A D(\nu_T^{A^{-1}g}, \nu^*).
\end{align*}
\end{proof}
\begin{remark}
How large $T$ has to be? 
\end{remark}
\fi
%========================================================================================= 
 \if0
 Instead of sampling $\phi$ and $\psi$ uniformly from the unit ball, sample $\phi,\psi \sim {\cN} (0,\,A^{-1}A^{{-1}^{\top}})$ independently. Therefore, $\alpha_i = \phi^{\top}A_i$, $\beta_i = \psi^{\top}A_i$ are independent standard normal variables. Moreover, 
 \[
 \gamma  = \min_{i,j} \left\vert \big(\frac{\alpha_i}{\beta_i}\big)^2 - \big(\frac{\alpha_j}{\beta_j}\big)^2 \right\vert.
 \] 
 
 Note that for $x\ge0$, $\Phi(x) \approx \frac12 (1+\sqrt{1-e^{-\frac{2}{\pi}x^2}})$ \citep{aludaat2008note}.
  
  When $d$ is large,
 \[
 \Prob{|\beta_i| \ge \sqrt{\frac{\pi}{2}\log \frac{d^2}{d^2-1}} \text{ for all } \beta_i\text{'s}} = (2- 2\Phi(\sqrt{\frac{\pi}{2}\log \frac{d^2}{d^2-1}})-1)^d \approx \left(1-\frac{1}{d}\right)^d \approx \frac{1}{e}.
 \]
 Thus, with probability at least $p (\approx \frac{1}{e})$,
 \[
 \gamma \le 2 \min_i \left\vert \big(\frac{\alpha_i}{\beta_i}\big)^2\right\vert \le 2 \frac{2}{\pi}\frac{1}{\log \frac{d^2}{d^2-1}}\min_i |\alpha_i|^2.
 \]
 
 Now consider the case that at least two of $\beta_i$'s are greater than $\sqrt{\frac{\pi}{2}\log d}$,
 \begin{align*}
 & \Prob{|\beta_i| \ge \sqrt{\frac{\pi}{2}\log d} \text{ for at least 2 } \beta_i\text{'s}} \\
 =  & 1 - \Prob{|\beta_i| \le \sqrt{\frac{\pi}{2}\log d} \text{ for all } \beta_i\text{'s}} - \Prob{|\beta_i| \le \sqrt{\frac{\pi}{2}\log d} \text{ for (d-1) } \beta_i\text{'s}} \\
 \approx & 1 - (1-\frac{1}{d})^{\frac{d}{2}} -  (1-\frac{1}{d})^{\frac{d-1}{2}}(1-\sqrt{1-\frac{1}{d}}) \\
 \approx & 1 -\frac{1}{\sqrt{e}}.
 \end{align*}
 Thus, with probability at least $p_2$ ($\approx 1-\frac{1}{\sqrt{e}}$),
 \[
 \E[\gamma] \le \frac{4}{\pi} \frac{1}{\log d} \E[\alpha_i^2] = \frac{4}{\pi} \frac{1}{\log d}.
 \]
 \fi

\end{document}
