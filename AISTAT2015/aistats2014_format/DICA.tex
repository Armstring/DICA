\documentclass[twoside]{article}
\usepackage{aistats2014}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[authoryear]{natbib}
\usepackage{enumerate}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}

\newcommand{\xcom}[1]{x_{#1}}
\newcommand{\scom}[1]{s_{#1}}
\newcommand{\cset}[2]{\left\{#1\,:\,#2\right\}}
\newcommand{\Ephione}{\mathbb{E}_{\phi_1}}
\newcommand{\Ephitwo}{\mathbb{E}_{\phi_2}}
\newcommand{\Epsi}{\mathbb{E}_{\psi}}
\newcommand{\Ephi}{\mathbb{E}_{\phi}}
\newcommand{\EZ}{\mathbb{E}_{Z}}
\newcommand{\cN}{\cal{N}}
\renewcommand{\P}{{\mathcal P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ex}[1]{\mathbb{E}[#1]}
\newcommand{\Em}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\snorm}[1]{\left\|#1\right\|} % scaling norm
\newcommand{\lmax}[1]{\lambda_{\mathrm{max}}(#1)}
\newcommand{\lmin}[1]{\lambda_{\mathrm{min}}(#1)}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\dprod}[2]{\langle #1,#2 \rangle_{M}}
\newcommand{\hA}{\hat{A}}
\newcommand{\hb}{\hat{b}}
\newcommand{\hC}{\hat{C}}
\newcommand{\hAp}{\hat{A}^\prime}
\newcommand{\hbp}{\hat{b}^\prime}
\newcommand{\hCp}{\hat{C}^\prime}
\renewcommand{\th}{\theta}
\newcommand{\vth}{\hat{\theta}_{V}}
\newcommand{\lth}{\hat{\theta}_{\lambda}}
\newcommand{\hth}{\hat{\theta}}
\newcommand{\ltho}{\hat{\theta}_{\lambda_1}}
\newcommand{\ltht}{\hat{\theta}_{\lambda_2}}
\newcommand{\hlth}{\hat{\theta}_{\hat{\lambda}}}
\newcommand{\slth}{\hat{\theta}_{\lambda^*}}
\newcommand{\plth}{\hat{\theta}_{\lambda_p}}
\newcommand{\hthp}{\hat{\theta}^\prime}
\newcommand{\lthp}{\hat{\theta}^\prime_{\lambdap}}
\newcommand{\ath}{\hat{\theta}_{a}}
\newcommand{\thetap}{\theta^\prime}
\newcommand{\sth}{\theta^*}
\newcommand{\ind}[1]{\mathbb{I}_{\left\{ #1 \right\}}}
\newcommand{\hzeta}{\hat{\zeta}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\cf}{\emph{cf.}}
\newcommand{\T}[1]{T\left( #1 \right)}
\newcommand{\St}[1]{S\left( #1 \right)}
\newcommand{\Ts}[2]{T_{#1}\left( #2 \right)}
\newcommand{\Ss}[2]{S_{#1}\left( #2 \right)}
\newcommand{\Ord}[1]{O\left( #1 \right)}
\newcommand{\etc}{\emph{etc.}}
\newcommand{\bsth}{\bar{\theta}^*}
\newcommand{\sal}{\emph} %Emphasis the J.D. Salinger way :)
\newcommand{\gradL}{\nabla \mathcal{L}}
\newcommand{\lambdap}{{\lambda^\prime}}
\newcommand{\lambdahp}{\hat{\lambda}^\prime}
\newcommand{\pqLoss}[1]{\mathcal{L}_{#1}}
\newcommand{\pLoss}[2]{\pqLoss{#1}( #2 )}
\newcommand{\qLoss}{\pqLoss{M}}
\newcommand{\Loss}[1]{\qLoss(#1)}
\newcommand{\hpqLoss}[1]{\mathcal{\hat{L}}_{#1}}
\newcommand{\hpLoss}[2]{\hpqLoss{#1}( #2 )}
\newcommand{\hqLoss}{\hpqLoss{M}}
\newcommand{\hLoss}[1]{\hqLoss(#1)}
\newcommand{\bX}{\bar{X}}
\newcommand{\bY}{\bar{Y}}
\newcommand{\rth}{\hat{\theta}}
\newcommand{\tth}{\tilde{\theta}}
\newcommand{\maxeig}{\nu_{\max}}
\newcommand{\mineig}{\nu_{\min}}
\newcommand{\ra}{\rightarrow}
\newcommand{\real}{\mathbb{R}}
\newcommand{\one}[1]{\mathbf{1}_{\{#1\}}}
\newcommand{\rl}[1]{\mathbb{R}^{#1}}
\newcommand{\Mset}{\mathcal{M}(\varepsilon)}
\newcommand{\Cset}{\mathcal{C}}
\newcommand{\truel}{L}
\newcommand{\zol}{L_{\mathrm{0-1}}}
\newcommand{\hinl}{L_{\mathrm{hinge}}}
\newcommand{\phil}{L_{\varphi}}
\newcommand{\lspace}{\left\{1, \dots,K \right\}}
\newcommand{\risk}{\mathcal{R}}
\newcommand{\scoref}{\in \mathcal{H}}
\newcommand{\subscoref}{\in \mathcal{H}^\prime}
\newcommand{\hQ}{\hat{Q}}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\esssup}{ess\,sup}
\renewcommand{\natural}{\mathbb{N}}


% theorems/definitions
\newtheorem{lemma}{Lemma}[section]
\newtheorem{thm}[lemma]{Theorem}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{cor}[lemma]{Corollary}
\newtheorem{example}[lemma]{Example}
\newtheorem{prop}[lemma]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{remark}[lemma]{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{note}[lemma]{Note}
\newtheorem{problem}[lemma]{Problem}

\newcommand{\FF}{\mathcal{F}}
\newcommand{\TT}{\mathcal{T}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\KK}{\mathcal{K}}
\newcommand{\eps}{\varepsilon}

% If your paper is accepted, change the options for the package
% aistats2014 as follows:
%
%\usepackage[accepted]{aistats2014}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Deterministic Independent Component Analysis}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
Abstract.
\end{abstract}

\section{Introduction}
\label{sec:Intro}
Independent Component Analysis (ICA), as a main tool for separating blind sources, has received much attention in the past decades. 
The ICA model assumes a $d$-dimensional vector $X$ is a linear mixture of $d$ independent variables $(S_1,\ldots, S_d)$ with Gaussian noise:
\[
X = AS+\epsilon,
\]
where $\epsilon$ is a $d$-dimensional Gaussian noise, and $A$ is a nonsingular mixing matrix.
Our interest is to reconstruct $A$ given observations of $X$ with rigorous guarantee.

The literature of the ICA model is vast in both practical algorithms and theoretical analysis. 
We refer to \citep{comon2010handbook} for a comprehensive survey.
Perhaps the most popular ICA algorithm in engineering application would be the FastICA \citep{hyvarinen1999fast}. 
In general, FastICA tries to find a linear transformation $W$ for $X$ by optimizing a particular designed contrast function, 
such that the resulted coordinates of $WX$ are as independent as possible. 
The optimal $W$ would be $A^{-1}$ under additional assumptions about the sources, thereby $A$ is recovered.  
Theoretical analysis of the FastICA algorithm has been conducted in many aspects, 
but as far as we know, finite sample bound for FastICA is still not available.
Besides, consistency of the estimation of $A$ can only be guaranteed for a particular forth-moments-based contrast function with noise-less observations(i.e. $X = AS$) \citep{miettinen2014fourth}. 

Another class of ICA methods views ICA in a semiparametric manner with parameters $(W, p_1, \ldots, p_d)$ where $W$ is the targeted matrix, and nuisance parameters $p_i$ is the density function of the $i$-th source. 
Under additional statistical assumptions about the source signals, consistent estimators have been proposed for various settings. The first  $\sqrt{n}$-consistent estimator of $A$ (which implies a finite sample bound in the rate of $\sqrt{n}$), to our best knowledge, is proposed and analyzed in \citet{samarov2004nonparametric}.  
Later \citep{chen2005consistent} showed that the estimator proposed in \citet{eriksson2003characteristic} is also $\sqrt{n}$-consistent. Another estimator which is more efficient is also proposed by the same authors in \citep{chen2006efficient}.
However, these results also assume a noise-free setting. 
Moreover, they involve an intermediate procedure of estimating a probability function or a score function, which consequentially invites extra technical conditions. 

The first provable ICA method that can deal with the noisy case is by \citep{arora2012provable}. 
The idea is still based on the forth moments. 
After a quasi-whitening procedure, \citep{arora2012provable} reduced the ICA problem into a problem of finding all the local optimizer of a specific function defined using the forth order cumulant. Then a polynomial-time algorithm is proposed to find all the local optimizers with theoretical guarantee.
Despite its great progress in theory, \citep{arora2012provable}'s algorithm requires an exhaustive search for an input parameter, $\beta$ in the paper.
The interval of the valid $\beta$s is not yet well understood, which impairs the usefulness of the algorithm in practice.

The forth moment statics are also used in many other ICA related works. 
Recently, a series of more promising methods are proposed based on the algebraic structure of the model \citep{hsu2013learning,anandkumar2012tensordecomposition,anandkumar2012method}. 
In fact, this idea is also discussed by \citet{cardoso1999high} as a intuitive argument to construct a contrast function. 
The first rigorous proof of this idea is developed using matrix perturbation tools in \citep{anandkumar2012tensordecomposition,anandkumar2012method,goyal2014fourier} in a general tensor perspective. 
However, these results exponentially depend on the number of source signals $d$.
In particular, these methods all require an eigen-decomposition of some flatten tensor where the minimal gap between the eigenvalues plays an essential role. 
A naive analysis of this gap will introduce an exponential dependence on the dimension of the flatten tensor $d$. 
This dependence is also observed in \citep{cardoso1999high,goyal2014fourier}.
One way to circumvent such dependence would be directly decomposing a high order tensor using power method which requires no flatten procedure, as in \citep{anandkumar2014guaranteed}. 
However, it is well known that power method is unstable in practice for high order tensor. 
Another drawback is that this method, when applied to the ICA problem, introduces an error term that does not approach 0 as the sample size approaches infinity. 

In this paper we proposed a practical method for ICA which also has meaningful theoretical guarantee (polynomial-time; with constant probability the reconstruction error of $A$ polynomially depends on all the parameters and vanishes as the sample size going to infinity).
Our algorithm is a refined version of \cite{hsu2013learning}'s algorithm.  
Another contribution of the present paper is that the theoretical analysis is conducted in a deterministic manner. 
In practice ICA is also known to work well for unmixing the mixture of various deterministic signals. 
One of the classic demonstrations of ICA is showing that the periodic signals can be well recovered from their mixtures. \citep{HyvOja00}.
Such phenomenon suggests that the usual probabilistic notion is unsatisfactory if one wishes to have deeper understanding of ICA.   
Our deterministic analysis help investigate this curious phenomenon without losing any generality to the traditional stochastic setting. 

Formally, let $s:\natural \ra \real^d$ be a $d$-dimensional deterministic ``signal''. 
Denote by $s_i$ the $i$th component of $s$.
We present the ICA model as a linear mixture of functions (details will be introduced later).
Then under a measure of functional dependence, we analyze  \citep{hsu2013learning}'s algorithm.
A detailed analysis shows that its performance depends on the minimal gap of the eigenvalues (of some random matrix. The exact form is presented later).
A meaningful lower bound  (polynomial in the parameters of the ICA model) for this gap is essential for the soundness of the algorithm, but not yet available \citep{cardoso1999high}.
A key step of our algorithm is reducing the minimal eigenvalue gap problem to the problem of minimal spacing of i.i.d. Cauchy random variables, for which a polynomial lower bound is available, by the quasi-whitening procedure.
This idea is also used in \citep{frieze1996learning} and \citep{arora2012provable}.
With this lower bound, we then show that the new algorithm has provable guarantee on the reconstruction error of $A$. 

The rest of this paper is organized as follows: 
We introduce some preliminaries in Section \ref{sec:Preliminaries}. Section \ref{sec:AnalysisHK} is devoted to the analysis of HK algorithm. Then in section \ref{sec:DICA} we propose our algorithm and show that it has the 'real' provable guarantee.
Lastly, experimental results are reported in Section \ref{sec:ExpRes}.

\subsection{Notations}
Denote the maximal (respectively minimal) singular value of a matrix $A$ by  $\sigma_{\max}(A)$ (respectively $\sigma_{\min}(A)$). Also, let $A_{(2,\min)} = \min_{i} \|A_i\|_2$, $A_{(2,\max)} = \max_{i} \|A_i\|_2$, and $A_{\max} = \max_{i,j} |A_{i,j}|$.

\section{Preliminaries}
\label{sec:Preliminaries}
\subsection{Induced Probability}
\label{subsec:InducesProb}
Given arbitrary measurable domains $(\Omega,\AA)$ and a sequence of probability measures $(\mu_t)_{t\ge0}$ over it, we define $\nu_t$ to be the probability measure over $\real^d$ that is induced by $s$ and $\mu_t$ for any function $s: \Omega \ra \real^d$ and $t\ge 0$:
In particular, for any Borel set $A\subset \real^d$,
\[
\nu_t(A)= \mu_t\Bigl( \cset{z}{s(z)\in A} \Bigr)
= \mu_t ( s^{-1}(A) ).
\]
Note that if $\Omega = \natural$, $\AA = 2^\natural$ and $\mu_t$ is the uniform probability distribution on $\{0,\ldots,t\}$, then $\nu_t$ is the empirical probability.

In what follows we fix $\Omega$ and $(\mu_t)$ and define all the concepts that follow relative to these. 
In this paper we only consider about real functions.
When the limit of $(\nu_t)$ exists, it will be denoted by $\nu$.
Also, if we want to emphasize the dependence on $s$ then we will use $\nu^{(s)}$.
Similarly, when the dependence of $\nu_t$ on $s$ is important, we will use $\nu^{(s)}_t$.
\begin{definition}
A function $s:\Omega \rightarrow \real^d$ is called \emph{ergodic} w.r.t. $(\mu_t)_{t\ge0}$
if the sequence of  induced probability measures $(\nu_t)_{t\ge 0}$ is weakly convergent.
\end{definition}
A simple example of an ergodic function is a real periodic function.
The following lemma shows that once $\nu$ exists, it is a probability measure. 
\begin{prop}
\label{prop:ergodicfunction}
Let $\scom{i}$ denote the $i$th coordinate function of $s$. Assume that
\[
\lim_{t\to\infty} \int |\scom{i}(x)|\, d\mu_t(x) 
\]
exists and is finite for all $1 \le i \le d$ and that $\esssup_{\mu_t} |\scom{i}|<\infty$ for any $1\le i \le d$ and $t\ge 0$.
Then all limit points of the sequence $\{\nu_t\}$, with respect to the weak topology, are probability measures over $\real^d$.
\end{prop}
\subsection{ICA}
\label{subsec:ICA}
Assume we are given a $d$-dimensional observed function $x(t)$ defined by  
\begin{equation}
\label{equ:ICA}
x(t) = As(t), \quad t\ge0
\end{equation}
where $A$ is a $d\times d$ nonsingular matrix and  $s$ is a $d$-dimensional signal function. 
Note that the above deterministic setting does not lose any generality to the traditional stochastic setting of ICA with Gaussian noise, given that we can rewrite the stochastic setting as 
\[
x = As+\eps = A(s+A^{-1}\eps),
\] 
where $\eps$ is the Gaussian noise. In practice, we are not going to observe the true distribution of $x$, but $x(t)$, $t\ge 0$, which leads to Equation \eqref{equ:ICA}.



\if0
In this section, we propose a natural notion fot the independence of signal functions in an analogous way, which is compatible with the stochastic setting but much more general.
\subsection{Independent Components}
\label{subsec:IndeComp}
Despite the perceptual intuitiveness of the definition of ergodic function, the space of ergodic functions is, unexpectedly, not closed under addition, as shown in Example \ref{eg:ergodic}. 
We provide a sufficient and necessary condition for a linear combination 
of ergodic functions to be ergodic in Proposition \ref{prop:comp}. 
It turns out the space spanned by the components of an ergodic function is a space of ergodic functions.
Such observation leads to our definition of independence of components, and build the foundation of the rest work of our paper.
\begin{example}
\label{eg:ergodic}
Let $f,g:\natural \rightarrow \{-1,+1\}$ and $(\mu_t)$ be defined as follows: 
\[
f(t) = \begin{cases} +1, & t \equiv 0 \pmod 4; \\
					-1, & t \equiv 1 \pmod 4; \\
					+1,  & t \equiv 2  \pmod 4; \\
					-1, & t \equiv 3  \pmod 4,
					\end{cases}
					\]\[
g(t) = \begin{cases} +1, & t\equiv 0 \pmod 4; \\
					-1, & t\equiv 1 \pmod 4; \\
					-1,  & t\equiv 2 \pmod 4; \\
					+1, & t\equiv 3 \pmod 4.
					\end{cases}
\]
\[
\quad \mu_{2t+1}(x) = \begin{cases} 1/2, & x=0; \\
					1/2, & x=1, 
					\end{cases}
\mu_{2t}(x) = \begin{cases} 1/2, & x=2; \\
					1/2, & x=3;.
					\end{cases}
\]
Then, $\nu^{(f)}_t$, $\nu^{(g)}_t$ are both the uniform  distribution on $\{-1,+1\}$, hence they converge and the limit is the same uniform  distribution. 
However, while $\nu^{(f+g)}_{2t+1}$ is the uniform  distribution on $\{-2,+2\}$,  $\nu^{(f+g)}_{2t}$ is the degenerate
distribution that puts all the mass at $0$. Thus, $\nu^{(f+g)}_{t}$ fails to be convergent. 
\end{example}

\begin{prop}\label{prop:comp}
$s = (\scom{1},\ldots,\scom{d})^{\top}$ is a $d$-dimensional ergodic function, if and only if 
 for any $m\times d$ matrix $A$, $x = A s$ is an $m$-dimensional ergodic function.
\end{prop}  
Based on Lemma \ref{prop:comp}, we can now define the independence of the components of a multidimensional ergodic function $f$ based on its induced limiting distribution $\nu^{(f)}$:
\begin{definition}
Given an ergodic function $s = (\scom{1},\ldots, \scom{d})^{\top}:\Omega \rightarrow \real^d$, 
we say that $\{\scom{1},\ldots, \scom{d}\}$ are \emph{independent} components, 
	if $\nu^{(\scom{1})}\otimes\ldots\otimes\nu^{(\scom{d})} = v^{(s)}$.
\end{definition}
\begin{remark}
Note that the existence of $\nu^{(s^{(i)})}$ is guaranteed by Proposition \ref{prop:comp}. 
The definition of independent components can also be extended to the independence of ergodic functions: 
$f$ and $g$ are called independent, if their joint function $(f,g)$ is an ergodic function with independent components. 
\end{remark}
\begin{remark}
Assume that $f = (f_1,f_2)^{\top}:\real\rightarrow\real^2$ is an ergodic function, and both $f_1$ and $f_2$ are periodic with respective periods $T_1$ and $T_2$. 
Let $\mu_t$ be the uniform probability distribution on $[0,t]$. 
%$\mu_t((a,b)) = (b-a)/t$ for $0\le a\le b\le t$.
 If $T_1/T_2$ is irrational, then it is easy to show that $f_1$ and $f_2$ are independent. 
 This example shows how period signals can be fitted into our framework.
\end{remark}
\fi

\section{Analysis of \citet{hsu2013learning}'s Algorithm}
\label{sec:AnalysisHK}

\if0
We now analyze the ICA problem in our deterministic framework proposed in Section \ref{sec:DeterFrame}.

\subsection{Identifiability}
\label{subsec:Indentifiability}
The first essential question is the identifiability of the ICA model.
Our result requires the minimal assumption that each component $s_i$ is non-Gaussian\footnote{this result could be extended to allow one Gaussian.}, which is the same as that of stochastic setting.
\begin{thm}
\label{thm:CorofICA}
Let $s = (\scom{1},\ldots,\scom{d})^{\top}$ be an ergodic function with independent components and
assume that neither of $\nu_{\scom{i}}$, $i=1,\ldots,d$ is a Gaussian distribution. If
\begin{equation}
\left(
\begin{array}{ccc}
\xcom{1} \\
\vdots \\
\xcom{d}
\end{array}
\right) = A
\left(
\begin{array}{ccc}
\scom{1} \\
\vdots \\
\scom{d}
\end{array}
\right)
\end{equation}
are independent, then $A$ is equivalent to a permutation matrix up to scaling.
\end{thm}

\subsection{Reconstruction Error}
\label{subsec:HowDifficult}

Standard results about the reconstruction of $A$ can be phrased as follows:
Let $S$ be a random $d$-dimensional vector, whose components are independent of each other.
Let $X = A S$, where $A$ is a nonsingular $d\times d$ matrix. Then, given the empirical distribution of $X$, assuming that the joint of $S$ satisfies some additional assumptions, the matrix $A$ can be approximately reconstructed up to permutation and scaling, with provable error.

In general, we believe these results carry through to the deterministic setting, by replacing the empirical distribution of $S$ with $\nu^{(s)}$ where $s$ is a $d$-dimensional deterministic signal.
In this paper, we analyze a randomized ICA algorithm of \citep{DHsu2012}(HK algorithm) as an example. 
In the deterministic setting, it clearly shows that the sample procedure plays an important role in the performance.
However, we have difficult in analyzing the behavior of this sample procedure, which leads to a vacuous theoretical result.
To solve this problem, we proposed a refined version of the HK algorithm, called deterministic ICA algorithm. 
This is the first `real' polynomial time moment-based algorithm for ICA. 
Details will be discussed in the next section due to its complexity.  
\fi

We analyze the performance of \citet{hsu2013learning}'s algorithm (HK) in this section.

\subsection{HK algorithm}
\label{subsec:HKalg}
\citet{DHsu2012} proposed an algorithm for ICA based on the idea of moment methods.
For $p\ge 1$, $\eta\in \real^d$, 
let 
\begin{equation}
\label{eq:momnent}
m_p(\eta) = \E_{Y\sim \nu^{(s)}}[ (\eta^\top A Y)^p ]
\end{equation}
and let
\begin{equation}
\label{eq:funcf}
f(\eta) = \frac1{12} \left( m_4(\eta) - 3 m_2(\eta)^2 \right)\,.
\end{equation}
In general, we denote by adding a ``hat'' on the top of a symbol its empirical estimate.
\begin{algorithm}[H]
\caption{HK algorithm}
\begin{algorithmic}[1]
\INPUT $x(t)$ for $0\le t \le T$. 
\OUTPUT An estimation of the mixing matrix $A$. 
\STATE Sample $\phi$ and $\psi$ independently from a standard Gaussian distribution of dimension $d$;
\STATE Evaluate $\nabla^2\widehat{f}(\phi)$ and $\nabla^2\widehat{f}(\psi)$, 
\STATE Compute $\widehat{M} = (\nabla^2 \widehat{f}(\phi))(\nabla^2\widehat{f}(\psi))^{-1}$;
\STATE Compute all the eigenvectors of $\widehat{M}$, $\{\mu_1,\ldots,\mu_d\}$;
\STATE Return $\widehat{A} = (\mu_1,\ldots,\mu_d)$.
\end{algorithmic}
\end{algorithm}
In particular, $\nabla^2\widehat{f}(\eta)$  is evaluated from the observations by
\begin{equation}
\label{eq:G}
\nabla^2 \widehat{f}(\eta) = \widehat{G}(\eta):= \widehat{G_1}(\eta) - \widehat{G_2}(\eta) -2\widehat{G_3}(\eta),
\end{equation}
where 
\begin{align*}
&\widehat{ G_1}(\eta) = \frac1n\sum_{t=1}^{n} \big(\eta^{\top}x(t)\big)^2x(t)x(t)^{\top}; \\
& \widehat{G_2}(\eta) = \frac{1}{n^2}\sum_{t=1}^{n} \big(\eta^{\top}x(t)\big)^2 \sum_{t=1}^{n}x(t)x(t)^{\top}; \\
& \widehat{G_3}(\eta) = \frac{1}{n^2}\Big(\sum_{t=1}^{n} \big(\eta^{\top}x(t)\big)x(t)\Big) \Big(\sum_{t=1}^{n} \big(\eta^{\top}x(t)\big)x(t)\Big)^{\top}.
\end{align*} 
\begin{remark}
In practice, HK algorithm may generate complex output. 
This occurs only when $\nabla^2\widehat{f}(\psi)$ is singular due to numerical precision. 
Note that theoretically this happens with probability 0. 
One way of fixing this problem is re-sampling a new pair of $\phi$ and $\psi$.
\end{remark}
Even though the authors believed that the algorithm has provable guarantee, no rigorous analysis is provided. 
We try to fill the gap in the next subsection. 
A detailed analysis shows that the performance of the algorithm may be not as trivial as it is claimed.

\subsection{Reconstruction error}
\label{subsec:errorHK}
Our analysis is developed in the deterministic setting.
We try to have weak assumption instead of independence for the source signals. 
Clearly, no meaningful results will be achieved if $s$ is allowed to go arbitrarily wild. 
We define a `distance' for two distributions, which is then used as a `dependence measure' of $s$. 
\begin{definition}
Given two distributions $\nu_1$ and $\nu_2$, let $D_k(\nu_1,\nu_2) = \sup_{f\in\mathcal{F}} |\int f(x)d\nu_1(x) - \int f(x)d\nu_2(x)|$, where $\mathcal{F}$ is the set of all monomials up to degree $k$.
\end{definition} 
\begin{remark}
In general, we will need a condition that $D_k(\nu_{T}, \mu)$ is small enough for some $k$ and some product probability measure $\mu$.
In a stochastic setting where the observations are i.i.d samples, the empirical distribution will weakly converge to the popular distribution(which based on the independence assumption is a product probability measure). 
Therefore, this condition is satisfied given enough observations.
\end{remark} 

We assume $s_i$, the $i$th component of the function $s$, is bounded by a constant $C$. 
In what follows in this paper, we fix $\mu = \mu_1\otimes \ldots \otimes \mu_d$ as some product measure
satisfying $\E_{Y\sim\mu_i}[Y]=0$ and $\kappa_i := \E_{Y\sim \mu_i}[Y^4] - 3\left(\E_{Y\sim \mu_i}[Y^2]\right)^2\neq 0$.
Let 
\[
\xi = \left( 6C^2D_2(\mu, \nu_T) + D_4(\mu, \nu_T)\right),
\]
which is a measure of how dependent the  component signals of $s$ are.
Denote the diagonal matrix $\text{diag}(\kappa_1,\cdots,\kappa_d)$ by $K$. 
Also, denote $\max_{i} \kappa_i$ by $\kappa_{\max}$ and $\min_{i} \kappa_i$ by $\kappa_{\min}$.

The following lemma provides a lower bound for $\min_i |\psi^{\top}A_i|$.
\begin{lemma}
\label{lem:dmin}
With probability at least $1/4$, $\min_i |\psi^{\top}A_i| \ge \frac{\sqrt{2\pi}}{2d}A_{(2,\min)}$.  
\end{lemma}
Denote the event of Lemma \ref{lem:dmin} by $\Epsi$. Let 
\begin{equation}
\label{def:kappa}
\gamma =  \min_{i,j: i\neq j} \left\vert \left(\frac{\phi^{\top}A_i}{\psi^{\top}A_i}\right)^2 - \left(\frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert, 
\end{equation}
The performance of \citet{DHsu2012}'s algorithm depends on this parameters, as shown in the following theorem.
\begin{thm}
 \label{thm:efficiency}
Let 
 \begin{align*}
 Q = &\Big(\frac{4d^7A_{(2,\max)}^4A_{\max}^2\kappa_{\max}\sigma_{\max}^2(A) }{\pi\kappa^2_{\min}A^4_{(2,\min)}\sigma_{\min}^4(A)} \\
 & + \frac{2\sqrt{2\pi}d^6A_{(2,\max)}^2A_{\max}^2}{\pi\kappa_{\min}A^2_{(2,\min)}\sigma_{\min}^2(A)} \Big)
  \xi.
 \end{align*}
 Assume the following conditions hold:
 \begin{enumerate}
 \vspace{-3mm}
 \item $\widehat{M}$ has distinct eigenvalues;
 \item $\gamma > 4\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} Q$
 \item $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\gamma}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } Q$;
 \item $\xi \le \frac{\sqrt{2\pi}\kappa_{\min}A^2_{(2,\min)}\sigma_{\min}^2(A)}{4d^6 A_{(2,\max)}^2A_{\max}^2}$.
  \end{enumerate}
 Then on the event $\Epsi$, there exists a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that for any $k$,
 \[
  \max_{1\le k\le d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4}{\gamma} \frac{\sigma_{\max}^2(A)}{ \sigma_{\min}(A)}Q.
  \]
 \end{thm}
\begin{remark}
Note that the first condition holds with probability 1. Also, the other three conditions will be satisfied when $\xi$ is small enough. 
%However, condition 2 and 3 are rarely satisfied in practice. 
%More discussion about this problem and weaker conditions are discussed in Section \ref{sec:ExpRes}. 
\end{remark}
\begin{remark}
The parameter $1/\gamma$ is essential in the result of the above theorem, in the sense that not only the reconstruction error bound is linear in $1/\gamma$, but the conditions, under which such bound is guaranteed, also require a small $1/\gamma$.

Note that $\gamma$ is the minimal gap of the eigenvalues of $M$.
Consider perturbing a $2\times 2$ matrix $A = \text{diag}([a,b])$ where $a<b$ by $E = \text{diag}([\eps,0])$. 
The eigen-spaces of $A+E$ will stay as $[1,0]$ and $[0,1]$ for $\eps < b-a$, but then they can be arbitrary vectors when $\eps = b-a$.  
Such discontinuity of the eigen-spaces suggests that the dependence on $\gamma$ is necessary for HK algorithm. 

Despite the important role that $\gamma$ plays in the efficiency of HK algorithm, we have no clue about its behavior. 
Even a polynomial (in the dimension $d$) lower bound of $\gamma$ is not yet available, to our best knowledge, in the literature. 
\citet{goyal2014fourier} provided a lower bound for $\gamma$ that is exponential in $d$.
This problem motivates us to refine the HK algorithm.
The idea is inspired by the ideas of \citep{arora2012provable} and \citep{frieze1996learning} using a quasi-whitening procedure, , as shown in the next Subsubsection.
\end{remark}


\section{Refined HK Algorithm}
\label{sec:DICA}
The new algorithm, called Determined ICA (DICA), is as follows. 
\begin{algorithm}[H]
\caption{Determined ICA (DICA)}
\begin{algorithmic}[1]
\INPUT $x(t)$ for $1\le t \le T$. 
\OUTPUT An estimation of the mixing matrix $A$. 
\STATE Sample $\psi$ from a $d$-dimensional standard Gaussian distribution;
\STATE Evaluate $\nabla^2\widehat{f}(\psi)$, \\
%\quad where $\widehat{m_p}(\eta) = \frac{1}{T}\sum_{k=1}^{T} (\eta^{\top}g(k))^p$, and $\widehat{f}(\eta) = \frac{1}{12}\big(\widehat{m_4}(\eta) - 3\widehat{m_2}(\eta)^2 \big)$;
\STATE Compute $\widehat{B}$ such that $\nabla^2\widehat{f}(\psi) = \widehat{B}\widehat{B}^{\top}$;
\STATE Sample $\phi_1$ and $\phi_2$ independently from the standard normal distribution;
\STATE Compute $\widehat{T}_1 = \widehat{G}(\widehat{B}^{-\top}\phi_1)$ and  $\widehat{T}_2 =\widehat{G}(\widehat{B}^{-\top}\phi_2)$;

\STATE Compute all the eigenvectors of $\widehat{M} = \widehat{T}_1\left(\widehat{T}_2\right)^{-1}$, $\{\mu_1,\ldots,\mu_d\}$;
\STATE Return $\widehat{A} = \{\mu_1,\ldots,\mu_d\}$.
\end{algorithmic}
\end{algorithm}

The DICA algorithm can be showed to have provable performance under some good event defined as follows:
\begin{definition}
We denote by $\Ephi$ the following event:
For some fixed constant $L_u$ and $\ell_l$ such that $\ell_l \le \frac{1}{5}\frac{\sqrt{\pi}}{\sqrt{2}d}$ and $L_u \ge 4\sqrt{d}$,
\begin{itemize}
\item $\|\phi_1\|_2 \le L_u$;
\item $\|\phi_2\|_2 \le L_u$, and $\min_i \{|\phi_2^{\top}R_i|\} \ge \ell_l$ where $R_i$ is the $i$th column of some orthonormal matrix $R$ (specified in the Appendix);
\end{itemize} 
\end{definition}  
%Note that $\phi_1$ and $\phi_2$ are sampled independently from standard normal distribution. 
Note that on the event $\Ephi$, $\|\phi_j^{\top}R\|_2\le L_u$ for $j\in\{1,2\}$. 
We will show later that this event $\Ephi$, as well as other events defined later, will hold simultaneously for at least a constant probability.
Recall that $\widehat{G}$ is defined in Equation \eqref{eq:G}.
Let 
\begin{align*}
& \bar{\xi} =   \frac{\sqrt{2}d^6A_{(2,\max)}^2A_{\max}^2}{\sqrt{\pi}\kappa_{\min}A^2_{(2,\min)}\sigma_{\min}^2(A)}\xi, \\
& \widehat{\xi} = \frac{3L_u^2d^7A^2_{\max}}{\pi\kappa_{\min}A^2_{(2,\min)}}\xi + \frac{2\sqrt{6}L_u^2d^2\sigma_{\max}^2(A)}{\pi A^2_{(2,\min)}}\bar{\xi},
\end{align*} 
and 
\begin{equation}
\label{def:gammaR}
\gamma_R =  \min_{i,j: i\neq j} \left\vert \left(\frac{\phi_1^{\top}R_i}{\phi_2^{\top}R_i}\right)^2 - \left(\frac{\phi_1^{\top}R_j}{\phi_2^{\top}R_j}\right)^2 \right\vert, 
\end{equation}
for the same orthonormal matrix $R$ in the event $\Ephi$.
%Similar to Lemma \ref{lem:dmin}, on the event $\Ephi$ with probability at least $1/4$, $\text{min}_i |\phi_1^{\top}R_i| \ge \frac{\sqrt{2\pi}}{2d}$ (respectively $\text{min}_i |\phi_2^{\top}R_i| \ge \frac{\sqrt{2\pi}}{2d}$). 
%Denote this event by $\Ephione$ (respectively $\Ephitwo$).
The performance of the algorithm is guaranteed by the following theorem.
\begin{thm}
\label{thm:Modefficiency}
Let 
 \[ 
 Q=  
 %\frac{32\sqrt{2}d^5\kappa^{3/2}_{\max}A^6_{(2,\max)}}{\pi^{5/2}\kappa_{\min}A^4_{(2,\min)}} \sqrt{\widehat{\xi}}.
 \frac{4L_u^2A^6_{(2,\max)}}{l_l^4 A^6_{(2,\min)}}\widehat{\xi}.
 \] 
 Assume the following conditions hold:
 \begin{enumerate}
 \item $\widehat{T}$ has distinct eigenvalues;
 \item $\gamma_R > 4 \frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) }\|E\|_2$; 
 \item $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\gamma_R}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } \|E\|_2$;
 \item $\xi \le \frac{\sqrt{\pi}\kappa_{\min}A^2_{(2,\min)}\sigma_{\min}^2(A)}{3\sqrt{2}d^6A_{(2,\max)}^2A_{\max}^2}$
 (so $\bar{\xi} \le 1/3$);
 \item $\widehat{\xi} \le \frac{l_l^2 A^2_{(2,\min)}}{2A^2_{(2,\max)}}$.
 \end{enumerate}
Then on the event $\Epsi \cap\Ephi$, there exists a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that for $1\le k\le d$,
\[
%\|c_k\widehat{R}_{\pi(k)} - (R^*R)_k\|_2 \le \frac{4}{\gamma_R} \|\widehat{T} - T\|_2\,,
\| c_k\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4\sigma^2_{\max}(A)}{\gamma_R\sigma_{\min}(A)} Q.
\]
\end{thm}
\begin{remark}
The 4th condition in fact could be implied by the 5th condition, if $\ell_l \ge 1$.
\end{remark}

\subsection{Behavior of $\gamma_R$}
\label{subsec:gammaR}
Similarly, the result of Theorem \ref{thm:Modefficiency} essentially depends on $\gamma_R$. 
However, a constant-probability lower bound for $\gamma_R$ can be developed (while it is not the case for $\gamma$), which makes Theorem \ref{thm:Modefficiency} more meaningful. 
Note that $\phi_1$ and $\phi_2$ are independently sampled from standard Gaussian distribution. 
Thus, $\{\phi_1^{\top}R_1, \cdots, \phi_1^{\top}R_d,$ $\phi_2^{\top}R_1, \cdots, \phi_2^{\top}R_d\}$ are $2d$ independent standard normal random variables. 
Let $Z_i = \frac{\phi_1^{\top}(R)_i}{\phi_2^{\top}(R)_i}$. Therefore, $Z_i$, $1\le i\le d$ are $d$ independent Cauchy$(0,1)$ random variables. 

Moreover for any $1\le i, j \le d$, to analyze $\left\vert Z_i^2 - Z_j^2\right\vert
$, WLOG we can assume both $Z_i$ and $Z_j$ are positive. Then, 
\begin{align*}
\gamma_R =	& \min_{i\neq j} \left\vert Z_i^2 - Z_j^2 \right\vert \\
	=		& \min_{i\neq j}\left\vert Z_i - Z_i \right\vert	\left\vert Z_i + Z_i \right\vert \\
	\ge 	& 2\min_i\vert Z_i\vert\min_{i\neq j} \left\vert Z_i - Z_j \right\vert.
\end{align*}
%Recall that on the event $\Ephione$, $\min_i\vert Z_i \vert \ge \frac{\sqrt{2\pi}}{2d}$. 
Also note that $\min_{i\neq j} \left\vert Z_i - Z_j \right\vert$ is the distribution of the minimal spacing of Cauchy random variables.

\begin{lemma}
\label{lem:CauchyGap}
Let $C = \frac{\pi}{2d^2}h$ for $0\le h\le 1$. 
With probability at least $1-h$,
\[
\min_{i\neq j} \left\vert Z_i - Z_j \right\vert \ge C, \text{and} \,  \min_i\vert Z_i\vert \ge C.
\]
\end{lemma}
Thus, $\gamma_R \ge C^2$ with probability at least $1-h$.
Denote this event by $\EZ$.
Let  
\[
\E = \Epsi \cap\Ephi \cap \EZ.
\]
It remains to argue that the probability of $\E$ is a constant.
\begin{thm}
\label{thm:ConstantProb}
The event $\E$ holds with probability at least $1/4\left(1-h-\ell-2\exp(-x) \right)$. 
Thus, picking $\ell_l = \frac{1}{5}\frac{\sqrt{\pi}}{\sqrt{2}d}$, $L_u = 4\sqrt{d}$ and $C = \frac{\pi}{10d^2}$, $\Prob{\E}\ge 1/20$.
\end{thm}
\begin{remark}
Note that all the constants in Lemma \ref{thm:ConstantProb} are polynomial in $d$(or $d^{-1}$ for the lower bound), thus the result of Theorem \ref{thm:Modefficiency} is polynomial in $d$ with at probability at $1/20$.
\end{remark}
 
\section{Experimental Results}
\label{sec:ExpRes}
\subsection{Comparison of $\gamma$ and $\gamma_R$}
\label{subsec:comparisonGamma}
Even though theoretical result for $\gamma$ is not available, we compare its behavior and that of $\gamma_R$ empirically. 
In this section, we test the expected values of $1/\gamma$ and $1/\gamma_R$ for general $A_1$ and high coherent matrix $A_2$. 
In particular, we generate $A_1$ by standard normal distribution, and $A_2 = v_b*\emph{1}' + 0.3*P$ where $\emph{1}$ is the column vector with all entries being 1, and both $v_b$ and the matrix $P$ are generated from standard normal distribution (with different dimensions). 
Another orthonormal matrix R is generated by computing the left column space of a non-singular matrix $N$ that is also generated from  standard normal distribution.  

\subsection{Simulations}
We investigate the performances of different ICA algorithms in a simulation setting. In particular, 3 algorithms are tested: 
\begin{itemize}
\item \citet{DHsu2012}'s algorithm(HK);
\item The refined \citet{DHsu2012}'s algorithm (DICA);
\item The default FastICA algorithm in the 'ITE' toolbox \cite{szabo12separation} (FICA). 
\end{itemize}
We do not test the algorithm of \citep{anandkumar2012tensordecomposition} because the tensor decomposition takes too long to converge and achieve a valid solution. 
\subsubsection{Data generation}
In the simulation, a common mixing matrix $A$ is generated in a same way as that for $A_1$ or $A_2$ in Subsection \ref{subsec:comparisonGamma}. 
Then we sample $k$ groups of observations, each having size $n$. 
For each observation $y$, $y = Ax+ c\times\eps$ where the signal $x$ is generated from either a product measure of $d$ uniform distributions $\text{Unif}(-\frac12, \frac12)$ or a BPSK signal with different frequency for each component, and $\eps$ is generated from a standard $d$-dimensional Gaussian distribution. 
When $x$ is a BPSK signal, in order to have the components of $x$ are close to independent, we need the ratio of their frequencies are irrational.
All the algorithms will be evaluated on these common $k$ group of observations.
We test the noise ratio $c$ from 0(noiseless) to 1(heavily noisy). 
\subsubsection{Error measures}
We measure the performances of the algorithms on three different measures for different purposes.

The first measure is the parameter recover error. in particular, we evaluate the following quantity between the true mixing matrix $A$ and the one returned by the algorithms $\widehat{A}$:
\begin{equation}
\label{equ:parerror}
\min_{\Pi,S} \|\widehat{A}\Pi S - A\|_{\text{Frob}},
\end{equation}
where $\Pi$ is a permutation matrix, and $S$ is a column scaling matrix (diagonal).

The second measure we used is an approximation of Equation \eqref{equ:parerror} proposed by \citet{comon1994independent}. 
Note that to evaluate Equation \eqref{equ:parerror} one has to enumerate all the permutation $\Pi$, which is not affordable in practice for a large dimension $d$. 
This error helps avoid this computation problem. 

The last measure is the divergence between the joint distribution of the recovered signals and the product of its marginal ones. 
This measure is common in practice when the true mixing matrix $A$ is unknown. 
Moreover, this measure can be reduced to the sum of the entropies of its marginal distribution, as shown in \cite{Learned-Miller:2003:IUS:945365.964306}. 
In particular for an output $\widehat{A}$, we evaluate the following quantity:
\begin{equation}
\sum_{i = 1}^{d} \text{Entropy}(\widehat{x_i}) + \log |\widehat{A}|,
\end{equation}
where $\widehat{x} = \widehat{A}^{-1}y$, and $|\widehat{A}|$ is the absolute value of the determine of $\widehat{A}$. We also use the entropy estimation function 'HShannon\_kNN\_k\_estimation' in the 'ITE' toolbox \cite{szabo14information}.

%For each algorithm, we also measure its running time. 
\subsubsection{Results}


\section{Conclusion}

\if0
\begin{figure}[h]
\vspace{.3in}
\centerline{\fbox{This figure intentionally left non-blank}}
\vspace{.3in}
\caption{Sample Figure Caption}
\end{figure}


\begin{table}[h]
\caption{Sample Table Title} \label{sample-table}
\begin{center}
\begin{tabular}{ll}
{\bf PART}  &{\bf DESCRIPTION} \\
\hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}
\fi

\subsubsection*{Acknowledgements}


%\newpage 
\bibliography{DICA}
\bibliographystyle{plainnat}

\newpage
\include{appendix}
\end{document}
