\documentclass[twoside]{article}
\usepackage{aistats2014}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[authoryear]{natbib}
\usepackage{enumerate}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}

\newcommand{\xcom}[1]{x_{#1}}
\newcommand{\scom}[1]{s_{#1}}
\newcommand{\cset}[2]{\left\{#1\,:\,#2\right\}}
\newcommand{\Ephione}{\mathbb{E}_{\phi_1}}
\newcommand{\Ephitwo}{\mathbb{E}_{\phi_2}}
\newcommand{\Epsi}{\mathbb{E}_{\psi}}
\newcommand{\Ephi}{\mathbb{E}_{\phi}}
\newcommand{\EZ}{\mathbb{E}_{Z}}
\newcommand{\cN}{\cal{N}}
\renewcommand{\P}{{\mathcal P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ex}[1]{\mathbb{E}[#1]}
\newcommand{\Em}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\snorm}[1]{\left\|#1\right\|} % scaling norm
\newcommand{\lmax}[1]{\lambda_{\mathrm{max}}(#1)}
\newcommand{\lmin}[1]{\lambda_{\mathrm{min}}(#1)}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\dprod}[2]{\langle #1,#2 \rangle_{M}}
\newcommand{\hA}{\hat{A}}
\newcommand{\hb}{\hat{b}}
\newcommand{\hC}{\hat{C}}
\newcommand{\hAp}{\hat{A}^\prime}
\newcommand{\hbp}{\hat{b}^\prime}
\newcommand{\hCp}{\hat{C}^\prime}
\renewcommand{\th}{\theta}
\newcommand{\vth}{\hat{\theta}_{V}}
\newcommand{\lth}{\hat{\theta}_{\lambda}}
\newcommand{\hth}{\hat{\theta}}
\newcommand{\ltho}{\hat{\theta}_{\lambda_1}}
\newcommand{\ltht}{\hat{\theta}_{\lambda_2}}
\newcommand{\hlth}{\hat{\theta}_{\hat{\lambda}}}
\newcommand{\slth}{\hat{\theta}_{\lambda^*}}
\newcommand{\plth}{\hat{\theta}_{\lambda_p}}
\newcommand{\hthp}{\hat{\theta}^\prime}
\newcommand{\lthp}{\hat{\theta}^\prime_{\lambdap}}
\newcommand{\ath}{\hat{\theta}_{a}}
\newcommand{\thetap}{\theta^\prime}
\newcommand{\sth}{\theta^*}
\newcommand{\ind}[1]{\mathbb{I}_{\left\{ #1 \right\}}}
\newcommand{\hzeta}{\hat{\zeta}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\cf}{\emph{cf.}}
\newcommand{\T}[1]{T\left( #1 \right)}
\newcommand{\St}[1]{S\left( #1 \right)}
\newcommand{\Ts}[2]{T_{#1}\left( #2 \right)}
\newcommand{\Ss}[2]{S_{#1}\left( #2 \right)}
\newcommand{\Ord}[1]{O\left( #1 \right)}
\newcommand{\etc}{\emph{etc.}}
\newcommand{\bsth}{\bar{\theta}^*}
\newcommand{\sal}{\emph} %Emphasis the J.D. Salinger way :)
\newcommand{\gradL}{\nabla \mathcal{L}}
\newcommand{\lambdap}{{\lambda^\prime}}
\newcommand{\lambdahp}{\hat{\lambda}^\prime}
\newcommand{\pqLoss}[1]{\mathcal{L}_{#1}}
\newcommand{\pLoss}[2]{\pqLoss{#1}( #2 )}
\newcommand{\qLoss}{\pqLoss{M}}
\newcommand{\Loss}[1]{\qLoss(#1)}
\newcommand{\hpqLoss}[1]{\mathcal{\hat{L}}_{#1}}
\newcommand{\hpLoss}[2]{\hpqLoss{#1}( #2 )}
\newcommand{\hqLoss}{\hpqLoss{M}}
\newcommand{\hLoss}[1]{\hqLoss(#1)}
\newcommand{\bX}{\bar{X}}
\newcommand{\bY}{\bar{Y}}
\newcommand{\rth}{\hat{\theta}}
\newcommand{\tth}{\tilde{\theta}}
\newcommand{\maxeig}{\nu_{\max}}
\newcommand{\mineig}{\nu_{\min}}
\newcommand{\ra}{\rightarrow}
\newcommand{\real}{\mathbb{R}}
\newcommand{\one}[1]{\mathbf{1}_{\{#1\}}}
\newcommand{\rl}[1]{\mathbb{R}^{#1}}
\newcommand{\Mset}{\mathcal{M}(\varepsilon)}
\newcommand{\Cset}{\mathcal{C}}
\newcommand{\truel}{L}
\newcommand{\zol}{L_{\mathrm{0-1}}}
\newcommand{\hinl}{L_{\mathrm{hinge}}}
\newcommand{\phil}{L_{\varphi}}
\newcommand{\lspace}{\left\{1, \dots,K \right\}}
\newcommand{\risk}{\mathcal{R}}
\newcommand{\scoref}{\in \mathcal{H}}
\newcommand{\subscoref}{\in \mathcal{H}^\prime}
\newcommand{\hQ}{\hat{Q}}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\esssup}{ess\,sup}
\renewcommand{\natural}{\mathbb{N}}


% theorems/definitions
\newtheorem{lemma}{Lemma}[section]
\newtheorem{thm}[lemma]{Theorem}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{cor}[lemma]{Corollary}
\newtheorem{example}[lemma]{Example}
\newtheorem{prop}[lemma]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{remark}[lemma]{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{note}[lemma]{Note}
\newtheorem{problem}[lemma]{Problem}

\newcommand{\FF}{\mathcal{F}}
\newcommand{\TT}{\mathcal{T}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\KK}{\mathcal{K}}
\newcommand{\eps}{\varepsilon}

% If your paper is accepted, change the options for the package
% aistats2014 as follows:
%
%\usepackage[accepted]{aistats2014}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Deterministic Independent Component Analysis}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
Abstract.
\end{abstract}

\section{Introduction}
\label{sec:Intro}
Independent Component Analysis (ICA), as a main tool for separating blind sources, has received much attention in the past decades. 
The ICA model assumes a $d$-dimensional vector $X$ is a linear mixture of $d$ independent variables $(S_1,\ldots, S_d)$ with Gaussian noise:
\[
X = AS+\epsilon,
\]
where $\epsilon$ is a $d$-dimensional Gaussian noise, and $A$ is a nonsingular mixing matrix.
Our interest is to reconstruct $A$ given observations of $X$ with rigorous guarantee.

The literature of the ICA model is vast in both practical algorithms and theoretical analysis. 
We refer to the book of \citet{comon2010handbook} for a comprehensive survey.
Perhaps the most popular ICA algorithm in engineering application would be the FastICA \citep{hyvarinen1999fast}. 
In general, FastICA tries to find a linear transformation $W$ for $X$ by optimizing a particular designed contrast function, 
such that the resulted coordinates of $WX$ are as independent as possible. 
The optimal $W$ would be $A^{-1}$ under additional assumptions about the sources, thereby $A$ is recovered.  
Theoretical analysis of the FastICA algorithm has been conducted in many aspects \citep{tichavsky2006performance,oja2006fastica,ollila2010deflation,dermoune2013fastica,wei2014convergence}, 
but as far as we know, finite sample bound for FastICA is still not available.
Besides, consistency of the estimation of $A$ can only be guaranteed for a particular forth-moments-based contrast function with noise-less observations(i.e. $X = AS$) \citep{miettinen2014fourth}. 

Another class of ICA methods views ICA in a semiparametric manner with parameters $(W, p_1, \ldots, p_d)$ where $W$ is the targeted matrix, and nuisance parameters $p_i$ is the density function of the $i$-th source. 
Under additional statistical assumptions about the source signals, consistent estimators have been proposed for various settings. The first  $\sqrt{n}$-consistent estimator of $A$ (which implies a finite sample bound in the rate of $\sqrt{n}$), to our best knowledge, is proposed and analyzed by \citet{samarov2004nonparametric}.  
Later \citet{chen2005consistent} showed that the estimator proposed in \citet{eriksson2003characteristic} is also $\sqrt{n}$-consistent. Another estimator which is more efficient is also proposed by the same authors in \citep{chen2006efficient}.
However, these results all assume a noise-free setting. 
Moreover, they involve an intermediate procedure of estimating a probability function or a score function, which consequentially invites extra technical conditions. 

The first provable ICA method that can deal with the noisy case is attributed to \citet{arora2012provable}. 
The idea is still based on the forth moments. 
After a quasi-whitening procedure, the ICA problem is reduced into a problem of finding all the local optimizer of a specific function defined using the forth order cumulant. Then a polynomial-time algorithm is proposed to find all the local optimizers with theoretical guarantee.
Despite its soundness in theory, this algorithm requires an exhaustive search for an input parameter, $\beta$ in the paper.
The interval of the valid $\beta$s is not yet well understood, which impairs the usefulness of the algorithm in practice.

The forth moment statics are also used in many other ICA related works. 
Recently, a series of more promising methods are proposed based on the algebraic structure of the model \citep{hsu2013learning,anandkumar2012tensordecomposition,anandkumar2012method}. 
In fact, this idea has been discussed earlier as a intuitive argument to construct a contrast function \citep{cardoso1999high}. 
The first rigorous proof of this idea is developed using matrix perturbation tools in a general tensor perspective \citep{anandkumar2012tensordecomposition,anandkumar2012method,goyal2014fourier}. 
However, these results exponentially depend on the number of source signals $d$.
More specifically, these methods all require an eigen-decomposition of some flatten tensor where the minimal gap between the eigenvalues plays an essential role. 
A naive analysis of this gap will introduce an exponential dependence on the dimension of the flatten tensor $d$. 
This dependence is also observed in the papers of \citet{cardoso1999high} and \citet{goyal2014fourier}.
One way to circumvent such dependence would be directly decomposing a high order tensor using power method which requires no flatten procedure \citep{anandkumar2014guaranteed}. 
However, it is well known that power method is unstable in practice for high order tensors. 
Another drawback is that this method, when applied to the ICA problem, introduces an error term that does not approach 0 as the sample size approaches infinity. 

In this paper we proposed a practical method for ICA which also has meaningful theoretical guarantee (polynomial-time; with constant probability the reconstruction error of $A$ polynomially depends on all the parameters and vanishes as the sample size going to infinity).
Our algorithm is a refined version of the ICA algorithm proposed by \cite{hsu2013learning} (HKICA).  
Another contribution of the present paper is that our theoretical analysis is conducted in a deterministic manner. 
In practice ICA is also known to work well for unmixing the mixture of various deterministic signals. 
One of the classic demonstrations of ICA is showing that the periodic signals can be well recovered from their mixtures \citep{HyvOja00}.
Such phenomenon suggests that the usual probabilistic notion is unsatisfactory if one wishes to have deeper understanding of ICA.   
Our deterministic analysis helps investigate this curious phenomenon without losing any generality to the traditional stochastic setting. 

Formally, let $s:\natural \ra \real^d$ be a $d$-dimensional deterministic ``signal''. 
Denote by $s_i$ the $i$th component of $s$.
We present the ICA model as a linear mixture of functions (details will be introduced later).
Then we analyze the HKICA algorithm \citep{hsu2013learning} under this presentation using a measure of functional dependence.
A detailed analysis shows that its performance depends on the minimal gap of the eigenvalues (of some random matrix. The exact form is presented later.).
A meaningful lower bound  (polynomial in the parameters of the ICA model) for this gap is essential for the soundness of this algorithm, but not yet available \citep{cardoso1999high}.
A key step of our algorithm is reducing such minimal eigenvalue gap problem to the problem of minimal spacing of i.i.d. Cauchy random variables, for which a polynomial lower bound is available, by the quasi-whitening procedure.
This idea is inspired by \citet{frieze1996learning} and \citet{arora2012provable}.
With this lower bound, we then show that the new algorithm has provable guarantee on the reconstruction error of $A$. 

The rest of this paper is organized as follows: 
We introduce some preliminaries in Section \ref{sec:Preliminaries}. Section \ref{sec:AnalysisHK} is devoted to the analysis of the HKICA algorithm. Then in section \ref{sec:DICA} we propose our algorithm and show that it has the 'real' provable guarantee.
Lastly, experimental results are reported in Section \ref{sec:ExpRes}.

\subsection{Notations}
Denote the maximal (respectively minimal) singular value of a matrix $A$ by  $\sigma_{\max}(A)$ (respectively $\sigma_{\min}(A)$). Also, let $A_{(2,\min)} = \min_{i} \|A_i\|_2$, $A_{(2,\max)} = \max_{i} \|A_i\|_2$, and $A_{\max} = \max_{i,j} |A_{i,j}|$.

\section{Preliminaries}
\label{sec:Preliminaries}
We discuss the induced probability measure of a function in this section, which serves as the basis of our deterministic analysis. 
Then we propose the ICA problem in a deterministic manner. 
\subsection{Induced Probability}
\label{subsec:InducesProb}
Given arbitrary measurable domains $(\Omega,\AA)$ and a sequence of probability measures $(\mu_t)_{t\ge0}$ over it, we define $\nu_t$ to be the probability measure over $\real^d$ that is induced by $s$ and $\mu_t$ for any function $s: \Omega \ra \real^d$ and $t\ge 0$:
In particular, for any Borel set $A\subset \real^d$,
\[
\nu_t(A)= \mu_t\Bigl( \cset{z}{s(z)\in A} \Bigr)
= \mu_t ( s^{-1}(A) ).
\]
Note that if $\Omega = \natural$, $\AA = 2^\natural$ and $\mu_t$ is the uniform probability distribution on $\{0,\ldots,t\}$, then $\nu_t$ is the empirical probability.

In what follows we fix $\Omega$ and $(\mu_t)$ and define all the concepts that follow relative to these. 
In this paper we only consider about real functions.
When the limit of $(\nu_t)$ exists, it will be denoted by $\nu$.
Also, if we want to emphasize the dependence on $s$ then we will use $\nu^{(s)}$.
Similarly, when the dependence of $\nu_t$ on $s$ is important, we will use $\nu^{(s)}_t$.
\begin{definition}
A function $s:\Omega \rightarrow \real^d$ is called \emph{ergodic} w.r.t. $(\mu_t)_{t\ge0}$
if the sequence of  induced probability measures $(\nu_t)_{t\ge 0}$ is weakly convergent.
\end{definition}
A simple example of an ergodic function is a real periodic function.
The following lemma shows that once $\nu$ exists, it is a probability measure. 
\begin{prop}
\label{prop:ergodicfunction}
Let $\scom{i}$ denote the $i$th coordinate function of $s$. Assume that
\[
\lim_{t\to\infty} \int |\scom{i}(x)|\, d\mu_t(x) 
\]
exists and is finite for all $1 \le i \le d$ and that $\esssup_{\mu_t} |\scom{i}|<\infty$ for any $1\le i \le d$ and $t\ge 0$.
Then all limit points of the sequence $\{\nu_t\}$, with respect to the weak topology, are probability measures over $\real^d$.
\end{prop}
\subsection{ICA}
\label{subsec:ICA}
Assume we are given a $d$-dimensional observed function $x(t)$ defined by  
\begin{equation}
\label{equ:ICA}
x(t) = As(t), \quad t\ge0
\end{equation}
where $A$ is a $d\times d$ nonsingular matrix and  $s$ is a $d$-dimensional signal function. 
Note that the above deterministic setting does not lose any generality to the traditional stochastic setting of ICA with Gaussian noise, given that we can rewrite the stochastic setting as 
\[
x = As+\eps = A(s+A^{-1}\eps),
\] 
where $\eps$ is a Gaussian noise. In practice, we are not going to observe the true distribution of $x$, but $x(t)$, $t\ge 0$, which leads to Equation \eqref{equ:ICA}.



\if0
In this section, we propose a natural notion fot the independence of signal functions in an analogous way, which is compatible with the stochastic setting but much more general.
\subsection{Independent Components}
\label{subsec:IndeComp}
Despite the perceptual intuitiveness of the definition of ergodic function, the space of ergodic functions is, unexpectedly, not closed under addition, as shown in Example \ref{eg:ergodic}. 
We provide a sufficient and necessary condition for a linear combination 
of ergodic functions to be ergodic in Proposition \ref{prop:comp}. 
It turns out the space spanned by the components of an ergodic function is a space of ergodic functions.
Such observation leads to our definition of independence of components, and build the foundation of the rest work of our paper.
\begin{example}
\label{eg:ergodic}
Let $f,g:\natural \rightarrow \{-1,+1\}$ and $(\mu_t)$ be defined as follows: 
\[
f(t) = \begin{cases} +1, & t \equiv 0 \pmod 4; \\
					-1, & t \equiv 1 \pmod 4; \\
					+1,  & t \equiv 2  \pmod 4; \\
					-1, & t \equiv 3  \pmod 4,
					\end{cases}
					\]\[
g(t) = \begin{cases} +1, & t\equiv 0 \pmod 4; \\
					-1, & t\equiv 1 \pmod 4; \\
					-1,  & t\equiv 2 \pmod 4; \\
					+1, & t\equiv 3 \pmod 4.
					\end{cases}
\]
\[
\quad \mu_{2t+1}(x) = \begin{cases} 1/2, & x=0; \\
					1/2, & x=1, 
					\end{cases}
\mu_{2t}(x) = \begin{cases} 1/2, & x=2; \\
					1/2, & x=3;.
					\end{cases}
\]
Then, $\nu^{(f)}_t$, $\nu^{(g)}_t$ are both the uniform  distribution on $\{-1,+1\}$, hence they converge and the limit is the same uniform  distribution. 
However, while $\nu^{(f+g)}_{2t+1}$ is the uniform  distribution on $\{-2,+2\}$,  $\nu^{(f+g)}_{2t}$ is the degenerate
distribution that puts all the mass at $0$. Thus, $\nu^{(f+g)}_{t}$ fails to be convergent. 
\end{example}

\begin{prop}\label{prop:comp}
$s = (\scom{1},\ldots,\scom{d})^{\top}$ is a $d$-dimensional ergodic function, if and only if 
 for any $m\times d$ matrix $A$, $x = A s$ is an $m$-dimensional ergodic function.
\end{prop}  
Based on Lemma \ref{prop:comp}, we can now define the independence of the components of a multidimensional ergodic function $f$ based on its induced limiting distribution $\nu^{(f)}$:
\begin{definition}
Given an ergodic function $s = (\scom{1},\ldots, \scom{d})^{\top}:\Omega \rightarrow \real^d$, 
we say that $\{\scom{1},\ldots, \scom{d}\}$ are \emph{independent} components, 
	if $\nu^{(\scom{1})}\otimes\ldots\otimes\nu^{(\scom{d})} = v^{(s)}$.
\end{definition}
\begin{remark}
Note that the existence of $\nu^{(s^{(i)})}$ is guaranteed by Proposition \ref{prop:comp}. 
The definition of independent components can also be extended to the independence of ergodic functions: 
$f$ and $g$ are called independent, if their joint function $(f,g)$ is an ergodic function with independent components. 
\end{remark}
\begin{remark}
Assume that $f = (f_1,f_2)^{\top}:\real\rightarrow\real^2$ is an ergodic function, and both $f_1$ and $f_2$ are periodic with respective periods $T_1$ and $T_2$. 
Let $\mu_t$ be the uniform probability distribution on $[0,t]$. 
%$\mu_t((a,b)) = (b-a)/t$ for $0\le a\le b\le t$.
 If $T_1/T_2$ is irrational, then it is easy to show that $f_1$ and $f_2$ are independent. 
 This example shows how period signals can be fitted into our framework.
\end{remark}
\fi

\section{Analysis of the HKICA Algorithm}
\label{sec:AnalysisHK}
We analyze the performance of the HKICA algorithm in this section.

\subsection{the HKICA algorithm}
\label{subsec:HKalg}
\citet{DHsu2012} proposed an algorithm for ICA based on the idea of moment methods.
For $p\ge 1$, $\eta\in \real^d$, 
let 
\begin{equation}
\label{eq:momnent}
m_p(\eta) = \E_{Y\sim \nu^{(s)}}[ (\eta^\top A Y)^p ]
\end{equation}
and let
\begin{equation}
\label{eq:funcf}
f(\eta) = \frac1{12} \left( m_4(\eta) - 3 m_2(\eta)^2 \right)\,.
\end{equation}
In general, we denote by adding a ``hat'' on the top of a symbol its empirical estimate.
\begin{algorithm}[H]
\caption{the HKICA algorithm}
\begin{algorithmic}[1]
\INPUT $x(t)$ for $1\le t \le T$. 
\OUTPUT An estimation of the mixing matrix $A$. 
\STATE Sample $\phi$ and $\psi$ independently from a standard Gaussian distribution of dimension $d$;
\STATE Evaluate $\nabla^2\widehat{f}(\phi)$ and $\nabla^2\widehat{f}(\psi)$, 
\STATE Compute $\widehat{M} = (\nabla^2 \widehat{f}(\phi))(\nabla^2\widehat{f}(\psi))^{-1}$;
\STATE Compute all the eigenvectors of $\widehat{M}$, $\{\mu_1,\ldots,\mu_d\}$;
\STATE Return $\widehat{A} = (\mu_1,\ldots,\mu_d)$.
\end{algorithmic}
\end{algorithm}
In particular, $\nabla^2\widehat{f}(\eta)$  is evaluated from the observations by
\begin{equation}
\label{eq:G}
\nabla^2 \widehat{f}(\eta) = \widehat{G}(\eta):= \widehat{G_1}(\eta) - \widehat{G_2}(\eta) -2\widehat{G_3}(\eta),
\end{equation}
where 
\begin{align*}
&\widehat{ G_1}(\eta) = \frac1n\sum_{t=1}^{n} \big(\eta^{\top}x(t)\big)^2x(t)x(t)^{\top}; \\
& \widehat{G_2}(\eta) = \frac{1}{n^2}\sum_{t=1}^{n} \big(\eta^{\top}x(t)\big)^2 \sum_{t=1}^{n}x(t)x(t)^{\top}; \\
& \widehat{G_3}(\eta) = \frac{1}{n^2}\Big(\sum_{t=1}^{n} \big(\eta^{\top}x(t)\big)x(t)\Big) \Big(\sum_{t=1}^{n} \big(\eta^{\top}x(t)\big)x(t)\Big)^{\top}.
\end{align*} 
\begin{remark}
\label{rmk:symmetrization}
In practice, the HKICA algorithm may generate complex output. 
This occurs only when $\nabla^2\widehat{f}(\psi)$ is singular. 
Otherwise, the eigenvalues of $\widehat{M}$ would be the same as those of
$\widehat{B}^{-\top}\nabla^2 \widehat{f}(\phi)\widehat{B}^{-1}$ which is symmetric and thus has all real eigenvalues, where $\widehat{B} $ is the square root of $\nabla^2\widehat{f}(\psi)$. 
Note that theoretically singularity of $\nabla^2\widehat{f}(\psi)$ happens with probability 0. 
One way of fixing this problem is keeping sampling $\phi$ and $\psi$ until a real eigen-decomposition exists.

Another possible way, avoiding re-sampling and yet with larger error, is by the symmetrizing trick mentioned above.
Instead of computing $\widehat{M} = (\nabla^2 \widehat{f}(\phi))(\nabla^2\widehat{f}(\psi))^{-1}$, one can calculate the eigen-decomposition of  $\widehat{B}^{-\top}\nabla^2 \widehat{f}(\phi)\widehat{B}^{-1}$. 
Note that $\widehat{B} \approxeq ADR^{\top}$ for some diagonal matrix $D$ and orthonormal matrix $R$. 
Hence such eigen-decomposition will recover $\widehat{R}\approxeq R$. 
Then $A$ can be reconstructed, up to permutation and scaling of columns, by $\widehat{A} = \widehat{B}\widehat{R}$.
Note that $ \widehat{B}^{-\top}\nabla^2 \widehat{f}(\phi)\widehat{B}^{-1}$ is always symmetric, thus yields a real eigen-decomposition. \qed
\end{remark}

Even though the authors believed that the HKICA algorithm has provable guarantee, no rigorous analysis is provided. 
We try to fill the gap in the next section. 
A detailed analysis shows that the performance of the algorithm may be not as trivial as it is claimed.

\subsection{Reconstruction error}
\label{subsec:errorHK}
Our analysis is developed in a deterministic setting.
We try to have weaker assumption instead of assuming independent signals. 
Clearly, no meaningful results will be achieved if $s$ is allowed to go arbitrarily wild. 
We assume $s_i$, the $i$th component of the function $s$, is bounded by a constant $C$. 
In what follows in this paper, we fix $\mu = \mu_1\otimes \ldots \otimes \mu_d$ as some product measure
satisfying $\E_{Y\sim\mu_i}[Y]=0$ and $\kappa_i := \E_{Y\sim \mu_i}[Y^4] - 3\left(\E_{Y\sim \mu_i}[Y^2]\right)^2\neq 0$.
Denote the diagonal matrix $\text{diag}(\kappa_1,\cdots,\kappa_d)$ by $K$. 
Also, denote $\max_{i} \kappa_i$ by $\kappa_{\max}$ and $\min_{i} \kappa_i$ by $\kappa_{\min}$.
We define a `distance' for two distributions, as follows. 
\begin{definition}
Given two distributions $\nu_1$ and $\nu_2$, let $D_k(\nu_1,\nu_2) = \sup_{f\in\mathcal{F}} |\int f(x)d\nu_1(x) - \int f(x)d\nu_2(x)|$, where $\mathcal{F}$ is the set of all monomials up to degree $k$.
\end{definition} 
Let 
\begin{equation}
\xi = \left( 6C^2D_2(\mu, \nu_T) + D_4(\mu, \nu_T)\right),
\end{equation}
which serves as a measure of how dependent the  component signals of $s$ are.

\begin{remark}
\label{rmk:xi}
In general, we will need a condition that $\xi$ is small enough, so that the components of $s$ are 'independent' enough.
In the traditional stochastic setting where the observations are i.i.d samples, the empirical distribution will weakly converge to the popular distribution which, based on the independence assumption, is a product probability measure. 
Therefore, this condition will be satisfied given the sample size is large enough. \qed
\end{remark} 

The following lemma provides a lower bound for $\min_i |\psi^{\top}A_i|$.
\begin{lemma}
\label{lem:dmin}
Let $c = \frac{\sqrt{\pi}A_{(2,\min)}}{\sqrt{2}d} \ell$ for $0\le \ell \le 1$. Then with probability at least $(1-\frac{\ell}{2})^2$, 
$\min_i |\psi^{\top}A_i| \ge \frac{\sqrt{2\pi}}{2d}A_{(2,\min)}\ell$. 
\end{lemma}
Denote the event of Lemma \ref{lem:dmin} by $\Epsi$.
Let 
\begin{equation}
\label{def:kappa}
\gamma_A =  \min_{i,j: i\neq j} \left\vert \left(\frac{\phi^{\top}A_i}{\psi^{\top}A_i}\right)^2 - \left(\frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert. 
\end{equation}
The performance of the HKICA algorithm depends on this parameters, as shown in the following theorem.
\begin{thm}
 \label{thm:efficiency}
Let 
 \begin{align*}
 Q = &\Big(\frac{4d^7A_{(2,\max)}^4A_{\max}^2\kappa_{\max}\sigma_{\max}^2(A) }{\pi\kappa^2_{\min}A^4_{(2,\min)}\sigma_{\min}^4(A)} \\
 & + \frac{2\sqrt{2\pi}d^6A_{(2,\max)}^2A_{\max}^2}{\pi\kappa_{\min}A^2_{(2,\min)}\sigma_{\min}^2(A)} \Big)
  \xi.
 \end{align*}
 Assume the following conditions hold:
 \begin{enumerate}
 \vspace{-3mm}
 \item $\widehat{M}$ has distinct eigenvalues;
 \item $\gamma_A > 4\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} Q$
 \item $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\gamma_A}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } Q$;
 \item $\xi \le \frac{\sqrt{2\pi}\kappa_{\min}A^2_{(2,\min)}\sigma_{\min}^2(A)}{4d^6 A_{(2,\max)}^2A_{\max}^2}$.
  \end{enumerate}
 Then on the event $\Epsi$, there exists a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that for any $k$,
 \[
  \max_{1\le k\le d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4}{\gamma_A} \frac{\sigma_{\max}^2(A)}{ \sigma_{\min}(A)}Q.
  \]
 \end{thm}
\begin{remark}
Note that the first condition holds with probability 1. Also, the other three conditions will be satisfied when $\xi$ is small enough (for a fixed $\gamma_A$).
In particular, as mentioned in the Remark \ref{rmk:xi}, these conditions will be satisfied in the traditional stochastic setting of ICA (for a fixed $\gamma_A$), given large enough sample size. \qed
%However, condition 2 and 3 are rarely satisfied in practice. 
%More discussion about this problem and weaker conditions are discussed in Section \ref{sec:ExpRes}. 
\end{remark}
\begin{remark}
The parameter $1/\gamma_A$ is essential in the result of the above theorem, in the sense that not only the reconstruction error bound is linear in $1/\gamma_A$, but the conditions, under which such bound is guaranteed, also require a small $1/\gamma_A$.

Note that $\gamma_A$ is actually the minimal gap of the eigenvalues of $M$.
Consider perturbing a $2\times 2$ matrix $A = \text{diag}([a,b])$ where $a<b$ by $E = \text{diag}([\eps,0])$. 
The eigen-spaces of $A+E$ will stay as $[1,0]$ and $[0,1]$ for $\eps < b-a$, but then they can be arbitrary vectors when $\eps = b-a$.  
Such discontinuity of the eigen-spaces suggests that the dependence on $\gamma_A$ is necessary for the HKICA algorithm because of the eigen-decomposition involved. 

Despite the important role that $\gamma_A$ plays in the efficiency of the HKICA algorithm, we have no clue about its behavior. 
Even a polynomial (in the dimension $d$) lower bound of $\gamma_A$ is not yet available, to our best knowledge, in the literature. 
\citet{goyal2014fourier} provided a lower bound for $\gamma_A$ that is exponential in $d$.
This problem motivates us to refine the HKICA algorithm.
The idea is inspired by the ideas of \citet{arora2012provable} and \citet{frieze1996learning} using a quasi-whitening procedure, as shown in the next section. \qed
\end{remark}


\section{A Refined HKICA Algorithm}
\label{sec:DICA}
Recall that $\widehat{G}$ is defined in Equation \eqref{eq:G}.
Our new algorithm, named as Determined ICA (DICA), is as follows. 
\begin{algorithm}[H]
\caption{Determined ICA (DICA)}
\begin{algorithmic}[1]
\INPUT $x(t)$ for $1\le t \le T$. 
\OUTPUT An estimation of the mixing matrix $A$. 
\STATE Sample $\psi$ from a $d$-dimensional standard Gaussian distribution;
\STATE Evaluate $\nabla^2\widehat{f}(\psi)$, \\
%\quad where $\widehat{m_p}(\eta) = \frac{1}{T}\sum_{k=1}^{T} (\eta^{\top}g(k))^p$, and $\widehat{f}(\eta) = \frac{1}{12}\big(\widehat{m_4}(\eta) - 3\widehat{m_2}(\eta)^2 \big)$;
\STATE Compute $\widehat{B}$ such that $\nabla^2\widehat{f}(\psi) = \widehat{B}\widehat{B}^{\top}$;
\STATE Sample $\phi_1$ and $\phi_2$ independently from the standard normal distribution;
\STATE Compute $\widehat{T}_1 = \widehat{G}(\widehat{B}^{-\top}\phi_1)$ and  $\widehat{T}_2 =\widehat{G}(\widehat{B}^{-\top}\phi_2)$;

\STATE Compute all the eigenvectors of $\widehat{M} = \widehat{T}_1\left(\widehat{T}_2\right)^{-1}$, $\{\mu_1,\ldots,\mu_d\}$;
\STATE Return $\widehat{A} = \{\mu_1,\ldots,\mu_d\}$.
\end{algorithmic}
\end{algorithm}
\begin{remark}
The difficult in analyzing $\gamma_A$ of the HKICA algorithm is due to its dependence on the unknown mixing matrix $A$. 
The way of DICA to avoid this problem can be viewed in a tensor perspective. 
Instead of flattening two $4$-dimensional tensors into matrices simply by marginalizing in a random direction, in DICA we marginalize in a structured direction as $\psi^{\top}A^{-1}$ for a random $\psi$. 
Another possible interpretation could be in a  probabilistic perspective, as sampling $\phi_1$ and $\phi_2$ based on an estimation of the variance matrix. 
In particular, we would like to sample $\phi$ from a Gaussian distribution $N(0,V)$ where $V = A^{-\top}A^{-1}$, so that $\{\phi^{\top}A_1, \ldots, \phi^{\top}A_d \}$ is independent and hence $\gamma_A$ could be easier to analyze. 
Despite the true value of $A^{-\top}A^{-1}$ is not available, we can estimate it by $\left(\nabla^2\widehat{f}(\psi)\right)^{-1}$ up to permutation and scaling,
which can still maintain the independence of $\{\phi^{\top}A_1, \ldots, \phi^{\top}A_d \}$). 
%Then in DICA, $\widehat{B}^{-\top}\phi$ is a way of sampling such direction $v$.
This perspective also suggest that a better estimation of $A^{-\top}A^{-1}$ would help improve DICA. \qed
\end{remark}
\begin{remark}
Similarly, the DICA algorithm also suffers the complex output. 
A same symmetrizing trick can be applied again, as in Remark \ref{rmk:symmetrization}. \qed 
\end{remark}

The DICA algorithm can be showed to have provable performance under some good events defined as follows:
\begin{definition}
We denote by $\Ephi$ the following event:
For some fixed constant $L_u$ and $\ell_l$ such that $\ell_l \le \frac{1}{5}\frac{\sqrt{\pi}}{\sqrt{2}d}$ and $L_u \ge 4\sqrt{d}$,
\begin{itemize}
\item $\|\phi_1\|_2 \le L_u$;
\item $\|\phi_2\|_2 \le L_u$, and $\min_i \{|\phi_2^{\top}R_i|\} \ge \ell_l$ where $R_i$ is the $i$th column of some orthonormal matrix $R$ (specified in the Appendix);
\end{itemize} 
\end{definition}  
%Note that $\phi_1$ and $\phi_2$ are sampled independently from standard normal distribution. 
Note that on the event $\Ephi$, $\|\phi_j^{\top}R\|_2\le L_u$, $j\in\{1,2\}$ for any orthonormal matrix $R$. 
We will show later that this event $\Ephi$, as well as other events defined later, will hold simultaneously with high probability.
Let 
\begin{align*}
& \bar{\xi} =   \frac{\sqrt{2}d^6A_{(2,\max)}^2A_{\max}^2}{\sqrt{\pi}\kappa_{\min}A^2_{(2,\min)}\sigma_{\min}^2(A)}\xi, \\
& \widehat{\xi} = \frac{3L_u^2d^7A^2_{\max}}{\pi\kappa_{\min}A^2_{(2,\min)}}\xi + \frac{2\sqrt{6}L_u^2d^2\sigma_{\max}^2(A)}{\pi A^2_{(2,\min)}}\bar{\xi},
\end{align*} 
and 
\begin{equation}
\label{def:gammaR}
\gamma_R =  \min_{i,j: i\neq j} \left\vert \left(\frac{\phi_1^{\top}R_i}{\phi_2^{\top}R_i}\right)^2 - \left(\frac{\phi_1^{\top}R_j}{\phi_2^{\top}R_j}\right)^2 \right\vert, 
\end{equation}
for the same orthonormal matrix $R$ in the event $\Ephi$.
%Similar to Lemma \ref{lem:dmin}, on the event $\Ephi$ with probability at least $1/4$, $\text{min}_i |\phi_1^{\top}R_i| \ge \frac{\sqrt{2\pi}}{2d}$ (respectively $\text{min}_i |\phi_2^{\top}R_i| \ge \frac{\sqrt{2\pi}}{2d}$). 
%Denote this event by $\Ephione$ (respectively $\Ephitwo$).
The performance of the algorithm is guaranteed by the following theorem.
\begin{thm}
\label{thm:Modefficiency}
Let 
 \[ 
 Q=  
 %\frac{32\sqrt{2}d^5\kappa^{3/2}_{\max}A^6_{(2,\max)}}{\pi^{5/2}\kappa_{\min}A^4_{(2,\min)}} \sqrt{\widehat{\xi}}.
 \frac{4L_u^2A^6_{(2,\max)}}{l_l^4 A^6_{(2,\min)}}\widehat{\xi}.
 \] 
 Assume the following conditions hold:
 \begin{enumerate}
 \item $\widehat{T}$ has distinct eigenvalues;
 \item $\gamma_R > 4 \frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) }\|E\|_2$; 
 \item $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\gamma_R}\frac{\sigma_{\max}^2(A)}{\sigma_{\min}(A) } \|E\|_2$;
 \item $\xi \le \frac{\sqrt{\pi}\kappa_{\min}A^2_{(2,\min)}\sigma_{\min}^2(A)}{3\sqrt{2}d^6A_{(2,\max)}^2A_{\max}^2}$
 (so $\bar{\xi} \le 1/3$);
 \item $\widehat{\xi} \le \frac{l_l^2 A^2_{(2,\min)}}{2A^2_{(2,\max)}}$.
 \end{enumerate}
Then on the event $\Epsi \cap\Ephi$, there exists a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that for $1\le k\le d$,
\[
%\|c_k\widehat{R}_{\pi(k)} - (R^*R)_k\|_2 \le \frac{4}{\gamma_R} \|\widehat{T} - T\|_2\,,
\| c_k\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4\sigma^2_{\max}(A)}{\gamma_R\sigma_{\min}(A)} Q.
\]
\end{thm}
\begin{remark}
The 4th condition in fact could be implied by the 5th condition, if $\ell_l \ge 1$. \qed
\end{remark}

\subsection{Behavior of $\gamma_R$}
\label{subsec:gammaR}
Similarly, the result of Theorem \ref{thm:Modefficiency} essentially depends on $\gamma_R$. 
However, a constant-probability lower bound for $\gamma_R$ can be developed (while it is not the case for $\gamma_A$), which makes Theorem \ref{thm:Modefficiency} more meaningful. 
Note that $\phi_1$ and $\phi_2$ are independently sampled from standard Gaussian distribution. 
Thus, $\{\phi_1^{\top}R_1, \cdots, \phi_1^{\top}R_d,$ $\phi_2^{\top}R_1, \cdots, \phi_2^{\top}R_d\}$ are $2d$ independent standard normal random variables. 
Let $Z_i = \frac{\phi_1^{\top}(R)_i}{\phi_2^{\top}(R)_i}$. Therefore, $Z_i$, $1\le i\le d$ are $d$ independent Cauchy$(0,1)$ random variables. 

Moreover for any $1\le i, j \le d$, to analyze $\left\vert Z_i^2 - Z_j^2\right\vert
$, WLOG we can assume both $Z_i$ and $Z_j$ are positive. Then, 
\begin{align*}
\gamma_R =	& \min_{i\neq j} \left\vert Z_i^2 - Z_j^2 \right\vert \\
	=		& \min_{i\neq j}\left\vert Z_i - Z_i \right\vert	\left\vert Z_i + Z_i \right\vert \\
	\ge 	& 2\min_i\vert Z_i\vert\min_{i\neq j} \left\vert Z_i - Z_j \right\vert.
\end{align*}
%Recall that on the event $\Ephione$, $\min_i\vert Z_i \vert \ge \frac{\sqrt{2\pi}}{2d}$. 
Also note that $\min_{i\neq j} \left\vert Z_i - Z_j \right\vert$ is the distribution of the minimal spacing of Cauchy random variables.

\begin{lemma}
\label{lem:CauchyGap}
Let $c = \frac{\pi}{2d^2}h$ for $0\le h\le 1$. 
With probability at least $1-h$,
\[
\min_{i\neq j} \left\vert Z_i - Z_j \right\vert \ge c, \text{and} \,  \min_i\vert Z_i\vert \ge c.
\]
\end{lemma}
Thus, $\gamma_R \ge c^2$ with probability at least $1-h$.
Denote this event by $\EZ$.
Let  
\[
\E = \Epsi \cap\Ephi \cap \EZ.
\]
It remains to argue that the probability of $\E$ is a constant.
\begin{lemma}
\label{lem:ConstantProb}
The event $\E$ holds with probability at least $1/4\left(1-h-\ell-2\exp(-x) \right)$. 
Thus, picking $\ell_l = \frac{1}{5}\frac{\sqrt{\pi}}{\sqrt{2}d}$, $L_u = 4\sqrt{d}$ and $C = \frac{\pi}{10d^2}$, $\Prob{\E}\ge 1/20$.
\end{lemma}
\begin{remark}
Note that all the constants in Lemma \ref{lem:ConstantProb} are polynomial in $d$(or $d^{-1}$ for the lower bound), thus the result of Theorem \ref{thm:Modefficiency} is polynomial in $d$ with at probability at $1/20$.
\end{remark}
 
\section{Experimental Results}
\label{sec:ExpRes}
\subsection{Comparison of $\gamma_A$ and $\gamma_R$}
\label{subsec:comparisonGamma}
Even though theoretical result for $\gamma_A$ is not available, we compare its behavior and that of $\gamma_R$ empirically. 
In this section, we test the expected values of $1/\gamma_A$ and $1/\gamma_R$ for general $A_1$ and high coherent matrix $A_2$. 
In particular, we generate $A_1$ by standard normal distribution, and $A_2 = v_b*\emph{1}' + 0.3*P$ where $\emph{1}$ is the column vector with all entries being 1, and both $v_b$ and the matrix $P$ are generated from standard normal distribution (with different dimensions). 
Another orthonormal matrix R is generated by computing the left column space of a non-singular matrix $N$ that is also generated from  standard normal distribution.  

\subsection{Simulations}
We investigate the performances of different ICA algorithms in a simulation setting. In particular, 3 algorithms are tested: 
\begin{itemize}
\item The HKICA algorithm \citep{hsu2013learning};
\item Our refined HKICA algorithm (DICA);
\item The default FastICA algorithm in the 'ITE' toolbox \cite{szabo12separation} (FICA). 
\end{itemize}
We do not test the algorithm of \citep{anandkumar2012tensordecomposition} because the tensor decomposition takes too long to converge and achieve a valid solution. 
\subsubsection{Data generation}
In the simulation, a common mixing matrix $A$ is generated in a same way as that for $A_1$ or $A_2$ in Subsection \ref{subsec:comparisonGamma}. 
Then we sample $k$ groups of observations, each having size $n$. 
For each observation $y$, $y = Ax+ c\times\eps$ where the signal $x$ is generated from either a product measure of $d$ uniform distributions $\text{Unif}(-\frac12, \frac12)$ or a BPSK signal with different frequency for each component, and $\eps$ is generated from a standard $d$-dimensional Gaussian distribution. 
When $x$ is a BPSK signal, in order to have the components of $x$ are close to independent, we need the ratio of their frequencies are irrational.
All the algorithms will be evaluated on these common $k$ group of observations.
We test the noise ratio $c$ from 0(noiseless) to 1(heavily noisy). 
\subsubsection{Error measures}
We measure the performances of the algorithms on three different measures for different purposes.

The first measure is the parameter recover error. in particular, we evaluate the following quantity between the true mixing matrix $A$ and the one returned by the algorithms $\widehat{A}$:
\begin{equation}
\label{equ:parerror}
\min_{\Pi,S} \|\widehat{A}\Pi S - A\|_{\text{Frob}},
\end{equation}
where $\Pi$ is a permutation matrix, and $S$ is a column scaling matrix (diagonal).

The second measure we used is an approximation of Equation \eqref{equ:parerror} proposed by \citet{comon1994independent}. 
Note that to evaluate Equation \eqref{equ:parerror} one has to enumerate all the permutation $\Pi$, which is not affordable in practice for a large dimension $d$. 
This error helps avoid this computation problem. 

The last measure is the divergence between the joint distribution of the recovered signals and the product of its marginal ones. 
This measure is common in practice when the true mixing matrix $A$ is unknown. 
Moreover, this measure can be reduced to the sum of the entropies of its marginal distribution, as shown in \cite{Learned-Miller:2003:IUS:945365.964306}. 
In particular for an output $\widehat{A}$, we evaluate the following quantity:
\begin{equation}
\sum_{i = 1}^{d} \text{Entropy}(\widehat{x_i}) + \log |\widehat{A}|,
\end{equation}
where $\widehat{x} = \widehat{A}^{-1}y$, and $|\widehat{A}|$ is the absolute value of the determine of $\widehat{A}$. We also use the entropy estimation function 'HShannon\_kNN\_k\_estimation' in the 'ITE' toolbox \cite{szabo14information}.

%For each algorithm, we also measure its running time. 
\subsubsection{Results}


\section{Conclusion}

\if0
\begin{figure}[h]
\vspace{.3in}
\centerline{\fbox{This figure intentionally left non-blank}}
\vspace{.3in}
\caption{Sample Figure Caption}
\end{figure}


\begin{table}[h]
\caption{Sample Table Title} \label{sample-table}
\begin{center}
\begin{tabular}{ll}
{\bf PART}  &{\bf DESCRIPTION} \\
\hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}
\fi

\subsubsection*{Acknowledgements}


%\newpage 
\bibliography{DICA}
\bibliographystyle{plainnat}

\newpage
\include{appendix}
\end{document}
