\documentclass[english]{article} % For LaTeX2e

\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
%\definecolor{darkgreen}{RGB}{26, 82, 87} % solid linking color
\usepackage[bookmarks=false]{hyperref}
  \hypersetup{
  	pdftex,
    pdffitwindow=true,
    pdfstartview={FitH},
    pdfnewwindow=true,
    colorlinks,
    linktocpage=true,
    linkcolor=Green,
    urlcolor=Green,
    citecolor=Green
}


% uncomment the following line and comment the line after it if you want to turn
% off todos
%\usepackage[disable,backgroundcolor = White,textwidth=\marginparwidth]{todonotes}
\usepackage[backgroundcolor = White,textwidth=\marginparwidth]{todonotes}
\newcommand{\todoc}[2][]{\todo[color=Apricot,#1]{#2}}
\newcommand{\todob}[2][]{\todo[color=Cerulean!20,#1]{#2}}
\newcommand{\todoa}[2][]{\todo[color=Purple!20,#1]{#2}}
\newcommand{\todor}[2][]{\todo[color=Blue!10,#1]{#2}}

%\usepackage{nips12submit_e,times}
%\usepackage{geometry}
\usepackage{comment}
\usepackage{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[authoryear]{natbib}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{setspace}
\newcommand{\cset}[2]{\left\{#1\,:\,#2\right\}}
\DeclareMathOperator{\esssup}{ess\;sup}

\usepackage{svn-multi}
% For a good documentation of svn-multi, see http://www.tug.org/pracjourn/2007-3/scharrer/scharrer.pdf
% Do this for new svn tex files:
% svn propset svn:keywords "Id Author Date Rev URL" *.tex
\svnidlong {$HeadURL$} {$LastChangedDate$} {$LastChangedRevision$} {$LastChangedBy$}
\svnid{$Id$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Setting up the page geometry
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[margin=1.0in]{geometry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Setting up the headers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[ol]{\slshape\leftmark}
\fancyfoot[ol]{Rev: \svnrev\ (\svnfilerev)}
\fancyfoot[or]{\svnyear -\svnmonth -\svnday\ \svnhour:\svnminute} %Date
% If the information should be also placed % on the chapter page use:
\fancypagestyle{plain}{%
	\fancyhead{}
	\fancyhead[ol]{\slshape\leftmark}
	\fancyfoot[ol]{Rev: \svnrev\ (\svnfilerev)} \fancyfoot[or]{\svnyear -\svnmonth -\svnday\ \svnhour:\svnminute} %Date
}

%\usepackage{fullpage}
\setlength{\headheight}{15.0pt}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version


\usepackage{algorithmic}
\usepackage{babel}


\newcommand{\cN}{\cal{N}}
\renewcommand{\P}{{\mathcal P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ex}[1]{\mathbb{E}[#1]}
\newcommand{\Em}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\snorm}[1]{\left\|#1\right\|} % scaling norm
\newcommand{\lmax}[1]{\lambda_{\mathrm{max}}(#1)}
\newcommand{\lmin}[1]{\lambda_{\mathrm{min}}(#1)}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\dprod}[2]{\langle #1,#2 \rangle_{M}}
\newcommand{\hA}{\hat{A}}
\newcommand{\hb}{\hat{b}}
\newcommand{\hC}{\hat{C}}
\newcommand{\hAp}{\hat{A}^\prime}
\newcommand{\hbp}{\hat{b}^\prime}
\newcommand{\hCp}{\hat{C}^\prime}
\renewcommand{\th}{\theta}
\newcommand{\vth}{\hat{\theta}_{V}}
\newcommand{\lth}{\hat{\theta}_{\lambda}}
\newcommand{\hth}{\hat{\theta}}
\newcommand{\ltho}{\hat{\theta}_{\lambda_1}}
\newcommand{\ltht}{\hat{\theta}_{\lambda_2}}
\newcommand{\hlth}{\hat{\theta}_{\hat{\lambda}}}
\newcommand{\slth}{\hat{\theta}_{\lambda^*}}
\newcommand{\plth}{\hat{\theta}_{\lambda_p}}
\newcommand{\hthp}{\hat{\theta}^\prime}
\newcommand{\lthp}{\hat{\theta}^\prime_{\lambdap}}
\newcommand{\ath}{\hat{\theta}_{a}}
\newcommand{\thetap}{\theta^\prime}
\newcommand{\sth}{\theta^*}
\newcommand{\ind}[1]{\mathbb{I}_{\left\{ #1 \right\}}}
\newcommand{\hzeta}{\hat{\zeta}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\cf}{\emph{cf.}}
\newcommand{\T}[1]{T\left( #1 \right)}
\newcommand{\St}[1]{S\left( #1 \right)}
\newcommand{\Ts}[2]{T_{#1}\left( #2 \right)}
\newcommand{\Ss}[2]{S_{#1}\left( #2 \right)}
\newcommand{\Ord}[1]{O\left( #1 \right)}
\newcommand{\etc}{\emph{etc.}}
%\newcommand{\det}{\mathrm{det}}
\newcommand{\bsth}{\bar{\theta}^*}
\newcommand{\sal}{\emph} %Emphasis the J.D. Salinger way :)
\newcommand{\gradL}{\nabla \mathcal{L}}
\newcommand{\lambdap}{{\lambda^\prime}}
\newcommand{\lambdahp}{\hat{\lambda}^\prime}
\newcommand{\pqLoss}[1]{\mathcal{L}_{#1}}
\newcommand{\pLoss}[2]{\pqLoss{#1}( #2 )}
\newcommand{\qLoss}{\pqLoss{M}}
\newcommand{\Loss}[1]{\qLoss(#1)}

\newcommand{\hpqLoss}[1]{\mathcal{\hat{L}}_{#1}}
\newcommand{\hpLoss}[2]{\hpqLoss{#1}( #2 )}
\newcommand{\hqLoss}{\hpqLoss{M}}
\newcommand{\hLoss}[1]{\hqLoss(#1)}
\newcommand{\bX}{\bar{X}}
\newcommand{\bY}{\bar{Y}}


\newcommand{\rth}{\hat{\theta}}
\newcommand{\tth}{\tilde{\theta}}
\newcommand{\maxeig}{\nu_{\max}}
\newcommand{\mineig}{\nu_{\min}}

\newcommand{\ra}{\rightarrow}
\newcommand{\real}{\mathbb{R}}

\newcommand{\one}[1]{\mathbf{1}_{\{#1\}}}

\newcommand{\rl}[1]{\mathbb{R}^{#1}}
\newcommand{\Mset}{\mathcal{M}(\varepsilon)}
\newcommand{\Cset}{\mathcal{C}}
\newcommand{\truel}{L}
\newcommand{\zol}{L_{\mathrm{0-1}}}
\newcommand{\hinl}{L_{\mathrm{hinge}}}
\newcommand{\phil}{L_{\varphi}}
\newcommand{\lspace}{\left\{1, \dots,K \right\}}
\newcommand{\risk}{\mathcal{R}}
\newcommand{\scoref}{\in \mathcal{H}}
\newcommand{\subscoref}{\in \mathcal{H}^\prime}
\newcommand{\hQ}{\hat{Q}}
\DeclareMathOperator{\supp}{supp}
\renewcommand{\natural}{\mathbb{N}}


\newcounter{assumption}%[section]
\newcommand{\theassumptionletter}{A}
\renewcommand{\theassumption}{\theassumptionletter\arabic{assumption}}

\newenvironment{ass}[1][]{\begin{trivlist}\item[] \refstepcounter{assumption}%
 {\bf Assumption\ \theassumption\ #1} \it}{%\par\nobreak\noindent\sl\ignorespaces}{%
 \ifvmode\smallskip\fi\end{trivlist}}
\newcommand{\aref}[1]{(\ref{#1})}
\newenvironment{ass*}[1][]{\begin{trivlist}\item[] %
 {\bf Assumption\  #1} }{%\par\nobreak\noindent\sl\ignorespaces}{%
 \ifvmode\smallskip\fi\end{trivlist}}


\usepackage[capitalize]{cleveref}

% theorems/definitions
\newtheorem{lemma}{Lemma}[section]
\newtheorem{thm}[lemma]{Theorem}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{cor}[lemma]{Corollary}
\newtheorem{example}[lemma]{Example}
\newtheorem{prop}[lemma]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{remark}[lemma]{Remark}
%\newtheorem{exercise}{Exercise}[chapter]
\newtheorem*{solution}{Solution}
%\newtheorem{assumption}{Assumption}[section]
\newtheorem{note}[lemma]{Note}
\newtheorem{problem}[lemma]{Problem}

%\crefname{assumption}{assumption}{assumptions}
%\Crefname{assumption}{Assumption}{Assumptions}
%\crefname{Exercise}{Exercise}{Exercises}
%\Crefname{Exercise}{Exercise}{Exercises}


\newcommand{\FF}{\mathcal{F}}
\newcommand{\TT}{\mathcal{T}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\KK}{\mathcal{K}}
\newcommand{\eps}{\varepsilon}



\title{Deterministic ICA}
\author{
}
\date{
Version: \svnrev\\
Date: \svndate\\
Last change: \svnauthor
%\today\ @ \currenttime \ Version 0.5 (r5322)
}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\section{Motivation}
\label{sec:motivation}
% Handbook of Blind Source Separation, Independent Component Analysis and Applications
% Chapter 1, Contextual difficulties 1.1.2.1 Independence; sweeps the problem under the rug:
% At first glance, this assumption seems very strong. In fact, we may think it rarely occurs, and it is more difficult to satisfy than second order independence i.e. non- correlation. In the above biological problem, it was indeed tricky to explain statistical independence between p(t) and v(t). In fact, one can remark that “speed v(t) is related to location p(t) through v(t) = dp(t)/dt. The two variables then cannot be independent”. However, this functional dependence is not a statistical dependence. The knowledge of p(t) at a given instant t does not provide any information on v(t), and vice versa. In other words, while the random vectors of speed and position, [v(t1),v(t2),...,v(tk)]T	and [p(t1), p(t2),..., p(tk)]T	are generally not independent, the (marginal) random variables, v(t) and p(t), at any given time t are independent. Therefore, for instantaneous ICA algorithms, the dependence is irrelevant.
ICA (independent component analysis) algorithms are known to work well for unmixing the mixtures of various deterministic signals.
In fact, is is quite common that the papers start by showing pictures of periodic functions (perfectly deterministic and predictable) and then demonstrating that the functions can be recovered from their mixtures \citep[e.g.,][]{HyvOja00}.
The ICA identifiability question is what signals (functions) can be recovered from their mixtures and is the subject of this paper. \todoc{A nicer intro would show some examples when they can and when they cannot be identified. I suggest adding some examples, some figures.}

By definition, any two deterministic signals are independent of each other. Can then ICA algorithms unmix any mixtures of deterministic signals (with the usual restrictions that the mixing matrix is nonsingular and no components are repeated)? 
Clearly, this is not possible (even when the mixture is non-singular).
That ICA algorithms work well on some deterministic signals is a curious phenomenon which indicates that the usual probabilistic notion of independence is unsatisfactory if one wishes to have a deeper understanding of how and why ICA algorithms work.

Formally, the ICA identifiability question is as follows:
Let $f:\natural \ra \real^d$ be a $d$-dimensional deterministic ``signal''. 
We will denote by $f_i$ the $i$th component of $f$ and we will call $f$ the source.
Let $A = (a_{ij})$ be a $d\times d$ non-singular matrix and let $g:\natural \ra \real^d$ be defined by
\[
g(t) = A f(t), \quad t\in \natural.
\]
We will call $g$ the observed signal and by slightly abusing notation we will also write $g = A f$.%
\footnote{In fact, if $f$, $g$ are viewed as a $d\times \infty$ ``matrices'', then there is no abuse.}
In the ICA problem one observes the values of $g$ in a sequential manner and the goal is to recover the components of $f$ up to scaling and permutation. For $f,f':\natural \ra \real^d$, we shall call $f$ and $f'$ equivalent (which we denote by $f\sim f'$) if the components of $f$ and $f'$ agree up to scaling and permutation.
Then, we say that the source $f$ is identifiable if there is a mapping $\FF$ between the space of $d$-dimensional signals such that  $\FF( Af )\sim f$ for any nonsingular $A\in \real^{d\times d}$.
In the ICA identifiability question one asks for a ``large'' set of multidimensional signals that are identifiable. Ideally, one would like to ask for the largest set, but it is unclear whether such a set exists. \todoc{Does it?}

We are not the first to notice that ICA algorithms (sometimes) work for deterministic signals.
In particular, in their paper Pando G. Georgiev and Fabian J. Theis note the following:
\begin{quote}
 ``Our objective is to estimate the source signals sequentially one-by-one or simultaneously assuming that they are statistically independent.
 %
The uniqueness of such estimation (up to permutation and scaling), or identifiability of the linear ICA model, is justified in the literature by the Skitovitch-Darmois theorem [41,17].
 %
 Whereas this theorem is probabilistic in nature, an elementary lemma from optimization theory (although with a non-elementary proof) can serve the same purpose -- rigorous justification of the identifiability of ICA model, when maximization of the cumulants is used.\footnote{
Optimization Techniques for Data Representations with Biomedical Applications, in
P.M. Pardalos, H.E. Romeijn (eds.), Handbook of Optimization in Medicine,	253 Springer Optimization and Its Applications 26,  Springer Science+Business Media LLC 2009
pp. 253--290.}
\end{quote}

Our purpose is to extend this observation to other ICA models that do not rely on the maximization of cumulants, but rely on the concepts of independence (and thus, on the Darmois-Skitovitch theorem).

\todoc[inline]{This is a rather naive introduction. Browing the above-mentioned paper, it seems that at least Theis and Georgiev are well aware that identifiability is an algebraic question. A more thorough overview of the literature will be needed.
Note also that ICA is a special case of the ``Blind-Source Separation'' problem and these guys talk about at least two other interesting cases: Nonnegative matrix factorization and sparse component analysis.
Furthermore, the above ICA problem is the simplest of its kind. More interesting (and difficult) cases are when the number of observed components is smaller than the number of sources (underdetermined problem) or when there is noise added to the observations.
}

\section{Independence of Components}
\label{sec:IndeofComp}

The purpose of this section is to define a natural notion of independence for functions.
Let $f_1,f_2: \natural \ra \real$ be real-valued functions from the set of natural numbers.
Then, a natural definition for $f_1$ and $f_2$ being independent is that if 
$\nu_{t,i}$ is the empirical distribution of the values $(f_i(0),\ldots,f_i(t-1))$ over the reals
and $\nu_t$ is the empirical distribution of the values of the pairs $( (f_1(s),f_2(s))_{0\le s \le t-1})$
then $\nu_t \approx \nu_{t,1} \otimes \nu_{t,2}$ and we expect equality to hold as $t\ra\infty$.
We also see that it is natural to expect $\nu_t$, and also $\nu_{t,i}$ ($i=1,2$) to converge as $t\ra\infty$.

Since the choice to consider functions over the natural numbers is somewhat arbitrary, we will generalize these notions to arbitrary measurable domains $(\Omega,\AA)$ and a sequence of measures $(\mu_t)_{t=0,1,\ldots}$ over it.
If $\Omega = \natural$, $\AA = 2^\natural$ and $\mu_t$ is the uniform probability distribution on $\{0,\ldots,t\}$.
In what follows we fix $\Omega$ and $(\mu_t)$ and define all the concepts that follow relative to these.

Let $f: \Omega \ra \real^d$.
For $t\ge 0$, we define $\nu_t$ to be the probability measure over $\real^d$ that is induced by $f$ and $\mu_t$:
In particular, for any Borel set $A\subset \real^d$,
\[
\nu_t(A)=\int \ind{f(x) \in A}\, d\mu_t(x) = \mu_t\Bigl( \cset{x}{f(x)\in A} \Bigr)
= \mu_t ( f^{-1}(A) ).
\]
In short, we will call $\nu_t$ the $t$th probability measure induced by $f$.
Remember that a sequence of probability measures $(\nu_t)$ is said to converge weakly to a probability measure $\nu$ if $\int h d\nu_t$ converges to $\int h d\nu$ for any bounded, continuous function $h$.

\begin{definition}
A function $f:\Omega \rightarrow \real^d$ is called \emph{ergodic} w.r.t. $(\mu_t)$
if the sequence of  induced probability measures $(\nu_t)_{t\ge 0}$ is weakly convergent.
\end{definition}
In what follows, when $(\mu_t)$ is fixed, we will simply call $f$  ergodic, not mentioning $(\mu_t)$.
When the limit of $(\nu_t)$ exists, it will be denoted by $\nu$
and if we want to emphasize the dependence on $f$ then we will use $\nu^{(f)}$.
Similarly, when the dependence on $\nu_t$ on $f$ is important, we will use $\nu^{(f)}_t$.
A simple example of an ergodic function is a function over the reals (positive reals, natural numbers, etc.) that is periodic. 
\todoc[inline]{Somewhere we should make the remark that this framework is a generalization of the standard stochastic framework where you can take $\mu_t = \mu$.
And in fact, in some sense what we try to overcome here is that over the real line there is no uniform probability measure; in some sense we are building the probability theory of improper measures..?
}
\todor[inline]{I guess there are typoes in this paragraph? All the $\mu$, except the first two, are $\nu$?}
\todoc[inline]{Good catch. Good now? If yes, just remove the comments.}

The next question to be investigated is whether the space of ergodic functions is a subspace, i.e., whether it is closed under addition and multiplication by reals. The latter is clear. However, concerning closeness under addition, we have 
the following observation:
\begin{claim}
The space of ergodic functions is not closed under addition.
\end{claim}
\begin{proof}
Let $f,g:\natural \rightarrow \{-1,+1\}$ and $(\mu_t)$ be defined as follows: 
\[
f(t) = \begin{cases} +1, & t \equiv 0 \pmod 4; \\
					-1, & t \equiv 1 \pmod 4; \\
					+1,  & t \equiv 2  \pmod 4; \\
					-1, & t \equiv 3  \pmod 4,
					\end{cases}
\qquad\qquad 
g(t) = \begin{cases} +1, & t\equiv 0 \pmod 4; \\
					-1, & t\equiv 1 \pmod 4; \\
					-1,  & t\equiv 2 \pmod 4; \\
					+1, & t\equiv 3 \pmod 4.
					\end{cases}
\]
\[
\quad \mu_{2t+1}(x) = \begin{cases} 1/2, & x=0; \\
					1/2, & x=1; \\
					0,  & \text{otherwise},
					\end{cases}
\qquad\qquad 
\mu_{2t}(x) = \begin{cases} 1/2, & x=2; \\
					1/2, & x=3; \\
					0,  & \text{otherwise} .
					\end{cases}
\]
Then, $\nu^{(f)}_t$, $\nu^{(g)}_t$ are both the uniform  distribution on $\{-1,+1\}$, hence they converge and the limit is the same uniform  distribution. 
However, while $\nu^{(f+g)}_{2t+1}$ is the uniform  distribution on $\{-2,+2\}$,  $\nu^{(f+g)}_{2t}$ is the degenerate
distribution that puts all the mass at $0$. Thus, $\nu^{(f+g)}_{t}$ fails to be convergent. 
\end{proof}
Notice that the above example shows that $\nu_t^{(f)}$  and $\nu_t^{(g)}$ alone do not determine $\nu_t^{(f+g)}$ (which is not that surprising).
\todoc[inline]{What happens when $\Omega = \natural$ and $\mu_t$ is uniform on $\{0,1,\ldots, t\}$?
}
While the space of ergodic functions is not closed under addition, the space spanned by the components of an ergodic function is a space of ergodic functions, as in the following proposition. 
Such observation lead to our definition of independence of components, and build the foundation of the rest work of our paper.
\begin{prop}\label{prop:comp}
$f = (f_1,\ldots, f_d)^{\top}$ is a $d$-dimensional ergodic function, if and only if 
 for any $m\times d$ matrix $A$, $g = A f$ is an $m$-dimensional ergodic function.
\end{prop}
For the proof, we will need the Cram\'er-Wold theorem.
\newcommand{\BB}{\mathcal{B}}
%\begin{thm}[\citep{cramer1936some}]
%Let $F_1$ and $F_2$ be probability distributions over the Borel-sets $\BB_d$ of $\real^d$,
% and $t\in \real^d\setminus\{0\}$ be some nonzero $d$-dimensional vector.
%Then, $F_1=F_2$ if and only if $F_1(S_{t,z}) = F_2(S_{t,z})$ holds for all $z\in \real$,
%	where, for a given $z\in \real$, $S_{t,z} = \cset{ x\in \real^d }{t^\top x \le z }$. 
%\end{thm}
Fix a Borel probability distribution $F$ over $\real^d$, $t\in \real^d \setminus \{0\}$. Then,
the Borel probability distribution $F^{(t)}: \BB_1 \ra [0,1]$ defined using $F^{(t)}(S) = F( \cset{x\in \real^d}{t^\top x\in S} )$ is called
a one-dimensional projection of $F$ (with the projection specified using $t$). 
(We denote by $\BB_d$ the $\sigma$-algebra of Borel-sets of $\real^d$.)
%The previous result states that a $d$-dimensional Borel distributions is uniquely determined by its one dimensional projections.
%From this, the following result, which will be most useful to us, follows immediately:
\begin{thm}[\citealt{cramer1936some}]
\label{cor:cw}
Let $F$, $F_1,F_2,\ldots$ be Borel probability distributions over $\real^d$.
Then $F_t$ weakly converges to $F$ if and only if all the one-dimensional projections of $F_t$ weakly converge to the corresponding one-dimensional projection of $F$.
\todoc{I could not quite find this result in the cited paper, but it will surely follow from it. But maybe a better citation would be useful, e.g., to some probability book.}
\end{thm}
This result is stated and proved, for example, in the book of \citet{AtLa06}
as  Theorem 10.4.5.
\begin{proof}[Proof of \cref{prop:comp}] %\todoc{I think given the above corollary, this proof should be simplified.}
The converse direction is straightforward by picking $A$ to be the $d\times d$ identity matrix.

Now assume that $f= (f_1,\ldots, f_d)^{\top}$ is ergodic, thus $\nu_t^{(f)}: \mathcal{B}_d \rightarrow [0,1]$ weakly converges to $\nu^{(f)}$. Let $A\in \real^{m\times d}$ and $g = A f$.
 We need to show that $\nu_t^{(g)}$ weakly converges to some distribution $\nu^{(g)}$.
In particular, we also claim that $\nu^{(g)}(S) = \nu^{(f)}( \cset{x\in \real^d}{Ax \in S} )$, $S\in \BB_m$.
By \cref{cor:cw}, for this it suffices to show that the one-dimensional projections of $\nu_t^{(g)}$ weakly converge to the one-dimensional projection of $\nu^{(g)}$.
%Without loss of generality, we may assume that $A$ is non-singular. \todoc{Explain why..}
Clearly, any one-dimensional projection of $\nu_t^{(g)}$, defined by say $t\in \real^m$,
is a one-dimensional projection of $\nu_t^{(f)}$ defined by $A^\top t$ and the same also holds for $\nu^{(g)}$ and $\nu^{(f)}$.
By \cref{cor:cw}, the one-dimensional projections of $\nu_t^{(f)}$ weakly converge to the corresponding one-dimensional projections of $\nu^{(f)}$, hence the same holds
for the pair $(\nu_t^{(g)}, \nu^{(g)})$, finishing the proof.
\if0
Denote by $\bX_t$ a $d$-dimensional random vector with distribution $\nu_t^{(f)}$ and 
	let $\bX$ be a $d$-dimensional  random vector with distribution $\nu^{\top}$. 
Let $\bY_t =  A\bX_t$ and $ \bY =  A\bX$. 
By definition, $\nu_t^{(g)}$ is the distribution of $A\bX_t$,
while $\nu^{(g)}$ is the distribution of $A\bX$. 

To prove that $\bY_t$ weakly converges to $\bY$, by the Cram\'{e}r-Wold Theorem we only need to prove that 
for any $m$-dimension vector $\alpha = (\alpha_1,\ldots,\alpha_m)$, 
$\alpha\bY_t$ weakly converges to $\alpha\bY$. \todoc{Why does the convergence of $\bY_t$ to $\bY$ follows from this result?}
Note that again by the Cram\'{e}r-Wold Theorem 
\[
\alpha\bY_t = (\alpha A) \bX_t \xrightarrow{\mathcal{L}}  (\alpha A) \bX = \alpha \bY\,,
\]
finishing the proof.
\fi
\end{proof}

We define the independence of the components of a multidimensional ergodic function $f$ based on its induced limiting distribution $\nu^{(f)}$:
\begin{definition}
Given an ergodic function $f = (f_1,\ldots,f_d)^{\top}$, 
we say that $\{f_1,\ldots,f_d\}$ are \emph{independent} components, 
	if $\nu^{(f_1)}\otimes\ldots\otimes\nu^{(f_d)} = v^{(f)}$, 
	where $f = (f_1,\ldots,f_d):\Omega \rightarrow \real^d$.
	\if0
	We say $\{f_1,\ldots,f_d\}$ are pairwise independent, if $\{f_i,f_j\}$ are independent components of the function $\tilde{f}_{ij} = (f_i,f_j)^{\top}$ for any $1\le i\neq j \le d$. 
	\fi
\end{definition}

\begin{remark}
Note that the existence of $\nu^{(f_i)}$ 
%and $\nu^{(\tilde{f}_{ij})}$ are
is guaranteed by \cref{prop:comp}. 
The definition of independent components is also extended to the independence of ergodic functions, by requiring that the joint function of all the functions is ergodic with independent components. \todoc{This is a bit unclear.
}
\end{remark}
Do there exist ergodic functions that have independent (or dependent) components? 
Here, we provide a few examples based on periodic functions.
Let $m$ denote the Lebesgue measure over the reals.
\begin{prop}
Assume that $f = (f_1,f_2)^{\top}:\real\rightarrow\real^2$ is an ergodic function, and both $f_1$ and $f_2$ are periodic with respective periods $T_1$ and $T_2$. 
Let $\mu_t$ be the uniform probability distribution on $[0,t]$. 
%$\mu_t((a,b)) = (b-a)/t$ for $0\le a\le b\le t$.
 If $T_1/T_2$ is irrational, then $f_1$ and $f_2$ are independent. 
\end{prop}
%\vspace{-0.5cm}
\begin{proof}
 For two Borel sets $A\subset \real$ and $B\subset \real$, let $S_1 =f_1^{-1}(A)\cap [0,T_1] $ and $ S_2 = f_2^{-1}(B)\cap [0,T_2]$. It is easy to verify that $\nu^{(f_1)}(A) = \frac{m(S_1)}{T_1}$, and $\nu^{(f_2)}(B) = \frac{m(S_2)}{T_2}$. 
 Note that $\nu^{(f_1)}_t(A\times B) = \frac{m( f_1^{-1}(A)\cap f_2^{-1}(B)\cap [0,t] )}{t}$. Now assume that $nT_1 \le t< (n+1)T_1$ and $mT_2\le t <(m+1)T_2$. Thus,
 \begin{align*}
 \nu^{(f_1)}_t(A\times B) =  \frac{1}{t} \int_{ \substack{x\in f_1^{-1}(A) \\ 0\le x \le t}} \ind{x\in f_2^{-1}(B)}\, dx 
 					=	 \frac{1}{t} \sum_{u=0}^{n-1} \int_{S_1} \ind{uT_1+s \in f_2^{-1}(B)}\, ds + \frac{C}{t}
 \end{align*}
 for some constant $0\le C<T_1$. Letting $t\ra\infty$, we get
 \begin{align*}
  \lim_{t\rightarrow \infty} \nu^{(f_1)}_t(A\times B) 
  &=  \int_{S_1} \lim_{n\rightarrow \infty} \sum_{u=0}^{n-1} \frac{\ind{uT_1+s \in f_2^{-1}(B)}}{n T_1} \, ds,
 \end{align*}
 where the interchange of the limit and the integration is justified by Lebesgue's dominated theorem, once we show that the integrands converge.
To see this, fix $s\in \real$. Then,
 $\lim_{n\rightarrow \infty}\frac1n \sum_{u=0}^{n-1} \ind{uT_1+s \in f_2^{-1}(B)}
 =\lim_{n\rightarrow \infty}\frac1n | \cset{ 0\le u \le n-1 }{ f_2(uT_1+s) \in (B)}
 = \nu^{(f_2)}(B)$ since, as it is well known, thanks to $T_1/T_2$ being irrational, 
 $(u T_1+s \pmod T_2 )_{u\in \natural}$ is ``uniformly distributed'' in $[0,T_2)$.
 \todoc{We can cite P\'olya-Szeg\"o}
Thus, the interchange is justified and
 \begin{align*}
 \lim\limits_{n\rightarrow \infty} \nu^{(f_1)}_t(A\times B) = & \int_{x\in S_1} \frac{\nu^{(f_2)}(B)}{T_1}\, dc 
 														=  \frac{m(S_1)\nu^{(f_2)}(B)}{T_1} 
 														=  \nu^{(f_1)}(A)\nu^{(f_2)}(B),
 \end{align*}
which shows that  $f$ and $g$ are independent.
\end{proof}

When the ratio of the periods of two periodic functions is rational, the two functions may or may not be independent of each other:
\begin{prop}
Assume that $f = (f_1,f_2)^{\top}:\real\rightarrow\real^2$ is an ergodic function, and both $f_1$ and $f_2$ are periodic with respective periods $T_1$ and $T_2$. 
Let $\mu_t$ be the uniform probability distribution on $[0,t]$.   
If $T_1/T_2 = \frac{u}{v}$ is rational, then $f$ and $g$ are independent iff 
\[
\frac{m(\{f^{-1}(A)\cap g^{-1}(B) \cap [0,vT_1]\})}{v} = \frac{m(f^{-1}(A)\cap [0,T_1]) \,m(g^{-1}(B)\cap [0,T_2])}{T_2}
\]
for any Borel sets $A,B\subset \real$. \todoc{Can this condition be satisfied? Example?}
\end{prop}
The proof of the above proposition simply follows the definition of independent component. An example of the above proposition is:
$f=(f_1,f_2)^{\top}:\natural \rightarrow \{-1,+1\}^2$: 
\[
f(t) = \begin{cases} +1, & t\in(n,n+1/2]; \\
					 -1, & t\in(n+1/2,n+1]; 
					\end{cases}
\qquad\qquad 
g(t) = \begin{cases} +1, & t\in(n,n+1/4]; \\
					 -1, & t\in(n+1/4,n+3/4];\\
				  	 +1, & t\in(n+3/4,n+1];
					\end{cases}
\]
for any $n\in\natural$.
\todoc{Proof?}

When the domain of the function is discrete (e.g., the set of natural numbers),
	the condition in the previous result can be simplified. This simplified result clearly shows that the condition of the result
	is non-vacuous.
\begin{prop}
Assume that $f = (f_1,f_2)^{\top}:\natural\rightarrow\real^2$ is an ergodic function, and both $f_1$ and $f_2$ are periodic with respective periods $T_1$ and $T_2$. 
Let $\mu_t$ be the uniform probability distribution on $\{0,1,\ldots,t-1\}$.   
Then $f$ and $g$ are independent iff
\begin{align*}
|\cset{x\in\natural}{ f(x) = a, g(x) = b, 1\le x \le T_1T_2 }| = & \, |\cset{x\in\natural}{ f(x) = a, 1\le x \le T_1 }| \\
& \quad \times |\cset{x\in\natural}{ g(x) = b, 1\le x \le T_2 } |
\end{align*}
for any $a,b\in \real$. \todoc{Maybe we should use $0\le x \le T-1$ in the above condition everywhere?}
\end{prop}

In fact, the induced distribution of a sum of two independent components is the convolution of the induced distributions of the two component.
To state the result, let us remind the reader of the definition of the convolution $\nu*\mu$ of two distributions, 
 $\nu,\mu:\BB_d \ra [0,1]$:
For $S\in \BB_d$, 
 $(\nu* \mu)(S) = \int \nu( S-x) d\mu(x)$, where $S - x$ is the translation of $S\in \BB_d$ by $-x$, i.e., $S-x =\cset{y-x}{y\in S}$.

\if0
While the space of ergodic functions is not closed under addition, the sum of two independent ergodic functions is also ergodic and in fact its induced distribution is the convolution of the induced distributions of the two functions:
\fi
\begin{prop}
\label{prop:convolution}
If $(f,g)$ is ergodic with independent components then
$f+g$ is ergodic and
 $\nu^{(f+g)} = \nu^{(f)} * \nu^{(g)}$,
 where for $\nu,\mu: \BB_d \ra [0,1]$.
\end{prop}
\begin{proof}
The ergodicity of $f+g$ immediately follows from \cref{prop:comp}, i.e., $\nu^{(f+g)} = \lim\limits_{t\ra \infty} \nu_t^{(f+g)}$ exists.

Denote by $(X_{t,1},X_{t,2})^{\top}$ (and $(X_1,X_2)^{\top}$) the random vector with distribution $\nu_t^{((f,g)^{\top})}$ (and $\nu^{((f,g)^{\top})}$ respectively). 
Note that for any $t\ge0 $, $\nu_t^{(f+g)}$ is the probability distribution of $X_{t,1}+X_{t,2}$.
Hence,  $\nu_t^{(f+g)} = \nu_t^{(f)}* \nu_t^{(g)}$. \todoc{Hmm, this only follows if $(X_{t,1},X_{t,2})$ are independent of each other, no? I don't think this proof will go through!}
Thus, by \cref{cor:cw},
\[
\nu^{(f+g)} = \lim_{t\ra \infty} \nu_t^{(f+g)} = \lim_{t\ra \infty} \nu_t^{(f)}* \nu_t^{(g)} = \nu^{(f)} * \nu^{(g)}\,.
\]  
\todoc[inline]{Here, we use that the limit commutes with convolution. Why does this hold? Does this always hold?}
\if0
For simplicity, the statement is shown for the case when $f,g$ take on values in $\natural$. The proof of the general statement has a similar structure and is left out for brevity. \todoc{Hmm, should we do the full thing? Someone!?}
Fix $x\in \natural$ and let $A_x = \cset{\omega\in \Omega}{ f(\omega)+g(\omega)=x}$
We have
\begin{align*}
A_x
&=
\cup_{y\in \natural} \cset{\omega\in \Omega}{ f(\omega)=y, g(\omega)=x-y}\\
& =
\cup_{y\in \natural} \underbrace{\cset{\omega\in \Omega}{ (f,g)(\omega)=(y,x-y) }}_{A_{(y,x-y)}}\,.
\end{align*}
Note that $A_{(y,x-y)} \cap A_{(y',x-y')} = \emptyset$ when $y\ne y'$.
Hence,
$\mu_t( A_x ) = \sum_{y\in \natural} \mu_t(A_{(y,x-y)})$.

Hence, $\lim_{t\ra\infty} \mu_t( A_{(y,x-y)}) = \nu^{(f,g)}( \{ (y,x-y) \} )$, which shows that $f+g$ is also ergodic.
Furthermore, using the independence of $f$ and $g$, we have
$ \nu^{(f,g)}(  \{ (y,x-y) \}  )
= \nu^{(f)}( \{ y \} ) \nu^{(g)}( \{ x-y \} )$, and hence
$\nu^{(f+g)}(\{x\}) = \lim_{t\ra\infty} \mu_t( A_x ) = \sum_{y\in\natural} \nu^{(f)}( \{ y \} ) \nu^{(g)}( \{ x-y \} )$, and hence indeed
$\nu^{(f+g)} = \nu^{(f)} * \nu^{(g)}$.
\fi
\end{proof}
Of course, the result also extends to any number of independent functions.
\if0
Consider the sequence of empirical measures over $\real^d$ introduced by $f:\natural \rightarrow \real^d$: for any $t\ge 1$, let $\nu_t$ denote the a probability measure over $\real^d$ such that
for $ $ any Borel set $A \in \real^d$, $\nu_t(A)=\tfrac{1}{t} \sum_{k=1}^t \ind{f(k) \in A}$. 
\fi

\subsection{Will the limiting distribution be a probability distribution?}
In this section, we provide a sufficient condition under which all the cluster points of the induced distributions $(\nu_t)$ will be probability distributions. In particular, we have the following result:
\begin{prop}
\label{lem:ergodicfunction}
Let $f_i$ denote the $i$th coordinate function of $f$.
Assume that
\begin{equation}
\label{eq:ergodicproperty}
\lim_{t\to\infty} \int |f_i(x)|\, d\mu_t(x) 
\end{equation}
exists and is finite for all $1 \le i \le d$ and that $\esssup_{\mu_t} |f_i|<\infty$ for any $1\le i \le d$ and $t\ge 0$.
Then all limit points of the sequence $\{\nu_t\}$, with respect to the weak topology, are probability measures over $\real^d$.
\end{prop}
As we we will soon see, the statement of the lemma will follow from Prokhorov theorem.
For this, we need to introduce the concept of a ``tight'' set of probability measures.
Let $(X,\TT)$ be a topological space, $\AA$ be a $\sigma$-algebra over $X$ that contains the topology $\TT$,
$\KK$ a set of probability measures over $\AA$. Then $\KK$ is called \emph{tight} if for any $\eps>0$ there exists a compact set $K$ of $X$ such that for all measures $\mu \in \KK$, $\mu(K)\ge 1-\eps$. The intuitive idea behind tightness is that the measures ``do not escape to infinity''.
\begin{thm}[Prokhorov's Theorem]
Let $(S,\rho)$ be a separable metric space, $\KK$ be a set of probability measures over the Borel $\sigma$-algebra $\AA$ of $S$.
Then $\KK$ is tight iff the closure of $\KK$ is sequentially compact in the space of probability measures over $\AA$ equipped with the topology of weak convergence.
\end{thm}
An immediate corollary of this theorem is that if $\KK$ is tight than any cluster point of $\KK$ is also a probability measure.
With this, let us continue with the proof of \cref{lem:ergodicfunction}.
\begin{proof}
If we show that the set of probability measures $\{\nu_t\}$ is tight then, as suggested beforehand,
the statement will indeed follow from Prokhorov's theorem.
Denote by $m_i\in \mathbb{R}$ the limit of $ m_{t,i} = \int |f_i(x)|\, d\mu_t(x)$.
Pick any $\eps>0$.
By the convergence of $m_{t,i}$, there exists some $T>0$ such that
\begin{equation}
\label{eq:converge}
\left| \int |f_i(x)|\, d\mu_t (x)- m_i \right| <\eps
\end{equation}
for all $1 \le i \le d$ and $t>T$. Let $u_{i}=\tfrac{m_i+\eps}{\eps}$. Then the marginals
$\nu_{t,i}$ of $\nu_t$, $1\le i \le d$, satisfy $\nu_{t,i}([-u_{i},u_{i}]) > 1-\eps$ for all $1\le i\le d$ and $t>T$, otherwise \eqref{eq:converge} would be violated. 
Indeed, $\nu_{t,i}([-u_i,u_i]^c)=\int_{|f_i(x)|>u_i} 1\cdot d\mu_t(x)
\le \frac{1}{u_i}\int_{|f_i(x)|>u_i} |f_i(x)| d\mu_t(x)
\le \frac{1}{u_i} \int |f_i(x)| d\mu_t(x) \le \frac{m_i+\eps}{u_i}\le \eps$.

Let $F_{t,i} = \esssup_{\mu_t} |f_i|$ be the essential supremum of $|f_i|$ w.r.t. $\mu_t$.
Define 
$K=\prod_{i=1}^d [-u'_{i},u'_{i}]$ where $u'_{i}=\max\{u_{i},F_{1,i}, \ldots,F_{T,i}$.
We claim that 
$\nu_t(K)>1-d \eps$ for all $t \ge 1$, which would show that $\{\nu_t\}$ is tight.
To lower bound the $\nu_t$-measure of $K$, we upper bound the measure of its complement:
$\nu_t(K^c) \le \nu_t( \cup_{1\le i \le d} \real^{i-1} \times [-u_i',u_i']^c \times \real^{d-i})
\le \sum_{i=1}^d \nu_t( \real^{i-1} \times [-u_i',u_i']^c \times \real^{d-i})
=   \sum_{i=1}^d \nu_{t,i}( [-u_i',u_i']^c )
\le d \eps$, where the last inequality follows from $u_i'\ge u_i$ and our choice of $u_i$ when $t>T$,
while it follows from $u_i'\ge \esssup_{\mu_t} |f_i|$ when $t\ge T$.
\end{proof}


\if0
\begin{lemma}
\label{lem:ergodicfunction}
Let $f_i(t)$ denote the $i$th coordinate of $f(t)$, and assume that
\begin{equation}
\label{eq:ergodicproperty}
\lim_{t\to\infty} \tfrac{1}{t}\sum_{k=1}^t |f_i(k)|=m_i
\end{equation}
exists and is finite for all $1 \le i \le d$.
Then all limit points of the sequence $\{\nu_t\}$, with respect to the weak topology, are probability measures over $\real^d$.
\end{lemma}
\begin{proof}
The statement of the lemma follows by Prokhorov's theorem if we show that the set of probability measures $\{\nu_t\}$ is tight.
By the convergence of $|f_i(t)|$, for any $\eps>0$ there exists a $T>0$ such that
\begin{equation}
\label{eq:converge}
\left| \tfrac{1}{t} \sum_{k=1}^t |f_i(k)| - m_i \right| <\eps
\end{equation}
for all $1 \le i \le d$ and $t>T$. Let $u_{T,i}=\tfrac{m_i+\eps}{\eps}$. Then the marginals
$\nu_{t,i}$ of $\nu_t$, $1\le i \le d$, satisfy $\nu_{t,i}([0,u_{T,i}]) > 1-\eps$ for all $i,t$ considered, otherwise \eqref{eq:converge} would be violated. Thus, the compact set
$K_{\eps}=\prod_{i=1}^d [0,u'_{T,i}]$ with $u'_{T,i}=\max\{u_{T,i},|f_i(1),\ldots,|f_i(T)|\}$ satisfies
$\nu_t(K_{\eps})>1-d \eps$ for all $t \ge 1$ by the union bound, showing that $\{\nu_t\}$ is tight.
\end{proof}
\fi
%Note that \cref{lem:ergodicfunction} guarantees that all the convergent subsequences of $\{\nu_t\}$ will converge to probability measures. The results of this paper depend on a stronger assumption, as follows.


\section{Identifiability conditions}
In this section we present a set of results that guarantee identifiability of the sources
based on an observed mixture, given that the sources are independent.

\subsection{Identifiability based on the Darmois-Skitovich theorem}
Our first result requires the minimal assumption that each component $f_i$ is non-Gaussian:%
\footnote{
In fact, as a multidimensional standard normal distribution is invariant under orthogonal transformations, if more than two components had a Gaussian distribution, identifying the mixing matrix would be impossible. \todoc[inline]{Now, can the result be extended to allow one Gaussian? Or restrict identifiability to the non-Gaussian components?}
}
\begin{thm}
\label{thm:CorofICA}
Let $f = (f_1,\ldots,f_d)^{\top}$ be an ergodic function with independent components and %\todoc{We always have real-valued functions, right? So maybe mention this always or nowhere} 
assume that neither of $\nu_{f_i}$, $i=1,\ldots,d$ is a Gaussian distribution. If
\begin{equation}
\left(
\begin{array}{ccc}
g_1 \\
\vdots \\
g_d
\end{array}
\right) = A
\left(
\begin{array}{ccc}
f_1 \\
\vdots \\
f_d
\end{array}
\right)
\end{equation}
are independent, then $A$ is equivalent to a permutation matrix up to scaling.
\end{thm}

The proof of \cref{thm:CorofICA} is based on the celebrated Darmois-Skitovich theorem:
\begin{thm}[Darmois-Skitovich theorem, \citep{Darmois53,Skitovich53}]
Let $\xi_1,\ldots,\xi_d$ be independent random variables. If the linear forms $L_1 = \alpha_1\xi_1 + \ldots + \alpha_d\xi_d$ and $L_2 = \beta_1\xi_1 +\ldots + \beta_d\xi_d$, where the coefficients $\alpha_i$ and $\beta_i$ are nonzero, are independent, then the random variables $\xi_i$ $(1\le i \le d$) are Gaussian variables.
\end{thm}
Although the theorem is usually stated in the above form, it is clear that it is a statement that expresses the relationship between certain distributions. In particular, the theorem can be rewritten with no mention of random variables at all. We do this as this will be immediately useful for us:
%For the theorem, introduce the operator $M_c$ for $c\in \real\setminus \{0\}$ that acts on probability distributions and is defined by $(M_c \nu)(A) = \nu( \cset{ x/c}{x\in A} )$.
\begin{prop}[Darmois-Skitovich theorem -- distributional form]
\label{prop:Darmois-Skitovich-distr}
For $i=1,\ldots, d$, let $\nu_i$ be probability distributions over the reals.
Fix the nonzero real numbers $(\alpha_i)$, $(\beta_i)$.
Define the probability distributions $\nu^{(L_1)}$, $\nu^{(L_2)}$ over the reals and 
the probability distribution
 $\nu^{(L_1,L_2)}$ over $\real^2$ using
\begin{align*}
\nu^{(L_1)}(A) & = \int \ind{ \sum_i \alpha_i x_i \in A} \,d\nu_1(x_1)\dots d\nu_d(x_d)\,,\\
\nu^{(L_2)}(B) & = \int \ind{ \sum_i \beta_i x_i \in B} \,d\nu_1(x_1)\dots d\nu_d(x_d)\,,\\
\nu^{(L_1,L_2)}(A\times B) 
& = \int \ind{ \sum_i \alpha_i x_i \in A, \sum_i \beta_i x_i\in B} \,d\nu_1(x_1)\dots d\nu_d(x_d)\,,
\end{align*}
where $A,B\subset \real$ are Borel sets.
If $\nu^{(L_1,L_2)} = \nu^{(L_1)} \otimes \nu^{(L_2)}$ then $\nu_i$ ($1\le i \le d$) are Gaussian distributions.
\end{prop}
From this result, we see that the following holds true:
\begin{lemma}
Let $f = (f_1,\ldots,f_d)^{\top}$ be an ergodic function with independent components and fix some nonzero reals
 $(\alpha_i)$, $(\beta_i)$.
Let $g = \sum_i \alpha_i f_i$, $h = \sum_i \beta_i f_i$.
If $g$ and $h$ are independent  then $\nu_{f_i}$, $i=1,\ldots,d$ are Gaussians.
\end{lemma}
\begin{proof}
We apply~\cref{prop:Darmois-Skitovich-distr} with $\nu_i = \nu^{(f_i)}$, $i=1,\ldots,d$.
We claim that $\nu^{(g)} = \nu^{(L_1)}$, $\nu^{(h)} = \nu^{(L_2)}$,
$\nu^{(g,h)} = \nu^{(L_1,L_2)}$.
From this the result follows since if $g$ and $h$ are independent then $\nu^{(L_1,L_2)} = \nu^{(g,h)} = \nu^{(g)} \otimes \nu^{(h)} = \nu^{(L_1)} \otimes \nu^{(L_2)}$.
The claimed equalities follow by \cref{prop:convolution}. \todoc{At least I think..}
\end{proof}

As an immediate corollary, we have the following result:
\begin{cor}
\label{cor:DSTheoremCor}
Let $f_1,\ldots,f_d$ be independent, ergodic functions such that $\nu^{(f_i)}$ are not Gaussian distributions.
Assume further that functions $g = \alpha_1 f_1 + \ldots + \alpha_d f_d$ and 
$h= \beta_1f_1 +\ldots + \beta_d f_d$ are also independent.
Then, $\cset{i}{ \alpha_i \neq0} \cap \cset{j}{ \beta_j \neq0} = \emptyset$.
\end{cor}
Now, the proof of  \cref{thm:CorofICA}  follows immediately from this result:
\begin{proof}[Proof of \cref{thm:CorofICA}]
From \cref{cor:DSTheoremCor} it follows that $\cset{j}{ a_{ij}\ne 0 }$, $1\le i \le d$,
 are $d$ non-empty mutually exclusive subsets of the size $d$ set $\{1,\ldots,d\}$.
 Hence, these sets must form a partition of the set, with one element in each subset, which proves the theorem.
\end{proof}

%\subsection{Identifiability based on kurtosis maximization}
%(Delfosse & Loubaton, 1995)

%\subsection{Negentropy minimization}

\subsection{A general observation}
The standard instantaneous noiseless ICA identifiability results can all be phrased as follows:
Let $S$ be a random $d$-dimensional vector, whose components are independent of each other.
Let $X = A S$, where $A$ is a $d\times d$ matrix. Then, given the joint distribution of $X$, assuming that the joint of $S$ satisfies some additional assumption, the matrix $A$ can be identified up to permutation and scaling. \todoc{The previous section has one example; do we need more!?}
What is important to realize that the identifiability results can be expressed in terms of the joint distributions of the source.

Now, it should be clear all these results carry through to the deterministic setting, where the joint distribution of $S$ is replaced by $\nu^{(f)}$, where $f$ is a $d$-dimensional deterministic signal.

\todoc{We should add the generalization of all the other ``characterization'' results or at least the generalization of some of them.}

\section{Analysis of ICA algorithms}

\subsection{Efficiency}
In this section, we will take the ICA algorithm of \citet{DHsu2012} as an example. 
Introduce the notation $\E_{x\sim \nu}[f(x)]$ as a shorthand for $\int f(x) d\nu(x)$.
Assume the independent ergodic component functions $s_1,\ldots,s_d:\natural \ra \real$ are bounded by a constant $C$, and satisfy $\E_{x\sim\nu^{(s_i)}}[x]=0$, $\E_{x\sim \nu^{(s_i)}}[x^2]=1$, and $\kappa_i := \E_{x\sim \nu^{(s_i)}}[x^4]\neq 3$.%
\footnote{In this section, we will denote the sources by $s_i$ to avoid a clash with the function $f$ used by  \citet{DHsu2012} for a different purpose.}
%Here $\E_{\nu_{f_i}}$ is the expectation is taken with respect to the probability measure of $\nu_{f_i}$. 
For $1\le k\le d$, denote $A_{(t)} = (A_k,A_{ck})$, where $A_{ck}$ is the $(d-1)\times d$ matrix $(A_1,\ldots,A_{k-1},A_{k+1},\ldots,A_d)$, where $A_i$ is the $i$-th column of $A$. 
Without loss of generality, assume $\|A_i\|_2=1$.

For $p\ge 1$, $\eta\in \real^d$, 
let 
\begin{equation}
\label{eq:mpdef}
m_p(\eta) = \E_{x\sim \nu^{(s)}}[ (\eta^\top A x)^p ]
\end{equation}
 and let
\begin{equation}\label{eq:fdef}
f(\eta) = \frac1{12} \left( m_4(\eta) - 3 m_2(\eta)^2 \right)\,.
\end{equation}

The algorithm of \citet{DHsu2012} is as follows. 
\begin{algorithm}[H]
\caption{ICA algorithm of \citet{DHsu2012} \label{alg:icaHsu}}
\begin{algorithmic}[1]
\INPUT $g(k)$ for $1\le k \le t$. \todoc[inline]{start at zero!?}
\OUTPUT the estimate of the mixing matrix $A$. 
\STATE Sample $\phi$ and $\psi$ from a unit ball of $\real^d$, such that $\|\phi\|_2 = 1$ and $\|\psi\|_2 = 1$;
\STATE Evaluate $\nabla^2\widehat{f}(\phi)$ and $\nabla^2\widehat{f}(\psi)$, \\
\quad where $\widehat{m_p}(\eta) = \frac{1}{t}\sum_{k=1}^{t} (\eta^{\top}g(k))^p$, and $\widehat{f}(\eta) = \frac{1}{12}\big(\widehat{m_4}(\eta) - 3\widehat{m_2}(\eta)^2 \big)$;
\STATE Compute $\widehat{M} = (\nabla^2 \widehat{f}(\phi))(\nabla^2\widehat{f}(\psi))^{-1}$;
\STATE Compute all the eigenvectors of $\widehat{M}$, $\{\mu_1,\ldots,\mu_d\}$;
\STATE Return $\widehat{A} = (\mu_1,\ldots,\mu_d)$.
\end{algorithmic}
\end{algorithm}
In general, we denote by adding a ``hat'' on the top of a symbol its empirical estimate.
Note that the output of the algorithm entirely depends on the empirical distribution $\hat{\lambda}$ of $( g(k) )_{1\le k \le t}$ (i.e.,  $\hat{\lambda}(U) = \frac1t \sum_{k=1}^t \ind{g(k)\in U}$, $U\subset \real^d$). 
In particular, the output depends on $\E_{x\sim \hat{\lambda}}[ \nabla^2 (\eta^\top  x)^4 ]$
and $\nabla^2 \left( \E_{x\sim \hat{\lambda}}[(\eta^\top  x)^2 ]\right)^2$
 only, where $\eta\in \{\phi,\psi\}$. 
 \todoc[inline]{We expect that any ICA algorithm can be written as a mapping from the empirical distribution to the estimates. If $\FF$ is this mapping, the problem (when analyzing a particular ICA algorithm) is to derive an error estimate for  how close $\FF(\hat{\lambda})$ is to an unmixing matrix as a function of how close $\hat{\lambda}$ is to $\lambda(\cdot)$, $\lambda(U) = \E_{x\sim \nu}[ \ind{Ax\in U} ]$ where $A$ is some non-singular matrix and $\nu$ is some distribution which is the product of its marginals.
 Where should we discuss this?
 }

The following result is proven by \citet{DHsu2012}:
\begin{thm}[\citet{DHsu2012}, Theorem~4]
Assume that $\nu^{(s)} = \nu^{(s_1)}\otimes \dots \otimes \nu^{(s_d)}$,
$\E_{x\sim \nu^{(s_i)}}[x] = 0$, $\E_{x\sim \nu^{(s_i)}}[x^2] = 1$, 
$\E_{x\sim \nu^{(s_i)}}[x^4] \ne 3$, $i=1,\ldots,d$, $A$ is nonsingular
and that the columns $(A_i)_{1\le i \le d}$ of matrix $A$ have unit length. 
Let $m_4,m_2:\real^d \ra \real$ be defined by \eqref{eq:mpdef},
	while $f: \real^d \ra \real$ be defined by \eqref{eq:fdef}.
Let $\phi,\psi\in \real^d$ be vectors from the unit sphere of $\real^d$. Then, 
	assuming that the values $\lambda_i = \left(\frac{\phi^{\top}A_i}{\psi^{\top}A_i}\right)^2$
	are all distinct,
	the matrix
\begin{equation}
\label{eq:M}
M =(\nabla^2f(\phi))(\nabla^2f(\psi))^{-1} 
\end{equation}
can be written in the diagonal form
\begin{equation}
\label{eq:M2}
M = A 
\left(
\begin{array}{ccc}
\lambda_1 & & \\ %\left(\frac{\phi^{\top}A_1}{\psi^{\top}A_1}\right)^2 & &\\
    & \ddots & \\
    & & \lambda_d %\left(\frac{\phi^{\top}A_d}{\psi^{\top}A_d}\right)^2\\
\end{array} 
\right) 
A^{-1},
\end{equation}
\end{thm}
It follows from this theorem that 
if $\phi,\psi$ are chosen independently from the uniform distribution on the unit sphere of $\real^d$, with probability one,
the eigenvalues of  $M$ are all distinct and the corresponding eigenvectors
determine the rows of $A$ up to permutation and scaling.

Let 
\begin{equation}
\label{def:kappa}
\gamma =  \min_{i,j: i\neq j} \left\vert \left(\frac{\phi^{\top}A_i}{\psi^{\top}A_i}\right)^2 - \left(\frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert. 
\end{equation}
The following lemma shows that a small perturbation of $M$ will only result in a small variation of its eigenvectors, at least under some mild regularity conditions.

\begin{lemma}
\label{lem:eigenvectorvariation}
Denote $\widehat{M} = M+E$ be a perturbation of matrix $M$, where $M$ is defined in  \eqref{eq:M2}. 
Assume $\widehat{M}$ has distinct eigenvalues. Let $\delta = \gamma -  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$. 
If $\gamma > 4 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$, and $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\delta}\frac{\sigma_{\max}(A)}{\sigma_{\min}(A) } \|E\|_2$, then there exist a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that 
\[
\sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le 4d  \frac{\sigma_{\max}(A)}{\delta \sigma_{\min}(A) } \|E\|_2\,,
\]
where $\widehat{A}$ is the matrix of eigenvectors of $\widehat{M}$. 
\end{lemma}

\begin{proof}
For $1\le k\le d$, assume 
\[
A_{(k)}^{-1} E A_{(k)} =  
\left(
\begin{array}{cc}
F_{1k} & F_{2k}\\
F_{3k} & F_{4k} \\
\end{array} 
\right). 
\] 
Note that by definition, $\|F_{1k}\|_2,\|F_{4k}\|_2\le\|A_{(k)}^{-1} E A_{(k)}\|_2\le\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$,
 $\|F_{3k}\|_2\le\|A_{(k)}^{-1}EA_{k}\|_2\le\frac{1}{\sigma_{\min}(A)}\|E\|_2$,
 and $\|F_{2k}\|_2\le\|(A^{-1})_kEA_{(k)}\|_2\le\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$.
Let $\gamma_k = \|F_{3k}\|_2$, $\eta_k = \|F_{3k}\|_2$, and 
\[
\delta_k = \min_{j: j\neq k} 
\left\vert \left(\frac{\phi^{\top}A_k}{\psi^{\top}A_k}\right)^2 -\left( \frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert - \|F_{1k}\|_2 - \|F_{4k}\|_2\,.
\]
Thus 
\begin{align*}
\delta_k & = \min_{j:j\neq k} 
	\left\vert \left(\frac{\phi^{\top}A_k}{\psi^{\top}A_k}\right)^2 - \left(\frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert - \|F_{1k}\|_2 - \|F_{4k}\|_2\\
	& \ge \min_{j:j\neq k} \left\vert \left(\frac{\phi^{\top}A_k}{\psi^{\top}A_k}\right)^2 - \left(\frac{\phi^{\top}A_j}{\psi^{\top}A_j}\right)^2 \right\vert - 2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2\\
	& \ge  \gamma -  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2\\
	& >  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2 >0,
\end{align*}
and $\delta_k^2 > 4\gamma_k\eta_k$. 
Therefore, by Theorem 2.8, Chapter V of \citep{stewart1990matrix}, there exist a unique vector $v$ satisfying $\|v\|_2\le 2\frac{\gamma_k}{\delta_k}$ such that there exists one of a eigenvector $\widehat{A_k}$ of $\widehat{M}$ satisfying
 \[
 \|\widehat{A_k} - A_k\|_2 \le \|A_{ct}\|_2 \|v\|_2 \le 2\sigma_{\max}(A)\frac{\gamma_k}{\delta_k}.\le \frac{4\sigma_{\max}(A)}{\delta \sigma_{\min}(A) } \|E\|_2.
 \]
 By condition, for $i\neq j$,  $\frac{8\sigma_{\max}(A)}{\delta \sigma_{\min}(A) } \|E\|_2 < \|A_i - A_j\|_2\le \|A_i - \widehat{A_i}\|_2 + \|A_j - \widehat{A_i}\|_2$, thus $\widehat{A_i} \neq \widehat{A_j}$.  Summing up the upper bound gets the result. 
\end{proof}

The next lemma shows that $\|\widehat{X}^{-1}\|_2 = \|(X+E)^{-1}\|_2$ is close to $\frac{1}{\sigma_{\min}(X)}$.

\begin{lemma}
\label{lem:inversevariation}
If non-singular matrix $\widehat{X} = X+E$ satisfying that $\sigma_{\min}(X)>\|E\|_2$, then  $\|\widehat{X}^{-1}\|_2 \le \frac{1}{\sigma_{\min}(X)-\|E\|_2}$.
\end{lemma} 
\begin{proof}
Consider the minimal singular value of $\widehat{X}$. 
\[
 \min_{v:\|v\|_2=1} \|\widehat{X}v\|_2 = \min_{v:\|v\|_2=1}\|(X+E)v\|_2 \ge \|Xv\|_2 - \|Ev\|_2 \ge \sigma_{\min}(X) - \|E\|_2.
\]
\end{proof}

Now we can estimate the variance between $XY^{-1}$ and $(X+E_1)(Y+E_2)^{-1}$.
\begin{lemma}
\label{lem:Mvariation}
Assume $\sigma_{\min}(Y) > \|E_2\|_2$, then 
\[
\| XY^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \le \frac{c\|X\|_2\|E_2\|_2}{(\sigma_{\min}(Y) - \|E_2\|_2)^2}+\frac{\|E_1\|_2}{\sigma_{\min}(Y) - \|E_2\|_2},
\]
where $c = \frac{1+\sqrt{5}}{2}$.
\end{lemma}
\begin{proof}

\begin{align*}
	& \| XY^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \\
\le & \| XY^{-1} - X(Y+E_2)^{-1}\|_2 + \| X(Y+E_2)^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \\
\le & \|X\|_2\| Y^{-1} - (Y+E_2)^{-1}\|_2 + \| E_1\|_2 \|(Y+E_2)^{-1}\|_2 \\
\le & c \|X\|_2 \max\{\| Y^{-1}\|^2_2 , \|(Y+E_2)^{-1}\|^2_2\} \|E_2\|_2 + \| E_1\|_2 \|(Y+E_2)^{-1}\|_2,
\end{align*}
where $c = \frac{1+\sqrt{5}}{2}$. The last inequality is by Theorem 3.8, Chapter III of \citep*{stewart1990matrix}. Therefore by \cref{lem:inversevariation},
\[
\| XY^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \le \frac{c\|X\|_2\|E_2\|_2}{(\sigma_{\min}(Y) - \|E_2\|_2)^2}+\frac{\|E_1\|_2}{\sigma_{\min}(Y) - \|E_2\|_2}.
\]
\end{proof}

Note that in \cref{alg:icaHsu}, 
\begin{align*}
& \nabla^2 f(\eta) = G_1(\eta) - G_2(\eta) -G_3(\eta), \\
\text{and } \quad & \nabla^2 \widehat{f}(\eta) =\widehat{G_1}(\eta) - \widehat{G_2}(\eta) -\widehat{G_3}(\eta),
\end{align*}
where 
\begin{align*}
& G_1(\eta) = \int (\eta^{\top}Af)^2Aff^{\top}A^{\top}\,d\nu; \\
& G_2(\eta) = \int (\eta^{\top}Af)^2\,d\nu \int Aff^{\top}A^{\top} \,d\nu; \\
& G_3(\eta) = \Big(\int (\eta^{\top}Af)Af\,d\nu\Big)\Big(\int (\eta^{\top}Af)Af\,d\nu\Big)^{\top}. \\
&\widehat{ G_1}(\eta) = \frac1n\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)^2g(k)g(k)^{\top} = \int (\eta^{\top}Af)^2Aff^{\top}A^{\top}\,d\nu_t; \\
& \widehat{G_2}(\eta) = \frac{1}{n^2}\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)^2 \sum_{k=1}^{n}g(k)g(k)^{\top} = \int (\eta^{\top}Af)^2\,d\nu_t \int Aff^{\top}A^{\top} \,d\nu_t; \\
& \widehat{G_3}(\eta) = \frac{1}{n^2}\Big(\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)g(k)\Big) \Big(\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)g(k)\Big)^{\top} = \Big(\int (\eta^{\top}Af)Af\,d\nu_t\Big)\Big(\int (\eta^{\top}Af)Af\,d\nu_t\Big)^{\top}.
\end{align*}
Given two probability measure $\mu$ and $\nu$ over $(\Omega , \xi)$, recall the definition of total variance, $D_{tv}(\mu, \nu) = \sup_{A\in\xi} |\mu(A) - \nu(A)|$. 
The following lemma bounds $\|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta) \|_2$ by $D_{tv}(\nu_t , \nu)$:
\begin{lemma}
\label{lem:nablavariation}
\[
\|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta)  \|_2 \le d q D_{tv}(\nu_t , \nu) + 2dq D_{tv}(\nu_t , \nu)^2,
\]
where $q_{\eta} = d^2C^4\|\eta\|_2^2\sigma_{\max}(A)^4$.
\end{lemma}
\begin{proof}
Denote the $i$-th row of $A$ by $A_{:,i}$, and the $(i,j)$ entry of a matrix $M$ by $(M)_{i,j}$. Since $\|f_i\|_\infty \le C$,$\|f\|_2 \le C\sqrt{d}$. 
Thus $\big(\eta^{\top}Af\big)^2 \le d C^2\|\eta\|_2^2\sigma_{\max}(A)^2$ 
% \le d C^2\sigma_{\max}(A)^2 $
,  $|A_{:i}f| \le C\sqrt{d}\sigma_{\max}(A)$, 
therefore $|\big(\eta^{\top}Af\big)^2 A_{:i}ff^{\top} A_{:j}^{\top}| \le d^2C^4\sigma_{\max}(A)^4 \|\eta\|_2^2 = q_{\eta}$.
\[
|(G_1)_{i,j} - (\widehat{G_1})_{i,j} | \le q_{\eta} D_{tv}(\nu_t , \nu).
\]
Similarly, 
\begin{align*}
& |(G_2)_{i,j} - (\widehat{G_2})_{i,j} | \le q_{\eta} D_{tv}(\nu_t , \nu)^2 \\
\text{and } \quad & |(G_3)_{i,j} - (\widehat{G_3})_{i,j} | \le q D_{tv}(\nu_t , \nu)^2.
\end{align*}
Therefore,
\begin{align*}
	& \|\nabla^2 f(\eta) - \nabla^2 \widehat{f}(\eta)  \|_2 \\
\le & \|G_1 - \widehat{G_1}\|_2 + \|G_2 - \widehat{G_2}\|_2 + \|G_3 - \widehat{G_3}\|_2 \\
\le & d q_{\eta} D_{tv}(\nu_t , \nu) + 2dq_{\eta} D_{tv}(\nu_t , \nu)^2.
\end{align*}
\end{proof}

\if0
We still need to bound $\|\big(\nabla^2 f(\eta)\big)^{-1} - \big(\widehat{\nabla^2 f(\eta)}\big)^{-1}  \|_2 $, as follows.
\begin{lemma}
Assume $\|\eta\|_2 = 1$, then 
\[
\|\big(\nabla^2 f(\eta)\big)^{-1} - \big(\widehat{\nabla^2 f(\eta)}\big)^{-1}  \|_2 \le cd^2q^2\big(D_{tv}(\nu_n , \nu) + 2D_{tv}(\nu_n , \nu)^2\big).
\]
\end{lemma}
\begin{proof}
From the proof of \cref{lem:boundnablafeta}, $\|\nabla^2 f(\eta)\|_2, \|\widehat{\nabla^2 f(\eta)}\|_2 \le dq$, where $q = d^2C^4\sigma_{\max}(A)^4$.
Apply Theorem 3.8 of \citep*{stewart1990matrix}, we have 
\begin{align*}
& \|\big(\nabla^2 f(\eta)\big)^{-1} - \big(\widehat{\nabla^2 f(\eta)}\big)^{-1}  \|_2 \\
\le & c\max\{\|\nabla^2 f(\eta)\|_2, \|\widehat{\nabla^2 f(\eta)}\|_2\} \|\nabla^2 f(\eta) - \widehat{\nabla^2 f(\eta)}\|_2 \\
\le & cd^2q^2\big(D_{tv}(\nu_n , \nu) + 2D_{tv}(\nu_n , \nu)^2\big).
\end{align*}

\end{proof}
\fi


 \begin{thm}
 \label{thm:efficiency}
 Let 
 \[ 
 Q=  cd^2 \frac{q_{\psi}q_{\phi}\big(D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}{\big(\sigma_{\min}(\nabla^2f(\psi)) - d q_{\psi} \big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)\big)^2}
 +d\frac{q_{\phi}\big(D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}{\sigma_{\min}(\nabla^2f(\psi)) - dq_{\psi} \big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}.
 \] 
 Assume $t$ is large enough such that the following conditions hold:
 \begin{itemize}
 \item $\widehat{M}$ has distinct eigenvalues;
 \item $\sigma_{\min}(\nabla^2f(\psi)) > d q_\psi\big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)$;
 \item $\gamma > 4\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} Q$
 \item $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\delta}\frac{\sigma_{\max}(A)}{\sigma_{\min}(A) } Q$.
 \end{itemize}
 Then there exist a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4d^2q(cdq+1)}{\delta} \frac{\sigma_{\max}(A)}{ \sigma_{\min}(A)}Q ,
 \]
 where $\delta = \gamma -  2\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}Q>0$.
 \end{thm}
 \begin{proof}
 By \cref{lem:eigenvectorvariation}, 
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le 4d  \frac{\sigma_{\max}(A)}{\delta \sigma_{\min}(A) } \|M - \widehat{M} \|_2. 
 \]
 Note that $M = \nabla^2f(\phi))(\nabla^2f(\psi))^{-1}$,  then by \cref{lem:Mvariation},
 \[
 \|M - \widehat{M} \|_2 \le \frac{c\|\nabla^2f(\phi)\|_2\|\nabla^2f(\psi) - \nabla^2\widehat{f}(\psi)\|_2}{\big(\sigma_{\min}(\nabla^2f(\psi)) - \|\nabla^2f(\psi) - \nabla^2\widehat{f}(\psi)\|_2\big)^2} + \frac{\|\nabla^2f(\phi) - \nabla^2\widehat{f}(\phi)\|_2}{\sigma_{\min}(\nabla^2f(\psi)) - \|\nabla^2f(\psi) - \nabla^2\widehat{f}(\psi)\|_2}. 
 \]
 From the proof of \cref{lem:nablavariation}, we have $\|\nabla^2 f(\phi)\|_2\le dq_{\phi}$, where $q_{\phi} = d^2C^4\sigma_{\max}(A)^4\|\phi\|_2^2$. Apply \cref{lem:nablavariation}, 
 \begin{align*}
 \|M - \widehat{M} \|_2 \le &
 cd^2 \frac{q_{\psi}q_{\phi}\big(D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}{\big(\sigma_{\min}(\nabla^2f(\psi)) - d q_{\psi} \big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)\big)^2} \\
 	&\quad +d\frac{q_{\phi}\big(D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}{\sigma_{\min}(\nabla^2f(\psi)) - dq_{\psi} \big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}\\
 = & Q.
 \end{align*}
 So, 
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4d}{\delta} \frac{\sigma_{\max}(A)}{ \sigma_{\min}(A)}Q. 
 \]
 \end{proof}

\subsection{Condition number of ICA problem}
From \cref{thm:efficiency}, two parameters in the efficiency that depend on the algorithm rather than the ICA problem are the number $\gamma$ and $Q$. 
When $n$ is large enough, 
\[
Q\approx \left[ \frac{2q_{\psi}q_{\phi}}{\big(\sigma_{\min}(\nabla^2f(\psi))\big)^2}+\frac{q_{\phi}}{\sigma_{\min}(\nabla^2f(\psi))}\right]D_{tv}(\nu_n , \nu)^2.
\] 
Note that $\nabla^2f(\psi) = \sum_{i=1}^{d} \kappa_i(\psi^{\top}A_i)^2A_iA_i^{\top}$. Thus, $\sigma_{\min}(\nabla^2f(\psi)) = \min_{v:\|v\|_2=1}\|\sum_{i=1}^{d} \kappa_i(\psi^{\top}A_i)^2A_iA_i^{\top}v\|_2$ 

\begin{claim}
With large probability, $\sigma_{\min}(\nabla^2f(\psi))/\|\psi\|_2^2$ won't be small.
\end{claim}

The next
\begin{thm}
There exists a way sampling $\phi$ and $\psi$, such that $\gamma = \Theta(\log d)$.
\end{thm}
\begin{proof}
Instead of sampling $\phi$ and $\psi$ uniformly from the unit ball, sample $\phi,\psi \sim {\cN} (0,\,A^{-1}A^{{-1}^{\top}})$ independently. Therefore, $\alpha_i = \phi^{\top}A_i$, $\beta_i = \psi^{\top}A_i$ are independent standard normal variables. Moreover, 
\[
\gamma  = \min_{i,j} \left\vert \big(\frac{\alpha_i}{\beta_i}\big)^2 - \big(\frac{\alpha_j}{\beta_j}\big)^2 \right\vert.
\] 

Note that for $x\ge0$, $\Phi(x) \approx \frac12 (1+\sqrt{1-e^{-\frac{2}{\pi}x^2}})$ \citep{aludaat2008note}.
 
 When $d$ is large,
\[
\Prob{|\beta_i| \ge \sqrt{\frac{\pi}{2}\log \frac{d^2}{d^2-1}} \text{ for all } \beta_i\text{'s}} = (2- 2\Phi(\sqrt{\frac{\pi}{2}\log \frac{d^2}{d^2-1}})-1)^d \approx \left(1-\frac{1}{d}\right)^d \approx \frac{1}{e}.
\]
Thus, with probability at least $p (\approx \frac{1}{e})$,
\[
\gamma \le 2 \min_i \left\vert \big(\frac{\alpha_i}{\beta_i}\big)^2\right\vert \le 2 \frac{2}{\pi}\frac{1}{\log \frac{d^2}{d^2-1}}\min_i |\alpha_i|^2.
\]

Now consider the case that at least two of $\beta_i$'s are greater than $\sqrt{\frac{\pi}{2}\log d}$,
\begin{align*}
& \Prob{|\beta_i| \ge \sqrt{\frac{\pi}{2}\log d} \text{ for at least 2 } \beta_i\text{'s}} \\
=  & 1 - \Prob{|\beta_i| \le \sqrt{\frac{\pi}{2}\log d} \text{ for all } \beta_i\text{'s}} - \Prob{|\beta_i| \le \sqrt{\frac{\pi}{2}\log d} \text{ for (d-1) } \beta_i\text{'s}} \\
\approx & 1 - (1-\frac{1}{d})^{\frac{d}{2}} -  (1-\frac{1}{d})^{\frac{d-1}{2}}(1-\sqrt{1-\frac{1}{d}}) \\
\approx & 1 -\frac{1}{\sqrt{e}}.
\end{align*}
Thus, with probability at least $p_2$ ($\approx 1-\frac{1}{\sqrt{e}}$),
\[
\E[\gamma] \le \frac{4}{\pi} \frac{1}{\log d} \E[\alpha_i^2] = \frac{4}{\pi} \frac{1}{\log d}.
\]
\end{proof}

So the efficiency of the algorithm of \citet{DHsu2012} depends on $\gamma$. Is it actually independent to the algorithm? If so, we can define it as the condition number of a ICA problem, which define how difficult the ICA problem is. 

\bibliography{DICA}
\bibliographystyle{plainnat}
\end{document}
