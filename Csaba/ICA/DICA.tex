\documentclass[english]{article} % For LaTeX2e

\usepackage[usenames,dvipsnames]{xcolor}
% uncomment the following line and comment the line after it if you want to turn
% off todos
%\usepackage[disable,backgroundcolor = White,textwidth=\marginparwidth]{todonotes}
\usepackage[backgroundcolor = White,textwidth=\marginparwidth]{todonotes}
\newcommand{\todoc}[2][]{\todo[color=Apricot,#1]{#2}}
\newcommand{\todob}[2][]{\todo[color=Cerulean!20,#1]{#2}}
\newcommand{\todoa}[2][]{\todo[color=Purple!20,#1]{#2}}
\newcommand{\todor}[2][]{\todo[color=Blue!10,#1]{#2}}

\usepackage{nips12submit_e,times}

\usepackage{comment}
\usepackage{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[authoryear]{natbib}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{setspace}


\title{Deterministic ICA}


\author{
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version


\usepackage{algorithmic}
\usepackage{babel}


\newcommand{\cN}{\cal{N}}
\renewcommand{\P}{{\mathcal P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ex}[1]{\mathbb{E}[#1]}
\newcommand{\Em}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\snorm}[1]{\left\|#1\right\|} % scaling norm
\newcommand{\lmax}[1]{\lambda_{\mathrm{max}}(#1)}
\newcommand{\lmin}[1]{\lambda_{\mathrm{min}}(#1)}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\dprod}[2]{\langle #1,#2 \rangle_{M}}
\newcommand{\hA}{\hat{A}}
\newcommand{\hb}{\hat{b}}
\newcommand{\hC}{\hat{C}}
\newcommand{\hAp}{\hat{A}^\prime}
\newcommand{\hbp}{\hat{b}^\prime}
\newcommand{\hCp}{\hat{C}^\prime}
\renewcommand{\th}{\theta}
\newcommand{\vth}{\hat{\theta}_{V}}
\newcommand{\lth}{\hat{\theta}_{\lambda}}
\newcommand{\hth}{\hat{\theta}}
\newcommand{\ltho}{\hat{\theta}_{\lambda_1}}
\newcommand{\ltht}{\hat{\theta}_{\lambda_2}}
\newcommand{\hlth}{\hat{\theta}_{\hat{\lambda}}}
\newcommand{\slth}{\hat{\theta}_{\lambda^*}}
\newcommand{\plth}{\hat{\theta}_{\lambda_p}}
\newcommand{\hthp}{\hat{\theta}^\prime}
\newcommand{\lthp}{\hat{\theta}^\prime_{\lambdap}}
\newcommand{\ath}{\hat{\theta}_{a}}
\newcommand{\thetap}{\theta^\prime}
\newcommand{\sth}{\theta^*}
\newcommand{\ind}[1]{\mathbb{I}_{\left\{ #1 \right\}}}
\newcommand{\hzeta}{\hat{\zeta}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\cf}{\emph{cf.}}
\newcommand{\T}[1]{T\left( #1 \right)}
\newcommand{\St}[1]{S\left( #1 \right)}
\newcommand{\Ts}[2]{T_{#1}\left( #2 \right)}
\newcommand{\Ss}[2]{S_{#1}\left( #2 \right)}
\newcommand{\Ord}[1]{O\left( #1 \right)}
\newcommand{\etc}{\emph{etc.}}
%\newcommand{\det}{\mathrm{det}}
\newcommand{\bsth}{\bar{\theta}^*}
\newcommand{\sal}{\emph} %Emphasis the J.D. Salinger way :)
\newcommand{\gradL}{\nabla \mathcal{L}}
\newcommand{\lambdap}{{\lambda^\prime}}
\newcommand{\lambdahp}{\hat{\lambda}^\prime}
\newcommand{\pqLoss}[1]{\mathcal{L}_{#1}}
\newcommand{\pLoss}[2]{\pqLoss{#1}( #2 )}
\newcommand{\qLoss}{\pqLoss{M}}
\newcommand{\Loss}[1]{\qLoss(#1)}

\newcommand{\hpqLoss}[1]{\mathcal{\hat{L}}_{#1}}
\newcommand{\hpLoss}[2]{\hpqLoss{#1}( #2 )}
\newcommand{\hqLoss}{\hpqLoss{M}}
\newcommand{\hLoss}[1]{\hqLoss(#1)}

\newcommand{\rth}{\hat{\theta}}
\newcommand{\tth}{\tilde{\theta}}
\newcommand{\maxeig}{\nu_{\max}}
\newcommand{\mineig}{\nu_{\min}}

\newcommand{\ra}{\rightarrow}
\newcommand{\real}{\mathbb{R}}

\newcommand{\one}[1]{\mathbf{1}_{\{#1\}}}

\newcommand{\rl}[1]{\mathbb{R}^{#1}}
\newcommand{\Mset}{\mathcal{M}(\varepsilon)}
\newcommand{\Cset}{\mathcal{C}}
\newcommand{\truel}{L}
\newcommand{\zol}{L_{\mathrm{0-1}}}
\newcommand{\hinl}{L_{\mathrm{hinge}}}
\newcommand{\phil}{L_{\varphi}}
\newcommand{\lspace}{\left\{1, \dots,K \right\}}
\newcommand{\risk}{\mathcal{R}}
\newcommand{\scoref}{\in \mathcal{H}}
\newcommand{\subscoref}{\in \mathcal{H}^\prime}
\newcommand{\hQ}{\hat{Q}}
\DeclareMathOperator{\supp}{supp}
\renewcommand{\natural}{\mathbb{N}}

\providecommand{\lemmaname}{Lemma}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\providecommand{\claimname}{Claim}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\corollaryname}{Corollary}

\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\newtheorem{lemma}[thm]{\protect\lemmaname}
\newtheorem{claim}[thm]{\protect\claimname}
\newtheorem{cor}[thm]{\protect\corollaryname}



\theoremstyle{definition}
\newtheorem{definition}{\protect\definitionname}


\theoremstyle{remark}
\newtheorem{rem}{\protect\remarkname}[thm]
\newtheorem{example}{\protect\examplename}[section]

\newcounter{assumption}%[section]
\newcommand{\theassumptionletter}{A}
\renewcommand{\theassumption}{\theassumptionletter\arabic{assumption}}

\newenvironment{ass}[1][]{\begin{trivlist}\item[] \refstepcounter{assumption}%
 {\bf Assumption\ \theassumption\ #1} \it}{%\par\nobreak\noindent\sl\ignorespaces}{%
 \ifvmode\smallskip\fi\end{trivlist}}
\newcommand{\aref}[1]{(\ref{#1})}
\newenvironment{ass*}[1][]{\begin{trivlist}\item[] %
 {\bf Assumption\  #1} }{%\par\nobreak\noindent\sl\ignorespaces}{%
 \ifvmode\smallskip\fi\end{trivlist}}

\begin{document}


\maketitle


\begin{abstract}

\end{abstract}

\section{Motivation}
\label{sec:motivation}
ICA (independent component analysis) algorithms are known to work well for unmixing the mixtures of various deterministic signals, as seen in one of the ICA tutorials, for example.
The ICA identifiability question is what signals (functions) can be recovered from their mixtures.

Let $f:\natural \ra \real^d$ be a $d$-dimensional deterministic signal. We will denote by $f_i$ the $i$th component of $f$.
Let $A = (a_{ij})$ be a $d\times d$ non-singular matrix and let $g:\natural \ra \real^d$ be defined by
\[
g(t) = A f(t), \quad t\in \natural.
\]
In the ICA problem one observes the values of $g$ in a sequential problem and the goal is to recover the components of $f$ (up to scaling and permutation).

We are not the first to ask the ICA identifiability question for deterministic signals (although this is not a thoroughly studied question).
In particular, in their paper Pando G. Georgiev and Fabian J. Thei note the following:
\begin{quote}
 ``Our objective is to estimate the source signals sequentially one-by-one or simultaneously assuming that they are statistically independent.
 %
The uniqueness of such estimation (up to permutation and scaling), or identifiability of the linear ICA model, is justified in the literature by the Skitovitch-Darmois theorem [41,17].
 %
 Whereas this theorem is probabilistic in nature, an elementary lemma from optimization theory (although with a non-elementary proof) can serve the same purpose -- rigorous justification of the identifiability of ICA model, when maximization of the cumulants is used.\footnote{
Optimization Techniques for Data Representations with Biomedical Applications, in
P.M. Pardalos, H.E. Romeijn (eds.), Handbook of Optimization in Medicine,	253 Springer Optimization and Its Applications 26,  Springer Science+Business Media LLC 2009
pp. 253--290.}
\end{quote}

Our purpose is to extend this observation to other ICA models that do not rely on the maximization of cumulants, but rely on the concepts of independence (and thus, on the Darmois-Skitovitch theorem).

\section{Independence of Components}
\label{sec:IndeofComp}
Given a sequence of probability measures $(\mu_t)_{t=0,1,\ldots}$ over a measurable space
$(\Omega, \mathcal{A})$ and a measurable function $f:\Omega\rightarrow\real^d$, 
consider the set of probability measures over $\real^d$ induced by $f$: 
for any $t\ge0$, let $\nu_t$ denote the a probability measure over $\real^d$ such that for any Borel set $A \in \real^d$, $\nu_t(A)=\int \ind{f(x) \in A}\, d\mu_t(x)$. 

\if0
Consider the sequence of empirical measures over $\real^d$ introduced by $f:\natural \rightarrow \real^d$: for any $t\ge 1$, let $\nu_t$ denote the a probability measure over $\real^d$ such that
for $ $ any Borel set $A \in \real^d$, $\nu_t(A)=\tfrac{1}{t} \sum_{k=1}^t \ind{f(k) \in A}$. 
\fi

\begin{lemma}
\label{lem:nicefunction}
Let $f_i$ denote the $i$th coordinate function of $f$ and assume that
\begin{equation}
\label{eq:niceproperty}
\lim_{t\to\infty} \int |f_i(x)|\, d\mu_t(x) 
\end{equation}
exists and is finite for all $1 \le i \le d$.
Then all limit points of the sequence $\{\nu_t\}$, with respect to the weak topology, are probability measures over $\real^d$.
\end{lemma}
\begin{proof}
The statement of the lemma follows by Prokhorov's theorem if we show that the set of probability measures $\{\nu_t\}$ is tight.
Denote by $m_i\in \mathbb{R}$ the limit of $ \int |f_i(x)|\, d\mu_t(x)$.
By the convergence of $|f_i(t)|$, for any $\epsilon>0$ there exists a $T>0$ such that
\begin{equation}
\label{eq:converge}
\left| \int |f_i(x)|\, d\mu_t (x)- m_i \right| <\epsilon
\end{equation}
for all $1 \le i \le d$ and $t>T$. Let $u_{T,i}=\tfrac{m_i+\epsilon}{\epsilon}$. Then the marginals
$\nu_{t,i}$ of $\nu_t$, $1\le i \le d$, satisfy $\nu_{t,i}([-u_{T,i},u_{T,i}]) > 1-\epsilon$ for all $i,t$ considered, otherwise \eqref{eq:converge} would be violated. Thus, by the union bound, the compact set
$K_{\epsilon}=\prod_{i=1}^d [-u'_{T,i},u'_{T,i}]$ with $u'_{T,i}=\max\{u_{T,i},|f_i(1)|,\ldots,|f_i(T)|\}$ satisfies
$\nu_t(K_{\epsilon})>1-d \epsilon$ for all $t \ge 1$, showing that $\{\nu_t\}$ is tight.
\end{proof}


\if0
\begin{lemma}
\label{lem:nicefunction}
Let $f_i(t)$ denote the $i$th coordinate of $f(t)$, and assume that
\begin{equation}
\label{eq:niceproperty}
\lim_{t\to\infty} \tfrac{1}{t}\sum_{k=1}^t |f_i(k)|=m_i
\end{equation}
exists and is finite for all $1 \le i \le d$.
Then all limit points of the sequence $\{\nu_t\}$, with respect to the weak topology, are probability measures over $\real^d$.
\end{lemma}
\begin{proof}
The statement of the lemma follows by Prokhorov's theorem if we show that the set of probability measures $\{\nu_t\}$ is tight.
By the convergence of $|f_i(t)|$, for any $\epsilon>0$ there exists a $T>0$ such that
\begin{equation}
\label{eq:converge}
\left| \tfrac{1}{t} \sum_{k=1}^t |f_i(k)| - m_i \right| <\epsilon
\end{equation}
for all $1 \le i \le d$ and $t>T$. Let $u_{T,i}=\tfrac{m_i+\epsilon}{\epsilon}$. Then the marginals
$\nu_{t,i}$ of $\nu_t$, $1\le i \le d$, satisfy $\nu_{t,i}([0,u_{T,i}]) > 1-\epsilon$ for all $i,t$ considered, otherwise \eqref{eq:converge} would be violated. Thus, the compact set
$K_{\epsilon}=\prod_{i=1}^d [0,u'_{T,i}]$ with $u'_{T,i}=\max\{u_{T,i},|f_i(1),\ldots,|f_i(T)|\}$ satisfies
$\nu_t(K_{\epsilon})>1-d \epsilon$ for all $t \ge 1$ by the union bound, showing that $\{\nu_t\}$ is tight.
\end{proof}
\fi
Note that Lemma \ref{lem:nicefunction} guarantees that all the convergent subsequences of $\{\nu_t\}$ will converge to probability measures. The results of this paper depend on a stronger assumption, as follows.

\begin{definition}
A deterministic function $f:\Omega \rightarrow \real^d$ is called \emph{nice for $(\mu_t)$},
if it satisfies equation \eqref{eq:niceproperty} and its induced probability measure sequence $\nu_t$ converges. 
Denote this limit by $\nu_f$ (or $\nu$ if not causing any ambiguity). 
\end{definition}
In what follows, when $(\mu_t)$ is fixed, we will call $f$ just nice, not mentioning $(\mu_t)$.

The next question to be investigated is whether the space of nice functions is a subspace, i.e., whether it is closed under addition and multiplication by reals. The latter is clear. However, concerning closeness under addition, we have 
the following observation:
\begin{claim}
The space of nice functions is not closed under addition.
\end{claim}
Intuitively, what happens is that we ask for closedness under convergence in distribution, while assuming only weak convergence. In particular, if $f$ is a nice function, it may be that $\nu_t$ converges to $\nu$ weakly

that weak convergence does 
Given two nice function $f$ and $g$, let $X_t$ (and $Y_t$ ) is the random variable induced by $\mu_t$ and $f$(and $g$, respectively). Then $\nu_{f+g,t}$ is the probability function of $X_t+Y_t$. Note that $X_t \xrightarrow{D} X$ and  $Y_t \xrightarrow{D} Y$. 

\emph{However, $X_t+Y_t \xrightarrow{D} X+Y$ is not guaranteed!} And this causes problem for Darmois-Skitovich theorem in next section.


\begin{proof}


Let $f,g:\natural \rightarrow \{\pm1\}$ and $(\mu_t)$ as follows: 
\[
f(t) = \begin{cases} 1 & t=1 \\
					-1 & t=2 \\
					1  & t=3 \\
					-1 & t=4 
					\end{cases};
\quad g(t) = \begin{cases} 1 & t=1 \\
					-1 & t=2 \\
					-1  & t=3 \\
					1 & t=4 
					\end{cases};
\]
\[
\quad \mu_{2t+1}(x) = \begin{cases} 1/2 & x=1 \\
					1/2 & x=2 \\
					0  & \text{otherwise} 
					\end{cases};	
\quad \mu_{2t}(x) = \begin{cases} 1/2 & x=3 \\
					1/2 & x=4 \\
					0  & \text{otherwise} 
					\end{cases};		
\]
Then $\nu_f$ and $\nu_g$ are Bernoulli distribution. However, $\nu_{f+g,2t+1}$ is Bernoulli distribution while $\nu_{f+g,2t}$ is degenerate at 0. 

If $\{\mu_t\}$ are empirical distributions, will this Corollary hold?
\end{proof}
The independence of sequences is defined based on $\nu_f$.
\begin{definition}
Given nice component functions $\{f_1,\ldots,f_d\}$, we say that $\{f_1,\ldots,f_d\}$ are \emph{independent}, if $\nu_{f_1}\times\ldots\times\nu_{f_d} = v_f$, where $f = (f_1,\ldots,f_d):\Omega \rightarrow \real^d$.
\end{definition}

Let $|S|$ be the length of the set $S\subset \real$. 
\begin{cor}
Assume $f:\real\rightarrow\real$ and $g:\real\rightarrow\real$ are periodic functions with period $T_1$ and $T_2$. 
Let $\mu_t((a,b)) = (b-a)/t$ for $0\le a\le b\le t$. If $T_1/T_2$ is irrational, then $f$ and $g$ are independent. 
\end{cor}
%\vspace{-0.5cm}
\begin{proof}
 For two Borel sets $A\subset \real$ and $B\subset \real$, Let $S_1 =f^{-1}(A)\cap [0,T_1] $ and $ S_2 = g^{-1}(B)\cap [0,T_2]$. It is easy to verify that $\nu_f(A) = \frac{|S_1|}{T_1}$, and $\nu_g(B) = \frac{|S_2|}{T_2}$. 
 Note that $\nu_{f,t}(A\times B) = \frac{|f^{-1}(A)\cap g^{-1}(B)\cap [0,t] |}{t}$. Now assume $nT_1 \le t\le (n+1)T_1$ and $mT_2\le t \le (m+1)T_2$. Thus,
 \begin{align*}
 \nu_{f,t}(A\times B) = & \frac{1}{t} \int_{ \substack{x\in f^{-1}(A) \\ 0\le x \le t}} \ind{x\in g^{-1}(B)}\, dx \\
 					=	& \frac{1}{t} \sum_{u=0}^{n-1} \int_{x\in S_1} \ind{uT_1+c \in g^{-1}(B)}\, dc + \frac{C}{t}
 \end{align*}
 for some constant $C$. Now let $n$ approach $\infty$,
 \begin{align*}
  \lim\limits_{n\rightarrow \infty} \nu_{f,t}(A\times B) = & \int_{x\in S_1} \lim\limits_{n\rightarrow \infty} \sum_{u=0}^{n-1} \frac{\ind{uT_1+c \in g^{-1}(B)}}{t} \, dc.
 \end{align*}
 Note that $\lim_{n\rightarrow \infty}\frac{\#\{uT_1+c, \vert u = 1,2,\ldots, n; \, nT_1+c \in g^{-1}(B)\, \text{for some }\}}{n} = \nu_{g}(B)$. 
 \todor{how to prove it?} Thus, 
 \begin{align*}
 \lim\limits_{n\rightarrow \infty} \nu_{f,t}(A\times B) = & \int_{x\in S_1} \frac{\nu_g(B)}{T_1}\, dc \\
 														= & \frac{|S_1|\nu_g(B)}{T_1} \\
 														= & \nu_f(A)\nu_g(B).
 \end{align*}
 Therefore, $f$ and $g$ is independent.
\end{proof}

\begin{cor}
Assume $f:\real\rightarrow\real$ and $g:\real\rightarrow\real$ are periodic functions with period $T_1$ and $T_2$. 
Let $\mu_t((a,b)) = (b-a)/t$ for $0\le a\le b\le t$.  
If $T_1/T_2 = \frac{u}{v}$ is rational, then $f$ and $g$ are independent IFF 
\[
\frac{|\{f^{-1}(A)\cap g^{-1}(B) \cap [0,vT_1]\}|}{v} = \frac{|f^{-1}(A)\cap [0,T_1]| \,|g^{-1}(B)\cap [0,T_2]|}{T_2}
\]
for any Borel sets $A,B\subset \real$.
\end{cor}

\begin{cor}
Assume $f:\natural \rightarrow\real$ and $g:\natural \rightarrow\real$ are periodic functions with period $T_1$ and $T_2$, 
and $\mu_t([1,b]) = \lfloor b\rfloor/\lfloor t\rfloor$ for $0\le b\le t$.  
Then $f$ and $g$ are independent IFF 
\begin{align*}
\#\big\{x\vert f(x) = a, g(x) = b, x\in \{1,\ldots,T_1T_2\} \big\} = & \, \#\big\{x\vert f(x) = a, x\in \{1,\ldots,T_1\} \big\} \\
& \quad \times \#\big\{x\vert g(x) = b, x\in \{1,\ldots,T_2\} \big\} 
\end{align*}
for any Borel sets $a,b\in \real$, where $\#S$ is the cardinality of $S$.
\end{cor}


\section{Correctness of ICA algorithms}
Assume the induced probability measure of each component $f_i$ is non-Gaussian, then correctness of ICA algorithms are guaranteed by Theorem \ref{thm:CorofICA}, as follows.
\begin{thm}
\label{thm:CorofICA}
Let $f_1,\ldots,f_d$ be mutually independent real-valued, nice functions and let $f = (f_1,\ldots,f_d)$.Moreover, assume $\nu_{f_i}$, $i=1,\ldots,d$ are non-Gaussian distributions. If
\begin{equation}
\left(
\begin{array}{ccc}
g_1 \\
\vdots \\
g_n
\end{array}
\right) = A
\left(
\begin{array}{ccc}
f_1 \\
\vdots \\
f_n
\end{array}
\right)
\end{equation}
are mutually independent, then $A$ is equivalent to a permutation matrix up to scaling.
\end{thm}

Before we prove Theorem \ref{thm:CorofICA}, we first need a lemma which is straightforward from Darmois-Skitovich theorem.
\begin{lemma}
Let $f_1,\ldots,f_d$ be mutually independent real-valued, nice functions and let $f = (f_1,\ldots,f_d)$.
Let $g = \sum_i \alpha_i f_i$, $h = \sum_i \beta_i f_i$ where all the coefficients $(\alpha_i)$, $(\beta_i)$ are nonzero.
If $g$ and $h$ are independent then $\nu_{f_i}$, $i=1,\ldots,d$ are Gaussian distributions.
\end{lemma}
\begin{proof}
By Section \ref{sec:IndeofComp}, $\nu_{f_i}$ exists and $\nu_{f_1},\ldots,\nu_{f_n}$ are mutually independent. 
Let $\xi_i$ be a random variable with the distribution of $\nu_{f_i}$, and  $\xi_1,\ldots,\xi_n$ are mutually independent. Note that $\nu_{g_i}$ is the probability function of $\sum_{i}\alpha_if_i$. (OR MAYBE NOT?) 
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:CorofICA}]
From Corollary \ref{cor:DSTheoremCor}. $n$ non-empty mutually exclusive subsets of a size $n$ set, must be a partition of the set, with one in each subset.
\end{proof}
\begin{thm}[Darmois-Skitovich theorem]
Let $\xi_1,\ldots,\xi_n$ be independent random variables. If the linear forms $L_1 = \alpha_1\xi_1 + \ldots + \alpha_n\xi_n$ and $L_2 = \beta_1\xi_1 +\ldots + \beta_n\xi_n$, where the coefficients $\alpha_i$ and $\beta_i$ are nonzero, are independent, then the random variables $\xi_i$ are Gaussian variables.
\end{thm}

\begin{cor}
\label{cor:DSTheoremCor}
Let $\xi_1,\ldots,\xi_n$ be independent random variables. If the random variables $\xi_i$ are non-Gaussian variables, and the linear forms $L_1 = \alpha_1\xi_1 + \ldots + \alpha_n\xi_n$ and $L_2 = \beta_1\xi_1 +\ldots + \beta_n\xi_n$.  Then $\{i\vert \alpha_i \neq0\} \cap \{j\vert \beta_j \neq0\} = \emptyset$.
\end{cor}


\section{Analysis of ICA algorithms}

\subsection{Efficiency}
In this section, we will take the ICA algorithm of \citet{DHsu2012} as an example. 
Assume the independent nice component functions $\{f_1,\ldots,f_d\}$ are bounded by a constant $C$, and satisfy $\E_{\nu_{f_i}}[X]=0$, $\E_{\nu_{f_i}}[X^2]=1$, and $\kappa_i := \E_{\nu_{f_i}}[X^4]\neq 3$. 
Here $\E_{\nu_{f_i}}$ is the expectation is taken with respect to the probability measure of $\nu_{f_i}$. 
For $1\le k\le d$, denote $A_{(t)} = (A_k,A_{ck})$, where $A_{ck}$ is the $(d-1)\times d$ matrix $(A_1,\ldots,A_{k-1},A_{k+1},\ldots,A_d)$, where $A_i$ is the $i$-th column of $A$. 
Without loss of generality, assume $\|A_i\|_2=1$.
The algoeithm of \citet{DHsu2012} is as follows. We denote $\widehat{E}$ as the empirical estimation of parameter $E$. 
\begin{algorithm}[H]
\caption{ICA algorithm of \citet{DHsu2012} \label{alg:icaHsu}}
\begin{algorithmic}[1]
\INPUT algorithmicinput $g(k)$ for $k\in\natural$.
\OUTPUT the coefficient matrix $A$. 
\STATE Sample $\phi$ and $\psi$ from a unit ball of $\real^d$, such that $\|\phi\|_2 = 1$ and $\|\psi\|_2 = 1$;
\STATE Evaluate $\widehat{\nabla^2f(\phi)}$ and $\widehat{\nabla^2f(\psi)}$, \\
\quad where $\widehat{m_p(\eta)} = \frac{1}{t}\sum_{k=1}^{t} (\eta^{\top}g(k))^p$, and $f(\eta) = \frac{1}{12}\big(\widehat{m_4(\eta)} - 3\widehat{m_2(\eta)}^2 \big)$;
\STATE Compute $\widehat{M} = (\widehat{\nabla^2f(\phi)})(\widehat{\nabla^2f(\psi))^{-1}}$;
\STATE Compute all the eigenvectors of $\widehat{M}$, $\{\mu_1,\ldots,\mu_d\}$;
\STATE Return $\widehat{A} = (\mu_1,\ldots,\mu_d)$.
\end{algorithmic}
\end{algorithm}

As shown in \citep{DHsu2012}, with probability 1,
\begin{equation}
\label{eq:M}
M = A 
\left(
\begin{array}{ccc}
\big(\frac{\phi^{\top}A_1}{\psi^{\top}A_1}\big)^2 & &\\
    & \ddots & \\
    & & \big(\frac{\phi^{\top}A_d}{\psi^{\top}A_d}\big)^2\\
\end{array} 
\right) 
A^{-1},
\end{equation}
has distinguish eigenvalues. Thus, if $M =(\nabla^2f(\phi))(\nabla^2f(\psi))^{-1} $ is estimated precisely, Algorithm \ref{alg:icaHsu} returns $A$ exactly  with probability 1. 
Let 
\begin{equation}
\label{def:kappa}
\kappa =  \min_{i,j: i\neq j} \vert (\frac{\phi^{\top}A_i}{\psi^{\top}A_i})^2 - \frac{\phi^{\top}A_j}{\psi^{\top}A_j})^2 \vert. 
\end{equation}
The following lemma shows a small perturbation of $M$ will only result a small variation of its eigenvectors, under some conditions.

\begin{lemma}
\label{lem:eigenvectorvariation}
Denote $\widehat{M} = M+E$ as a perturbation of matrix $M$, where $M$ is defined in Equation \eqref{eq:M}. 
Assume $\widehat{M}$ has distinguish eigenvalues. Let $\delta = \kappa -  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$. 
If $\kappa > 4 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$, and $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\delta}\frac{\sigma_{\max}(A)}{\sigma_{\min}(A) } \|E\|_2$, then there exist a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that 
\[
\sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le 4d  \frac{\sigma_{\max}(A)}{\delta \sigma_{\min}(A) } \|E\|_2
\]
where $\widehat{A}$ is the matrix of eigenvectors of $\widehat{M}$. 
\end{lemma}

\begin{proof}
For $1\le k\le d$, assume 
\[
A_{(k)}^{-1} E A_{(k)} =  
\left(
\begin{array}{cc}
F_{1k} & F_{2k}\\
F_{3k} & F_{4k} \\
\end{array} 
\right). 
\] 
Note that by definitions, $\|F_{1k}\|_2,\|F_{4k}\|_2\le\|A_{(k)}^{-1} E A_{(k)}\|_2\le\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$,
 $\|F_{3k}\|_2\le\|A_{(k)}^{-1}EA_{k}\|_2\le\frac{1}{\sigma_{\min}(A)}\|E\|_2$,
 and $\|F_{2k}\|_2\le\|(A^{-1})_kEA_{(k)}\|_2\le\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2$.
Let $\gamma_k = \|F_{3k}\|_2$, $\eta_k = \|F_{3k}\|_2$, and $\delta_k = \min_{j: j\neq k} \vert (\frac{\phi^{\top}A_k}{\psi^{\top}A_k})^2 - \frac{\phi^{\top}A_j}{\psi^{\top}A_j})^2 \vert - \|F_{1k}\|_2 - \|F_{4k}\|_2$. Thus 
\begin{align*}
\delta_k & = \min_{j:j\neq k} \vert (\frac{\phi^{\top}A_k}{\psi^{\top}A_k})^2 - \frac{\phi^{\top}A_j}{\psi^{\top}A_j})^2 \vert - \|F_{1k}\|_2 - \|F_{4k}\|_2\\
	& \ge \min_{j:j\neq k} \vert (\frac{\phi^{\top}A_k}{\psi^{\top}A_k})^2 - \frac{\phi^{\top}A_j}{\psi^{\top}A_j})^2 \vert - 2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2\\
	& \ge  \kappa -  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2\\
	& >  2 \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}\|E\|_2 >0,
\end{align*}
and $\delta_k^2 > 4\gamma_k\eta_k$. 
Therefore, by Theorem 2.8, Chapter V of \citep{stewart1990matrix}, there exist a unique vector $v$ satisfying $\|v\|_2\le 2\frac{\gamma_k}{\delta_k}$, such that there exists one of a eigenvector $\widehat{A_k}$ of $\widehat{M}$ satisfying
 \[
 \|\widehat{A_k} - A_k\|_2 \le \|A_{ct}\|_2 \|v\|_2 \le 2\sigma_{\max}(A)\frac{\gamma_k}{\delta_k}.\le \frac{4\sigma_{\max}(A)}{\delta \sigma_{\min}(A) } \|E\|_2.
 \]
 By condition, for $i\neq j$,  $\frac{8\sigma_{\max}(A)}{\delta \sigma_{\min}(A) } \|E\|_2 < \|A_i - A_j\|_2\le \|A_i - \widehat{A_i}\|_2 + \|A_j - \widehat{A_i}\|_2$, thus $\widehat{A_i} \neq \widehat{A_j}$.  Summing up the upper bound gets the result. 
\end{proof}

The next lemma shows that $\|\widehat{X}^{-1}\|_2 = \|(X+E)^{-1}\|_2$ is close to $\frac{1}{\sigma_{\min}(X)}$.

\begin{lemma}
\label{lem:inversevariation}
If non-singular matrix $\widehat{X} = X+E$ satisfying that $\sigma_{\min}(X)>\|E\|_2$, then  $\|\widehat{X}^{-1}\|_2 \le \frac{1}{\sigma_{\min}(X)-\|E\|_2}$.
\end{lemma} 
\begin{proof}
Consider the minimal singular value of $\widehat{X}$. 
\[
 \min_{v:\|v\|_2=1} \|\widehat{X}v\|_2 = \min_{v:\|v\|_2=1}\|(X+E)v\|_2 \ge \|Xv\|_2 - \|Ev\|_2 \ge \sigma_{\min}(X) - \|E\|_2.
\]
\end{proof}

Now we can estimate the variance between $XY^{-1}$ and $(X+E_1)(Y+E_2)^{-1}$.
\begin{lemma}
\label{lem:Mvariation}
Assume $\sigma_{\min}(Y) > \|E_2\|_2$, then 
\[
\| XY^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \le \frac{c\|X\|_2\|E_2\|_2}{(\sigma_{\min}(Y) - \|E_2\|_2)^2}+\frac{\|E_1\|_2}{\sigma_{\min}(Y) - \|E_2\|_2},
\]
where $c = \frac{1+\sqrt{5}}{2}$.
\end{lemma}
\begin{proof}

\begin{align*}
	& \| XY^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \\
\le & \| XY^{-1} - X(Y+E_2)^{-1}\|_2 + \| X(Y+E_2)^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \\
\le & \|X\|_2\| Y^{-1} - (Y+E_2)^{-1}\|_2 + \| E_1\|_2 \|(Y+E_2)^{-1}\|_2 \\
\le & c \|X\|_2 \max\{\| Y^{-1}\|^2_2 , \|(Y+E_2)^{-1}\|^2_2\} \|E_2\|_2 + \| E_1\|_2 \|(Y+E_2)^{-1}\|_2,
\end{align*}
where $c = \frac{1+\sqrt{5}}{2}$. The last inequality is by Theorem 3.8, Chapter III of \citep*{stewart1990matrix}. Therefore by Lemma \ref{lem:inversevariation},
\[
\| XY^{-1} - (X+E_1)(Y+E_2)^{-1}\|_2 \le \frac{c\|X\|_2\|E_2\|_2}{(\sigma_{\min}(Y) - \|E_2\|_2)^2}+\frac{\|E_1\|_2}{\sigma_{\min}(Y) - \|E_2\|_2}.
\]
\end{proof}

Note that in Algorithm \ref{alg:icaHsu}, 
\begin{align*}
& \nabla^2 f(\eta) = G_1(\eta) - G_2(\eta) -G_3(\eta), \\
\text{and } \quad & \widehat{\nabla^2 f(\eta)} =\widehat{G_1(\eta)} - \widehat{G_2(\eta)} -\widehat{G_3(\eta)},
\end{align*}
where 
\begin{align*}
& G_1(\eta) = \int (\eta^{\top}Af)^2Aff^{\top}A^{\top}\,d\nu; \\
& G_2(\eta) = \int (\eta^{\top}Af)^2\,d\nu \int Aff^{\top}A^{\top} \,d\nu; \\
& G_3(\eta) = \Big(\int (\eta^{\top}Af)Af\,d\nu\Big)\Big(\int (\eta^{\top}Af)Af\,d\nu\Big)^{\top}. \\
&\widehat{ G_1(\eta)} = \frac1n\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)^2g(k)g(k)^{\top} = \int (\eta^{\top}Af)^2Aff^{\top}A^{\top}\,d\nu_t; \\
& \widehat{G_2(\eta)} = \frac{1}{n^2}\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)^2 \sum_{k=1}^{n}g(k)g(k)^{\top} = \int (\eta^{\top}Af)^2\,d\nu_t \int Aff^{\top}A^{\top} \,d\nu_t; \\
& \widehat{G_3(\eta)} = \frac{1}{n^2}\Big(\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)g(k)\Big) \Big(\sum_{k=1}^{n} \big(\eta^{\top}g(k)\big)g(k)\Big)^{\top} = \Big(\int (\eta^{\top}Af)Af\,d\nu_t\Big)\Big(\int (\eta^{\top}Af)Af\,d\nu_t\Big)^{\top}.
\end{align*}
Given two probability measure $\mu$ and $\nu$ over $(\Omega , \xi)$, recall the definition of total variance, $D_{tv}(\mu, \nu) = \sup_{A\in\xi} |\mu(A) - \nu(A)|$. 
The following lemma bound $\|\nabla^2 f(\eta) - \widehat{\nabla^2 f(\eta)} \|_2$ by $D_{tv}(\nu_t , \nu)$.


\begin{lemma}
\label{lem:nablavariation}
\[
\|\nabla^2 f(\eta) - \widehat{\nabla^2 f(\eta)}  \|_2 \le d q D_{tv}(\nu_t , \nu) + 2dq D_{tv}(\nu_t , \nu)^2,
\]
where $q_{\eta} = d^2C^4\|\eta\|_2^2\sigma_{\max}(A)^4$.
\end{lemma}
\begin{proof}
Denote the $i$-th row of $A$ by $A_{:,i}$, and the $(i,j)$ entry of a matrix $M$ by $(M)_{i,j}$. Since $\|f_i\|_\infty \le C$,$\|f\|_2 \le C\sqrt{d}$. 
Thus $\big(\eta^{\top}Af\big)^2 \le d C^2\|\eta\|_2^2\sigma_{\max}(A)^2$ 
% \le d C^2\sigma_{\max}(A)^2 $
,  $|A_{:i}f| \le C\sqrt{d}\sigma_{\max}(A)$, 
therefore $|\big(\eta^{\top}Af\big)^2 A_{:i}ff^{\top} A_{:j}^{\top}| \le d^2C^4\sigma_{\max}(A)^4 \|\eta\|_2^2 = q_{\eta}$.
\[
|(G_1)_{i,j} - (\widehat{G_1})_{i,j} | \le q_{\eta} D_{tv}(\nu_t , \nu).
\]
Similarly, 
\begin{align*}
& |(G_2)_{i,j} - (\widehat{G_2})_{i,j} | \le q_{\eta} D_{tv}(\nu_t , \nu)^2 \\
\text{and } \quad & |(G_3)_{i,j} - (\widehat{G_3})_{i,j} | \le q D_{tv}(\nu_t , \nu)^2.
\end{align*}
Therefore,
\begin{align*}
	& \|\nabla^2 f(\eta) - \widehat{\nabla^2 f(\eta)}  \|_2 \\
\le & \|G_1 - \widehat{G_1}\|_2 + \|G_2 - \widehat{G_2}\|_2 + \|G_3 - \widehat{G_3}\|_2 \\
\le & d q_{\eta} D_{tv}(\nu_t , \nu) + 2dq_{\eta} D_{tv}(\nu_t , \nu)^2.
\end{align*}
\end{proof}

\if0
We still need to bound $\|\big(\nabla^2 f(\eta)\big)^{-1} - \big(\widehat{\nabla^2 f(\eta)}\big)^{-1}  \|_2 $, as follows.
\begin{lemma}
Assume $\|\eta\|_2 = 1$, then 
\[
\|\big(\nabla^2 f(\eta)\big)^{-1} - \big(\widehat{\nabla^2 f(\eta)}\big)^{-1}  \|_2 \le cd^2q^2\big(D_{tv}(\nu_n , \nu) + 2D_{tv}(\nu_n , \nu)^2\big).
\]
\end{lemma}
\begin{proof}
From the proof of Lemma \ref{lem:boundnablafeta}, $\|\nabla^2 f(\eta)\|_2, \|\widehat{\nabla^2 f(\eta)}\|_2 \le dq$, where $q = d^2C^4\sigma_{\max}(A)^4$.
Apply Theorem 3.8 of \citep*{stewart1990matrix}, we have 
\begin{align*}
& \|\big(\nabla^2 f(\eta)\big)^{-1} - \big(\widehat{\nabla^2 f(\eta)}\big)^{-1}  \|_2 \\
\le & c\max\{\|\nabla^2 f(\eta)\|_2, \|\widehat{\nabla^2 f(\eta)}\|_2\} \|\nabla^2 f(\eta) - \widehat{\nabla^2 f(\eta)}\|_2 \\
\le & cd^2q^2\big(D_{tv}(\nu_n , \nu) + 2D_{tv}(\nu_n , \nu)^2\big).
\end{align*}

\end{proof}
\fi


 \begin{thm}
 \label{thm:efficiency}
 Let 
 \[ 
 Q=  cd^2 \frac{q_{\psi}q_{\phi}\big(D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}{\big(\sigma_{\min}(\nabla^2f(\psi)) - d q_{\psi} \big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)\big)^2}
 +d\frac{q_{\phi}\big(D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}{\sigma_{\min}(\nabla^2f(\psi)) - dq_{\psi} \big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}.
 \] 
 Assume $t$ is large enough such that the following conditions hold:
 \begin{itemize}
 \item $\widehat{M}$ has distinguish eigenvalues;
 \item $\sigma_{\min}(\nabla^2f(\psi)) > d q_\psi\big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)$;
 \item $\kappa > 4\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} Q$
 \item $\min_{i,j:i\neq j} \|A_i - A_j\|_2 > \frac{8}{\delta}\frac{\sigma_{\max}(A)}{\sigma_{\min}(A) } Q$.
 \end{itemize}
 Then there exist a permutation $\pi$ and constants $\{c_1,\ldots,c_d\}$, such that
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4d^2q(cdq+1)}{\delta} \frac{\sigma_{\max}(A)}{ \sigma_{\min}(A)}Q ,
 \]
 where $\delta = \kappa -  2\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}Q>0$.
 \end{thm}
 \begin{proof}
 By Lemma \ref{lem:eigenvectorvariation}, 
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le 4d  \frac{\sigma_{\max}(A)}{\delta \sigma_{\min}(A) } \|M - \widehat{M} \|_2. 
 \]
 Note that $M = \nabla^2f(\phi))(\nabla^2f(\psi))^{-1}$,  then by Lemma \ref{lem:Mvariation},
 \[
 \|M - \widehat{M} \|_2 \le \frac{c\|\nabla^2f(\phi)\|_2\|\nabla^2f(\psi) - \widehat{\nabla^2f(\psi)}\|_2}{\big(\sigma_{\min}(\nabla^2f(\psi)) - \|\nabla^2f(\psi) - \widehat{\nabla^2f(\psi)}\|_2\big)^2} + \frac{\|\nabla^2f(\phi) - \widehat{\nabla^2f(\phi)}\|_2}{\sigma_{\min}(\nabla^2f(\psi)) - \|\nabla^2f(\psi) - \widehat{\nabla^2f(\psi)}\|_2}. 
 \]
 From the proof of Lemma \ref{lem:nablavariation}, we have $\|\nabla^2 f(\phi)\|_2\le dq_{\phi}$, where $q_{\phi} = d^2C^4\sigma_{\max}(A)^4\|\phi\|_2^2$. Apply Lemma \ref{lem:nablavariation}, 
 \begin{align*}
 \|M - \widehat{M} \|_2 \le &
 cd^2 \frac{q_{\psi}q_{\phi}\big(D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}{\big(\sigma_{\min}(\nabla^2f(\psi)) - d q_{\psi} \big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)\big)^2} \\
 	&\quad +d\frac{q_{\phi}\big(D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}{\sigma_{\min}(\nabla^2f(\psi)) - dq_{\psi} \big( D_{tv}(\nu_t , \nu) + 2 D_{tv}(\nu_t , \nu)^2\big)}\\
 = & Q.
 \end{align*}
 So, 
 \[
 \sum_{k=1}^{d}\| c_1\widehat{A}_{\pi(k)} - A_k\|_2 \le \frac{4d}{\delta} \frac{\sigma_{\max}(A)}{ \sigma_{\min}(A)}Q. 
 \]
 \end{proof}

\subsection{Condition number of ICA problem}
From Theorem \ref{thm:efficiency}, two parameters in the efficiency that depend on the algorithm rather than the ICA problem are the number $\kappa$ and $Q$. 
When $n$ is large enough, 
\[
Q\approx \left[ \frac{2q_{\psi}q_{\phi}}{\big(\sigma_{\min}(\nabla^2f(\psi))\big)^2}+\frac{q_{\phi}}{\sigma_{\min}(\nabla^2f(\psi))}\right]D_{tv}(\nu_n , \nu)^2.
\] 
Note that $\nabla^2f(\psi) = \sum_{i=1}^{d} \kappa_i(\psi^{\top}A_i)^2A_iA_i^{\top}$. Thus, $\sigma_{\min}(\nabla^2f(\psi)) = \min_{v:\|v\|_2=1}\|\sum_{i=1}^{d} \kappa_i(\psi^{\top}A_i)^2A_iA_i^{\top}v\|_2$ 

\begin{claim}
With large probability, $\sigma_{\min}(\nabla^2f(\psi))/\|\psi\|_2^2$ won't be small.
\end{claim}

The next
\begin{thm}
There exists a way sampling $\phi$ and $\psi$, such that $\kappa = \Theta(\log d)$.
\end{thm}
\begin{proof}
Instead of sampling $\phi$ and $\psi$ uniformly from the unit ball, sample $\phi,\psi \sim {\cN} (0,\,A^{-1}A^{{-1}^{\top}})$ independently. Therefore, $\alpha_i = \phi^{\top}A_i$, $\beta_i = \psi^{\top}A_i$ are independent standard normal variables. Moreover, 
\[
\kappa  = \min_{i,j} \vert \big(\frac{\alpha_i}{\beta_i}\big)^2 - \big(\frac{\alpha_j}{\beta_j}\big)^2 \vert.
\] 

Note that for $x\ge0$, $\Phi(x) \approx \frac12 (1+\sqrt{1-e^{-\frac{2}{\pi}x^2}})$ \citep{aludaat2008note}.
 
 When $d$ is large,
\[
\Prob{|\beta_i| \ge \sqrt{\frac{\pi}{2}\log \frac{d^2}{d^2-1}} \text{ for all } \beta_i\text{'s}} = (2- 2\Phi(\sqrt{\frac{\pi}{2}\log \frac{d^2}{d^2-1}})-1)^d \approx \left(1-\frac{1}{d}\right)^d \approx \frac{1}{e}.
\]
Thus, with probability at least $p (\approx \frac{1}{e})$,
\[
\kappa \le 2 \min_i \vert \big(\frac{\alpha_i}{\beta_i}\big)^2\vert \le 2 \frac{2}{\pi}\frac{1}{\log \frac{d^2}{d^2-1}}\min_i |\alpha_i|^2.
\]

Now consider the case that at least two of $\beta_i$'s are greater than $\sqrt{\frac{\pi}{2}\log d}$,
\begin{align*}
& \Prob{|\beta_i| \ge \sqrt{\frac{\pi}{2}\log d} \text{ for at least 2 } \beta_i\text{'s}} \\
=  & 1 - \Prob{|\beta_i| \le \sqrt{\frac{\pi}{2}\log d} \text{ for all } \beta_i\text{'s}} - \Prob{|\beta_i| \le \sqrt{\frac{\pi}{2}\log d} \text{ for (d-1) } \beta_i\text{'s}} \\
\approx & 1 - (1-\frac{1}{d})^{\frac{d}{2}} -  (1-\frac{1}{d})^{\frac{d-1}{2}}(1-\sqrt{1-\frac{1}{d}}) \\
\approx & 1 -\frac{1}{\sqrt{e}}.
\end{align*}
Thus, with probability at least $p_2$ ($\approx 1-\frac{1}{\sqrt{e}}$),
\[
\E[\kappa] \le \frac{4}{\pi} \frac{1}{\log d} \E[\alpha_i^2] = \frac{4}{\pi} \frac{1}{\log d}.
\]
\end{proof}

So the efficiency of the algorithm of \citet{DHsu2012} depends on $\kappa$. Is it actually independent to the algorithm? If so, we can define it as the condition number of a ICA problem, which define how difficult the ICA problem is. 

\bibliography{DICA}
\bibliographystyle{plainnat}
\end{document}
