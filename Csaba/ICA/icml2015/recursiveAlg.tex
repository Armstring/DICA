%!TEX root =  DICA.tex

\subsection{Recursive versions}

Recently \citet{vempala2014max} proposed an idea to improve the sample complexity of Fourier PCA algorithm \citep{goyal2014fourier}. 
The idea is that instead of recovering all the columns of $A$ in one eigen-decomposition, it only decompose the whole space into two sub-spaces, 
then recursively decompose each subspaces until it is 1-dimensional.
The insight of this recursive procedure is when the maximal spacing of the eigenvalues are much large than  the minimal one (e.g. improve from $\frac{\delta}{d^2}$ to $\frac{\delta}{\log d}$), then even the error may accumulated in the recursion, overall we still have better results. 
However, this algorithm is based on the assumption that the mixing matrix is orthonormal, so that the projection to its subspaces can always eliminate some component of the source signals. 

In this section, we adapt the idea of \citet{vempala2014max} and apply it to our algorithms. Due to space limit, we will only take the simplest recursive algorithm, recursive version of HKICA, as an example.
\begin{algorithm} 
\caption{recursive version of HKICA (HKICA\_recur)}
\label{alg:HKICA_recur}
\begin{algorithmic}[1]
\INPUT $x(t)$ for $1\le t \le T$. 
\OUTPUT An estimation of the mixing matrix $A$. 
\STATE Sample $\psi$ from a $d$-dimensional standard Gaussian distribution;
\STATE Evaluate $\nabla^2\hat{f}(\psi) = \hat{G}(\psi)$; \\
\STATE Compute $\hat{B}$ such that $\nabla^2\hat{f}(\psi) = \hat{B}\hat{B}^{\top}$;
\STATE Compute $\hat{y}(t) = \hat{B}^{-1}x(t)$ for $1\le t \le T$;
\STATE Let $P = I_d$;
\STATE Compute $\hat{R} = \text{Recur}(\hat{y}, P)$;
\STATE Return $\hat{B}\hat{R}$;
\end{algorithmic}
\end{algorithm}
\begin{algorithm} 
\caption{The `Recur' Helper}
\label{alg:recur}
\begin{algorithmic}[1]
\INPUT $x(t)$ for $1\le t \le T$, a projection matrix $P\in \R^{d\times k}$ ($d\ge k$). 
\OUTPUT An estimation of the mixing matrix $A\in \R^{d\times k}$. 

\STATE if $k==1$, Return $P$;
\STATE Sample $\phi_1$ and $\phi_2$ independently from a standard Gaussian distribution of dimension $d$;
\STATE Evaluate $\nabla^2\hat{f}(\phi_1)$ and $\nabla^2\hat{f}(\phi_2)$, 
\STATE Compute $\hat{T} = (\nabla^2 \hat{f}(\phi_1))(\nabla^2\hat{f}(\phi_2))^{-1}$;
\STATE Compute $\hat{M} = P^{\top} \hat{M} P$;
\STATE Compute all the eigen-decomposition of $\hat{T}$, its eigenvalues$\{\sigma_1,\ldots,\sigma_d\}$ where $\sigma_1\ge\ldots\ge \sigma_k$ and their corresponding eigenvectors $\{\mu_1,\ldots, \mu_k\}$;
\STATE Find the index $m = \arg\max \sigma_m - \sigma_{m+1}$; 
\STATE Let $P_1 = (\mu_1,\ldots,\mu_m)$, and $P_2 = (\mu_{m+1},\ldots,\mu_k)$;
\STATE Compute $W_1 = \text{Recur} (x, PP_1)$, and  $W_2 = \text{Recur} (x, PP_2)$;
\STATE Return $[W_1,W_2]$;
\end{algorithmic}
\end{algorithm}

\begin{remark}
Other algorithms can be modified into a recursive version in a similar way, based on the idea of mapping the estimated Hessian matrix into a small subspace by $T = P^{\top}MP$. 
\qed
\end{remark}
\begin{thm}
\label{thm:recursiveAlg}
Given the conditions in Theorem \ref{thm:finalRes}, with probability at least $1-\delta$, the recursive version of HKICA returns the mixing matrix with a guaranteed polynomial error bound.
\end{thm} 
\begin{remark}
In our result, the recursive version gets worse error bound, compared to the one-pass version. The reason is that the events $\Ephi$ and $\Ez$ could be required to hold for $d$ times. Therefore, for the final result to hold for probability at least $1-\delta$, the error bound for each recursion has to hold with probability at least $1-\frac{1}{d}\delta$. 

Recall that in our case we have lower bound for the minimal eigenvalue spacing as $O(\frac{1}{d^2}\delta)$, and lower bound for maximal eigenvalue spacing as $O(\frac{1}{d}\delta)$. By requiring it holds for higher probability ($1-\frac{1}{d}\delta$), the maximal gap actually reduced to $O(\frac{1}{d^2}\delta)$, same order as the one-pass version, leading to the same error bound.

Moreover, for the event $E\phi$, the recursive version essentially require the lower bound $\ell$ is $d$ times smaller than the one-pass version, which cause high degrees of $d$ in the error bound.

For the recursive version to help, one possible way is to prove that for each recursion, we only require the conclusion holds with probability at least $1-\frac{1}{\log d}\delta$, rather than $1-\frac{1}{d}\delta$. 
Another potential improvement is developing a lower bound for the maximal spacing that is sub-polynomial in $d$, e.g. $\frac{1}{\log d}\delta$, and sub-polynomial dependence of $\delta$ in the lower bound in the Event $\Ephi$, e.g. $\frac{1}{\log \frac{1}{\delta}}$.    
\qed
\end{remark}